{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5FX9WHJj7Hz"
      },
      "source": [
        "# Lesson 3: Basic Workflow Ingredients: Chaining, Routing, Parallelization, and Orchestration\n",
        "\n",
        "This Jupyter notebook demonstrates AI agent workflow patterns using Google Gemini, focusing on chaining, routing, and parallelization strategies.\n",
        "\n",
        "The sections are:\n",
        "\n",
        "1. Single LLM Call Problems - Demonstrates issues with complex prompts that try to do everything at once.\n",
        "\n",
        "2. Sequential Workflows - Breaking tasks into steps (generate questions → answer questions → find sources) for better consistency.\n",
        "\n",
        "3. Parallel Workflows - Running tasks in parallel (answering questions in parallel) for higher speed.\n",
        "\n",
        "4. Routing Workflows - Classifying user intent and routing to specialized handlers (technical support, billing, general questions).\n",
        "\n",
        "5. Orchestrator-Worker Pattern - A system where an orchestrator breaks complex queries into subtasks, specialized workers handle each task, and a synthesizer combines results into a cohesive response\n",
        "\n",
        "The examples include FAQ generation from multiple sources and a customer service system handling billing inquiries, product returns, and order status updates simultaneously."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROFh4KyZj7Hz"
      },
      "source": [
        "## Setup: Installing Required Dependencies\n",
        "\n",
        "First, we need to install the Google Generative AI library to interact with Gemini models:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZkyepXBz6YXx"
      },
      "outputs": [],
      "source": [
        "%pip install -q google-genai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfeCh44qj7H0"
      },
      "source": [
        "## Configuring the Gemini Client\n",
        "\n",
        "Next, we import the necessary libraries and configure the Gemini client. The API key is retrieved from Google Colab's userdata.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLzh9oIZ32FP",
        "outputId": "e1809406-3986-4fc9-bdfa-5c2aa2ce64a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gemini client initialized successfully.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import asyncio\n",
        "import random\n",
        "from pydantic import BaseModel, RootModel, Field\n",
        "from typing import List, Optional\n",
        "import time\n",
        "from enum import Enum\n",
        "from google import genai\n",
        "\n",
        "\n",
        "# Initialize the Gemini client\n",
        "# The client uses the GOOGLE_API_KEY from the environment\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    GOOGLE_API_KEY = userdata.get(\"GOOGLE_API_KEY\")\n",
        "except ImportError:\n",
        "    GOOGLE_API_KEY = os.environ.get(\"GOOGLE_API_KEY\")\n",
        "\n",
        "if not GOOGLE_API_KEY:\n",
        "    raise RuntimeError(\"GOOGLE_API_KEY not found. Set it in your environment or Colab userdata.\")\n",
        "\n",
        "# Create Gemini client\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
        "print(\"Gemini client initialized successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQHYrdrETWE5"
      },
      "source": [
        "# The problem with a single, large LLM call"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meM2RgTkj7H0"
      },
      "source": [
        "## Setting Up Mock Data\n",
        "\n",
        "We'll create three mock webpages about renewable energy topics that will serve as our source content for the FAQ generation examples. Each webpage has a title and detailed content about solar energy, wind turbines, and energy storage:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "yD8-KVvjTbiI"
      },
      "outputs": [],
      "source": [
        "# Here we define our mock webpage content. Each source has a title and text.\n",
        "\n",
        "webpage_1 = {\n",
        "    \"title\": \"The Benefits of Solar Energy\",\n",
        "    \"content\": \"\"\"\n",
        "    Solar energy is a renewable powerhouse, offering numerous environmental and economic benefits.\n",
        "    By converting sunlight into electricity through photovoltaic (PV) panels, it reduces reliance on fossil fuels,\n",
        "    thereby cutting down greenhouse gas emissions. Homeowners who install solar panels can significantly\n",
        "    lower their monthly electricity bills, and in some cases, sell excess power back to the grid.\n",
        "    While the initial installation cost can be high, government incentives and long-term savings make\n",
        "    it a financially viable option for many. Solar power is also a key component in achieving energy\n",
        "    independence for nations worldwide.\n",
        "    \"\"\",\n",
        "}\n",
        "\n",
        "webpage_2 = {\n",
        "    \"title\": \"Understanding Wind Turbines\",\n",
        "    \"content\": \"\"\"\n",
        "    Wind turbines are towering structures that capture kinetic energy from the wind and convert it into\n",
        "    electrical power. They are a critical part of the global shift towards sustainable energy.\n",
        "    Turbines can be installed both onshore and offshore, with offshore wind farms generally producing more\n",
        "    consistent power due to stronger, more reliable winds. The main challenge for wind energy is its\n",
        "    intermittency—it only generates power when the wind blows. This necessitates the use of energy\n",
        "    storage solutions, like large-scale batteries, to ensure a steady supply of electricity.\n",
        "    \"\"\",\n",
        "}\n",
        "\n",
        "webpage_3 = {\n",
        "    \"title\": \"Energy Storage Solutions\",\n",
        "    \"content\": \"\"\"\n",
        "    Effective energy storage is the key to unlocking the full potential of renewable sources like solar\n",
        "    and wind. Because these sources are intermittent, storing excess energy when it's plentiful and\n",
        "    releasing it when it's needed is crucial for a stable power grid. The most common form of\n",
        "    large-scale storage is pumped-hydro storage, but battery technologies, particularly lithium-ion,\n",
        "    are rapidly becoming more affordable and widespread. These batteries can be used in homes, businesses,\n",
        "    and at the utility scale to balance energy supply and demand, making our energy system more\n",
        "    resilient and reliable.\n",
        "    \"\"\",\n",
        "}\n",
        "\n",
        "all_sources = [webpage_1, webpage_2, webpage_3]\n",
        "\n",
        "# We'll combine the content for the LLM to process\n",
        "combined_content = \"\\n\\n\".join(\n",
        "    [f\"Source Title: {source['title']}\\nContent: {source['content']}\" for source in all_sources]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtGkdMErj7H1"
      },
      "source": [
        "## Example: Complex Single LLM Call\n",
        "\n",
        "This example demonstrates the problem with trying to do everything in one complex prompt. We're asking the LLM to generate questions, find answers, and cite sources all in a single call, which can lead to inconsistent results:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkGxJhhaTjBy",
        "outputId": "70df85d0-4bab-46bc-aad5-bbbc86fd6ba3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Complex prompt result (might be inconsistent):\n",
            "{\n",
            "  \"faqs\": [\n",
            "    {\n",
            "      \"question\": \"How does solar energy work?\",\n",
            "      \"answer\": \"Solar energy converts sunlight into electricity through photovoltaic (PV) panels.\",\n",
            "      \"sources\": [\n",
            "        \"The Benefits of Solar Energy\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"question\": \"What are the environmental advantages of using solar energy?\",\n",
            "      \"answer\": \"Solar energy reduces reliance on fossil fuels and cuts down greenhouse gas emissions.\",\n",
            "      \"sources\": [\n",
            "        \"The Benefits of Solar Energy\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"question\": \"What economic benefits can homeowners gain from installing solar panels?\",\n",
            "      \"answer\": \"Homeowners can significantly lower their monthly electricity bills and potentially sell excess power back to the grid.\",\n",
            "      \"sources\": [\n",
            "        \"The Benefits of Solar Energy\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"question\": \"What is the primary function of wind turbines?\",\n",
            "      \"answer\": \"Wind turbines capture kinetic energy from the wind and convert it into electrical power.\",\n",
            "      \"sources\": [\n",
            "        \"Understanding Wind Turbines\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"question\": \"What is the main challenge associated with wind energy?\",\n",
            "      \"answer\": \"The main challenge for wind energy is its intermittency, as it only generates power when the wind blows.\",\n",
            "      \"sources\": [\n",
            "        \"Understanding Wind Turbines\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"question\": \"Where can wind turbines be installed?\",\n",
            "      \"answer\": \"Wind turbines can be installed both onshore and offshore.\",\n",
            "      \"sources\": [\n",
            "        \"Understanding Wind Turbines\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"question\": \"Why is energy storage essential for renewable sources like solar and wind?\",\n",
            "      \"answer\": \"Energy storage is crucial for a stable power grid because these renewable sources are intermittent, requiring storage of excess energy and release when needed.\",\n",
            "      \"sources\": [\n",
            "        \"Energy Storage Solutions\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"question\": \"What are some common large-scale energy storage technologies?\",\n",
            "      \"answer\": \"Common forms of large-scale storage include pumped-hydro storage and battery technologies, especially lithium-ion.\",\n",
            "      \"sources\": [\n",
            "        \"Energy Storage Solutions\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"question\": \"How do offshore wind farms differ in power production from onshore ones?\",\n",
            "      \"answer\": \"Offshore wind farms generally produce more consistent power due to stronger, more reliable winds compared to onshore installations.\",\n",
            "      \"sources\": [\n",
            "        \"Understanding Wind Turbines\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"question\": \"What is the role of energy storage solutions in the overall energy system?\",\n",
            "      \"answer\": \"Energy storage solutions balance energy supply and demand, making the energy system more resilient and reliable.\",\n",
            "      \"sources\": [\n",
            "        \"Energy Storage Solutions\"\n",
            "      ]\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# This prompt tries to do everything at once: generate questions, find answers,\n",
        "# and cite sources. This complexity can often confuse the model.\n",
        "n_questions = 10\n",
        "prompt_complex = f\"\"\"\n",
        "Based on the provided content from three webpages, generate a list of exactly {n_questions} frequently asked questions (FAQs).\n",
        "For each question, provide a concise answer derived ONLY from the text.\n",
        "After each answer, you MUST include a list of the 'Source Title's that were used to formulate that answer.\n",
        "\n",
        "Your final output should be a JSON array where each object has three keys: \"question\", \"answer\", and \"sources\" (which is an array of strings).\n",
        "\n",
        "<provided_content>\n",
        "{combined_content}\n",
        "</provided_content>\n",
        "\"\"\".strip()\n",
        "\n",
        "# Pydantic classes for structured outputs\n",
        "class FAQ(BaseModel):\n",
        "    question: str\n",
        "    answer: str\n",
        "    sources: List[str]\n",
        "\n",
        "class FAQList(BaseModel):\n",
        "    faqs: List[FAQ]\n",
        "\n",
        "# Generate FAQs\n",
        "response_complex = client.models.generate_content(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    contents=prompt_complex,\n",
        "    config={\n",
        "        \"response_mime_type\": \"application/json\",\n",
        "        \"response_schema\": FAQList\n",
        "    },\n",
        ")\n",
        "result_complex = response_complex.parsed\n",
        "\n",
        "print(\"Complex prompt result (might be inconsistent):\")\n",
        "print(result_complex.model_dump_json(indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJ0GxwEK4RVR"
      },
      "source": [
        "# Building a sequential workflow\n",
        "\n",
        "Now, let's split the complex prompt above into a chain of simpler prompts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsKbKIbBj7H1"
      },
      "source": [
        "## Question Generation Function\n",
        "\n",
        "Let's create a function to generate questions from the content. This step focuses solely on creating relevant questions based on the provided material:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WET69j1c4clg",
        "outputId": "25655c61-24b6-4cf0-daf5-2334a6abd411"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully generated 10 questions.\n",
            "\n",
            "First few questions:\n",
            "1. What are the environmental and economic advantages of solar energy?\n",
            "2. How do photovoltaic (PV) panels generate electricity from sunlight?\n",
            "3. What are the financial considerations for homeowners interested in solar panels, including initial costs and potential savings?\n"
          ]
        }
      ],
      "source": [
        "class QuestionList(BaseModel):\n",
        "    questions: List[str]\n",
        "\n",
        "prompt_generate_questions = \"\"\"\n",
        "Based on the content below, generate a list of {n_questions} relevant and distinct questions that a user might have.\n",
        "Return these questions as a JSON array of strings.\n",
        "\n",
        "<provided_content>\n",
        "{combined_content}\n",
        "</provided_content>\n",
        "\"\"\".strip()\n",
        "\n",
        "def generate_questions(content, n_questions=10):\n",
        "    \"\"\"\n",
        "    Generate a list of questions based on the provided content.\n",
        "\n",
        "    Args:\n",
        "        content: The combined content from all sources\n",
        "\n",
        "    Returns:\n",
        "        list: A list of generated questions\n",
        "    \"\"\"\n",
        "    response_questions = client.models.generate_content(\n",
        "        model=\"gemini-2.5-flash\",\n",
        "        contents=prompt_generate_questions.format(n_questions=n_questions, combined_content=content),\n",
        "        config={\n",
        "            \"response_mime_type\": \"application/json\",\n",
        "            \"response_schema\": QuestionList\n",
        "        }\n",
        "    )\n",
        "\n",
        "    return response_questions.parsed.questions\n",
        "\n",
        "# Test the question generation function\n",
        "questions = generate_questions(combined_content, n_questions=10)\n",
        "print(f\"Successfully generated {len(questions)} questions.\")\n",
        "print(f\"\\nFirst few questions:\")\n",
        "for i, q in enumerate(questions[:3]):\n",
        "    print(f\"{i+1}. {q}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCoQfR3vj7H1"
      },
      "source": [
        "## Answer Generation Function\n",
        "\n",
        "Next, we create a function to generate answers for individual questions using only the provided content:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVXKXFl6j7H1",
        "outputId": "8316b365-a8bd-45b3-fc83-9216b1e5ebc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What are the environmental and economic advantages of solar energy?\n",
            "Answer: Environmental advantages of solar energy include reduced reliance on fossil fuels and a decrease in greenhouse gas emissions. Economic advantages include significantly lower monthly electricity bills for homeowners, the ability to sell excess power back to the grid, long-term savings, and contributing to energy independence for nations.\n"
          ]
        }
      ],
      "source": [
        "prompt_answer_question = \"\"\"\n",
        "Using ONLY the provided content below, answer the following question.\n",
        "The answer should be concise and directly address the question.\n",
        "\n",
        "Question:\n",
        "\"{question}\"\n",
        "\n",
        "Provided Content:\n",
        "---\n",
        "{combined_content}\n",
        "---\n",
        "\"\"\".strip()\n",
        "\n",
        "def answer_question(question, content):\n",
        "    \"\"\"\n",
        "    Generate an answer for a specific question using only the provided content.\n",
        "\n",
        "    Args:\n",
        "        question: The question to answer\n",
        "        content: The combined content from all sources\n",
        "\n",
        "    Returns:\n",
        "        str: The generated answer\n",
        "    \"\"\"\n",
        "    answer_response = client.models.generate_content(\n",
        "        model=\"gemini-2.5-flash\",\n",
        "        contents=prompt_answer_question.format(question=question, combined_content=content),\n",
        "    )\n",
        "    return answer_response.text\n",
        "\n",
        "# Test the answer generation function\n",
        "test_question = questions[0]\n",
        "test_answer = answer_question(test_question, combined_content)\n",
        "print(f\"Question: {test_question}\")\n",
        "print(f\"Answer: {test_answer}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddYTqf7Vj7H1"
      },
      "source": [
        "## Source Finding Function\n",
        "\n",
        "Finally, we create a function to identify which sources were used to generate an answer:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyApKT-hj7H1",
        "outputId": "aeed549e-0f50-4876-9315-602795c3f211"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What are the environmental and economic advantages of solar energy?\n",
            "Answer: Environmental advantages of solar energy include reduced reliance on fossil fuels and a decrease in greenhouse gas emissions. Economic advantages include significantly lower monthly electricity bills for homeowners, the ability to sell excess power back to the grid, long-term savings, and contributing to energy independence for nations.\n",
            "Sources: ['The Benefits of Solar Energy']\n"
          ]
        }
      ],
      "source": [
        "class SourceList(BaseModel):\n",
        "    sources: List[str]\n",
        "\n",
        "prompt_find_sources = \"\"\"\n",
        "You will be given a question and an answer that was generated from a set of documents.\n",
        "Your task is to identify which of the original documents were used to create the answer.\n",
        "Return a JSON object with a single key \"sources\" which is a list of the titles of the relevant documents.\n",
        "\n",
        "Question: \"{question}\"\n",
        "Answer: \"{answer}\"\n",
        "\n",
        "<provided_content>\n",
        "{combined_content}\n",
        "</provided_content>\n",
        "\"\"\".strip()\n",
        "\n",
        "def find_sources(question, answer, content):\n",
        "    \"\"\"\n",
        "    Identify which sources were used to generate an answer.\n",
        "\n",
        "    Args:\n",
        "        question: The original question\n",
        "        answer: The generated answer\n",
        "        content: The combined content from all sources\n",
        "\n",
        "    Returns:\n",
        "        list: A list of source titles that were used\n",
        "    \"\"\"\n",
        "    sources_response = client.models.generate_content(\n",
        "        model=\"gemini-2.5-flash\",\n",
        "        contents=prompt_find_sources.format(question=question, answer=answer, combined_content=content),\n",
        "        config={\n",
        "            \"response_mime_type\": \"application/json\",\n",
        "            \"response_schema\": SourceList\n",
        "        }\n",
        "    )\n",
        "    return sources_response.parsed.sources\n",
        "\n",
        "# Test the source finding function\n",
        "test_sources = find_sources(test_question, test_answer, combined_content)\n",
        "print(f\"Question: {test_question}\")\n",
        "print(f\"Answer: {test_answer}\")\n",
        "print(f\"Sources: {test_sources}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Illuwizvj7H1"
      },
      "source": [
        "## Executing the Sequential Workflow\n",
        "\n",
        "Now we combine all three functions into a sequential workflow: Generate Questions → Answer Questions → Find Sources. Each step is executed one after another for each question. Notice how much time it takes to run the full workflow.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1rd7ZpDij7H1",
        "outputId": "896e0ea6-de15-42ac-8aab-38b68b6adf55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  - Processing: 'What are the primary economic and environmental be...'\n",
            "  - Processing: 'What are the main challenges associated with wind ...'\n",
            "  - Processing: 'Why is effective energy storage crucial for integr...'\n",
            "  - Processing: 'What are the different types of large-scale energy...'\n",
            "\n",
            "Sequential processing completed in 20.49 seconds\n",
            "\n",
            "Generated FAQ List (Sequential):\n",
            "[\n",
            "  {\n",
            "    \"question\": \"What are the primary economic and environmental benefits of adopting solar energy?\",\n",
            "    \"answer\": \"The primary economic benefits of adopting solar energy include significantly lower monthly electricity bills for homeowners, the potential to sell excess power back to the grid, and contributing to energy independence for nations. The primary environmental benefit is the reduction of greenhouse gas emissions by decreasing reliance on fossil fuels.\",\n",
            "    \"sources\": [\n",
            "      \"The Benefits of Solar Energy\"\n",
            "    ]\n",
            "  },\n",
            "  {\n",
            "    \"question\": \"What are the main challenges associated with wind power, and how can these be addressed?\",\n",
            "    \"answer\": \"The main challenge associated with wind power is its intermittency, meaning it only generates power when the wind blows. This can be addressed through the use of energy storage solutions, such as large-scale batteries, pumped-hydro storage, and lithium-ion batteries, to ensure a steady supply of electricity.\",\n",
            "    \"sources\": [\n",
            "      \"Understanding Wind Turbines\",\n",
            "      \"Energy Storage Solutions\"\n",
            "    ]\n",
            "  },\n",
            "  {\n",
            "    \"question\": \"Why is effective energy storage crucial for integrating renewable sources like solar and wind into the power grid?\",\n",
            "    \"answer\": \"Effective energy storage is crucial because renewable sources like solar and wind are intermittent, only generating power when conditions allow. Storage allows for storing excess energy when plentiful and releasing it when needed to ensure a steady supply of electricity and a stable, resilient, and reliable power grid.\",\n",
            "    \"sources\": [\n",
            "      \"Understanding Wind Turbines\",\n",
            "      \"Energy Storage Solutions\"\n",
            "    ]\n",
            "  },\n",
            "  {\n",
            "    \"question\": \"What are the different types of large-scale energy storage solutions commonly used today?\",\n",
            "    \"answer\": \"The different types of large-scale energy storage solutions commonly used today are pumped-hydro storage and battery technologies, particularly lithium-ion.\",\n",
            "    \"sources\": [\n",
            "      \"Energy Storage Solutions\"\n",
            "    ]\n",
            "  }\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "def sequential_workflow(content, n_questions=10):\n",
        "    \"\"\"\n",
        "    Execute the complete sequential workflow for FAQ generation.\n",
        "\n",
        "    Args:\n",
        "        content: The combined content from all sources\n",
        "\n",
        "    Returns:\n",
        "        list: A list of FAQs with questions, answers, and sources\n",
        "    \"\"\"\n",
        "    # Generate questions\n",
        "    questions = generate_questions(content, n_questions)\n",
        "\n",
        "    # Answer and find sources for each question sequentially\n",
        "    final_faqs = []\n",
        "    for question in questions:\n",
        "        print(f\"  - Processing: '{question[:50]}...'\")\n",
        "\n",
        "        # Generate an answer for the current question\n",
        "        answer = answer_question(question, content)\n",
        "\n",
        "        # Identify the sources for the generated answer\n",
        "        sources = find_sources(question, answer, content)\n",
        "\n",
        "        final_faqs.append({\"question\": question, \"answer\": answer, \"sources\": sources})\n",
        "\n",
        "    return final_faqs\n",
        "\n",
        "# Execute the sequential workflow (measure time for comparison)\n",
        "start_time = time.time()\n",
        "sequential_faqs = sequential_workflow(combined_content, n_questions=4)\n",
        "end_time = time.time()\n",
        "print(f\"\\nSequential processing completed in {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "# Display the final result\n",
        "print(\"\\nGenerated FAQ List (Sequential):\")\n",
        "print(json.dumps(sequential_faqs, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ps92Komdj7H2"
      },
      "source": [
        "# Parallelization: Improving Efficiency\n",
        "\n",
        "While the sequential workflow works well, we can optimize it by running some steps in parallel. We can generate the answer and find sources simultaneously for all the questions. This can significantly reduce the overall processing time.\n",
        "\n",
        "**Important**: you may meet the rate limits of your account if you do this for a lot of questions. If you go over your rate limits, the API calls will return errors and retry after a timeout. Make sure to take this into account when building real-world products!\n",
        "\n",
        "## Implementing Parallel Processing\n",
        "\n",
        "Let's implement a parallel version of our workflow using Python's `asyncio` library.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "rlbe82L9j7H2"
      },
      "outputs": [],
      "source": [
        "async def answer_question_async(question, content):\n",
        "    \"\"\"\n",
        "    Async version of answer_question function.\n",
        "    \"\"\"\n",
        "    prompt = prompt_answer_question.format(question=question, combined_content=content)\n",
        "    response = await client.aio.models.generate_content(\n",
        "        model=\"gemini-2.5-flash\",\n",
        "        contents=prompt\n",
        "    )\n",
        "    return response.text\n",
        "\n",
        "async def find_sources_async(question, answer, content):\n",
        "    \"\"\"\n",
        "    Async version of find_sources function.\n",
        "    \"\"\"\n",
        "    prompt = prompt_find_sources.format(question=question, answer=answer, combined_content=content)\n",
        "    response = await client.aio.models.generate_content(\n",
        "        model=\"gemini-2.5-flash\",\n",
        "        contents=prompt,\n",
        "        config={\n",
        "            \"response_mime_type\": \"application/json\",\n",
        "            \"response_schema\": SourceList\n",
        "        }\n",
        "    )\n",
        "    return response.parsed.sources\n",
        "\n",
        "async def process_question_parallel(question, content):\n",
        "    \"\"\"\n",
        "    Process a single question by generating answer and finding sources in parallel.\n",
        "    \"\"\"\n",
        "    answer = await answer_question_async(question, content)\n",
        "    sources = await find_sources_async(question, answer, content)\n",
        "    return {\"question\": question, \"answer\": answer, \"sources\": sources}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeDiQIBVj7H2"
      },
      "source": [
        "## Executing the Parallel Workflow\n",
        "\n",
        "Now let's process all questions using parallel execution. We'll process multiple questions concurrently, which can significantly reduce the total processing time. Notice how much time it takes to run the full workflow and compare it with the execution time of the sequential workflow.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "uE5Zjuxyj7H2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e140ce81-a441-4c23-b245-be5210ac2041"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Parallel processing completed in 9.15 seconds\n",
            "\n",
            "Generated FAQ List (Parallel):\n",
            "[\n",
            "  {\n",
            "    \"question\": \"What are the primary environmental and economic benefits of utilizing solar energy?\",\n",
            "    \"answer\": \"The primary environmental benefit of utilizing solar energy is cutting down greenhouse gas emissions by reducing reliance on fossil fuels. The economic benefits include significantly lowering monthly electricity bills, the potential to sell excess power back to the grid, and contributing to national energy independence.\",\n",
            "    \"sources\": [\n",
            "      \"The Benefits of Solar Energy\"\n",
            "    ]\n",
            "  },\n",
            "  {\n",
            "    \"question\": \"What is the main challenge associated with wind energy, and how is this issue typically addressed?\",\n",
            "    \"answer\": \"The main challenge for wind energy is its intermittency, meaning it only generates power when the wind blows. This issue is typically addressed through the use of energy storage solutions, such as large-scale batteries.\",\n",
            "    \"sources\": [\n",
            "      \"Understanding Wind Turbines\",\n",
            "      \"Energy Storage Solutions\"\n",
            "    ]\n",
            "  },\n",
            "  {\n",
            "    \"question\": \"Why is effective energy storage crucial for integrating intermittent renewable sources like solar and wind into the power grid?\",\n",
            "    \"answer\": \"Effective energy storage is crucial because intermittent renewable sources like solar and wind only generate power when conditions are favorable, so storing excess energy when it's plentiful and releasing it when needed ensures a steady supply of electricity and a stable power grid by balancing supply and demand.\",\n",
            "    \"sources\": [\n",
            "      \"Understanding Wind Turbines\",\n",
            "      \"Energy Storage Solutions\"\n",
            "    ]\n",
            "  },\n",
            "  {\n",
            "    \"question\": \"What are the most common and emerging technologies used for large-scale energy storage in conjunction with renewable energy?\",\n",
            "    \"answer\": \"The most common form of large-scale energy storage is pumped-hydro storage. Battery technologies, particularly lithium-ion, are rapidly becoming more affordable and widespread.\",\n",
            "    \"sources\": [\n",
            "      \"Energy Storage Solutions\"\n",
            "    ]\n",
            "  }\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "async def parallel_workflow(content, n_questions=10):\n",
        "    \"\"\"\n",
        "    Execute the complete parallel workflow for FAQ generation.\n",
        "\n",
        "    Args:\n",
        "        content: The combined content from all sources\n",
        "\n",
        "    Returns:\n",
        "        list: A list of FAQs with questions, answers, and sources\n",
        "    \"\"\"\n",
        "    # Generate questions (this step remains sequential)\n",
        "    questions = generate_questions(content, n_questions)\n",
        "\n",
        "    # Process all questions in parallel\n",
        "    tasks = [process_question_parallel(question, content) for question in questions]\n",
        "\n",
        "    # Execute all tasks concurrently\n",
        "    parallel_faqs = await asyncio.gather(*tasks)\n",
        "\n",
        "    return parallel_faqs\n",
        "\n",
        "# Execute the parallel workflow (measure time for comparison)\n",
        "start_time = time.time()\n",
        "parallel_faqs = await parallel_workflow(combined_content, n_questions=4)\n",
        "end_time = time.time()\n",
        "print(f\"\\nParallel processing completed in {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "# Display the final result\n",
        "print(\"\\nGenerated FAQ List (Parallel):\")\n",
        "print(json.dumps(parallel_faqs, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYcKmZVfj7H2"
      },
      "source": [
        "## Sequential vs Parallel: Key Differences\n",
        "\n",
        "The main differences between sequential and parallel approaches:\n",
        "\n",
        "**Sequential Processing:**\n",
        "- Questions are processed one at a time\n",
        "- Predictable execution order\n",
        "- Easier to debug and understand\n",
        "- Higher total processing time\n",
        "\n",
        "**Parallel Processing:**\n",
        "- Multiple questions can be processed simultaneously\n",
        "- Significant reduction in total processing time\n",
        "- More complex error handling\n",
        "- Better resource utilization\n",
        "\n",
        "Both approaches produce the same results, but parallel processing can be significantly faster for larger datasets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKJAjzbk4oMC"
      },
      "source": [
        "# Building a routing workflow\n",
        "\n",
        "Routing is a method that categorizes an input and then sends it to a specific task designed to handle that type of input. This approach helps keep different functions separate and lets you create more specialized prompts. If you don't use routing, trying to optimize for one kind of input might negatively affect how well the system performs with other kinds of inputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0EZ8uunj7H2"
      },
      "source": [
        "## Intent Classification\n",
        "\n",
        "First, we create a classification prompt and function to determine the user's intent. This will help us route the query to the appropriate handler:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZS8GDHYs47En",
        "outputId": "99b9522c-2bd4-4d0c-82d7-a67eed9ee4f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: My internet connection is not working.\n",
            "Intent: IntentEnum.TECHNICAL_SUPPORT\n",
            "\n",
            "Query: I think there is a mistake on my last invoice.\n",
            "Intent: IntentEnum.BILLING_INQUIRY\n",
            "\n",
            "Query: What are your opening hours?\n",
            "Intent: IntentEnum.GENERAL_QUESTION\n",
            "\n"
          ]
        }
      ],
      "source": [
        "class IntentEnum(str, Enum):\n",
        "    \"\"\"\n",
        "    Defines the allowed values for the 'intent' field.\n",
        "    Inheriting from 'str' ensures that the values are treated as strings.\n",
        "    \"\"\"\n",
        "    TECHNICAL_SUPPORT = \"Technical Support\"\n",
        "    BILLING_INQUIRY = \"Billing Inquiry\"\n",
        "    GENERAL_QUESTION = \"General Question\"\n",
        "\n",
        "class UserIntent(BaseModel):\n",
        "    intent: IntentEnum\n",
        "\n",
        "prompt_classification = \"\"\"\n",
        "Classify the user's query into one of the following categories:\n",
        "{categories}\n",
        "\n",
        "Return only the category name and nothing else.\n",
        "\n",
        "User Query: \"{user_query}\"\n",
        "\"\"\".strip()\n",
        "\n",
        "\n",
        "def classify_intent(user_query):\n",
        "    \"\"\"Uses an LLM to classify a user query.\"\"\"\n",
        "    response = client.models.generate_content(\n",
        "        model=\"gemini-2.5-flash\",\n",
        "        contents=prompt_classification.format(\n",
        "            user_query=user_query,\n",
        "            categories=[intent.value for intent in IntentEnum]\n",
        "        ),\n",
        "        config={\n",
        "            \"response_mime_type\": \"application/json\",\n",
        "            \"response_schema\": UserIntent\n",
        "        }\n",
        "    )\n",
        "    return response.parsed.intent\n",
        "\n",
        "\n",
        "query_1 = \"My internet connection is not working.\"\n",
        "query_2 = \"I think there is a mistake on my last invoice.\"\n",
        "query_3 = \"What are your opening hours?\"\n",
        "\n",
        "intent_1 = classify_intent(query_1)\n",
        "print(f\"Query: {query_1}\\nIntent: {intent_1}\\n\")\n",
        "intent_2 = classify_intent(query_2)\n",
        "print(f\"Query: {query_2}\\nIntent: {intent_2}\\n\")\n",
        "intent_3 = classify_intent(query_3)\n",
        "print(f\"Query: {query_3}\\nIntent: {intent_3}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0ob6fP5j7H2"
      },
      "source": [
        "## Defining Specialized Handlers\n",
        "\n",
        "Next, we create specialized prompts for each type of query and a routing function that directs queries to the appropriate handler based on the classified intent:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WBkXZP15f3Z",
        "outputId": "e7c05956-d9ad-4d13-db4c-0cdd62d07e5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: My internet connection is not working.\n",
            "Intent: IntentEnum.TECHNICAL_SUPPORT\n",
            "Response: I'm sorry to hear your internet connection isn't working! That's definitely frustrating.\n",
            "\n",
            "To help me understand what might be going on and avoid suggesting steps you've already tried, could you tell me a bit more about it?\n",
            "\n",
            "For example:\n",
            "*   **What troubleshooting steps have you already attempted** (like restarting your modem/router, checking cables, etc.)?\n",
            "*   Are you seeing any specific error messages, or are there any unusual lights on your modem or router?\n",
            "*   Is it affecting all your devices (computer, phone, tablet), or just one?\n",
            "\n",
            "Query: I think there is a mistake on my last invoice.\n",
            "Intent: IntentEnum.BILLING_INQUIRY\n",
            "Response: \"I understand you're concerned about a mistake on your last invoice. I'd be happy to look into that for you. To access your billing details, could you please provide your account number?\"\n",
            "\n",
            "Query: What are your opening hours?\n",
            "Intent: IntentEnum.GENERAL_QUESTION\n",
            "Response: I'm sorry, I'm not sure how to help with that. Could you please rephrase your question?\n",
            "\n"
          ]
        }
      ],
      "source": [
        "prompt_technical_support = \"\"\"\n",
        "You are a helpful technical support agent.\n",
        "The user says: '{user_query}'.\n",
        "Provide a helpful first response, asking for more details like what troubleshooting steps they have already tried.\n",
        "\"\"\".strip()\n",
        "\n",
        "prompt_billing_inquiry = \"\"\"\n",
        "You are a helpful billing support agent.\n",
        "The user says: '{user_query}'.\n",
        "Acknowledge their concern and inform them that you will need to look up their account, asking for their account number.\n",
        "\"\"\".strip()\n",
        "\n",
        "prompt_general_question = \"\"\"\n",
        "You are a general assistant.\n",
        "The user says: '{user_query}'.\n",
        "Apologize that you are not sure how to help and ask them to rephrase their question.\n",
        "\"\"\".strip()\n",
        "\n",
        "\n",
        "def handle_query(user_query, intent):\n",
        "    \"\"\"Routes a query to the correct handler based on its classified intent.\"\"\"\n",
        "    if intent == IntentEnum.TECHNICAL_SUPPORT:\n",
        "        prompt = prompt_technical_support.format(user_query=user_query)\n",
        "    elif intent == IntentEnum.BILLING_INQUIRY:\n",
        "        prompt = prompt_billing_inquiry.format(user_query=user_query)\n",
        "    elif intent == IntentEnum.GENERAL_QUESTION:\n",
        "        prompt = prompt_general_question.format(user_query=user_query)\n",
        "    else:\n",
        "        prompt = prompt_general_question.format(user_query=user_query)\n",
        "    response = client.models.generate_content(\n",
        "        model=\"gemini-2.5-flash\",\n",
        "        contents=prompt\n",
        "    )\n",
        "    return response.text\n",
        "\n",
        "\n",
        "response_1 = handle_query(query_1, intent_1)\n",
        "print(f\"Query: {query_1}\\nIntent: {intent_1}\\nResponse: {response_1}\\n\")\n",
        "\n",
        "response_2 = handle_query(query_2, intent_2)\n",
        "print(f\"Query: {query_2}\\nIntent: {intent_2}\\nResponse: {response_2}\\n\")\n",
        "\n",
        "response_3 = handle_query(query_3, intent_3)\n",
        "print(f\"Query: {query_3}\\nIntent: {intent_3}\\nResponse: {response_3}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-j7Eg9GC5gaD"
      },
      "source": [
        "# Orchestrator-worker pattern\n",
        "\n",
        "The orchestrator-workers workflow uses a main LLM to dynamically break down complex tasks into smaller subtasks, which are then assigned to other \"worker\" LLMs. The orchestrator LLM also combines the results from these workers.\n",
        "\n",
        "This approach is ideal for complex problems where the specific steps or subtasks can't be known in advance. For instance, in a coding project, the orchestrator can decide which files need modifying and how, based on the initial request. While it might look similar to parallel processing, its key advantage is flexibility: instead of pre-defined subtasks, the orchestrator LLM determines them on the fly according to the given input."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UA9H_2t4j7H2"
      },
      "source": [
        "## Defining the Orchestrator\n",
        "\n",
        "The orchestrator is the central coordinator that breaks down complex user queries into structured JSON tasks. It analyzes the input and identifies what types of actions need to be taken, such as billing inquiries, product returns, or status updates:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "mwUdpaYqX4BH"
      },
      "outputs": [],
      "source": [
        "# Orchestrator\n",
        "class QueryTypeEnum(str, Enum):\n",
        "    BILLING_INQUIRY = \"BillingInquiry\"\n",
        "    PRODUCT_RETURN = \"ProductReturn\"\n",
        "    STATUS_UPDATE = \"StatusUpdate\"\n",
        "\n",
        "class Task(BaseModel):\n",
        "    query_type: str\n",
        "    invoice_number: Optional[str] = None\n",
        "    product_name: Optional[str] = None\n",
        "    reason_for_return: Optional[str] = None\n",
        "    order_id: Optional[str] = None\n",
        "\n",
        "class TaskList(BaseModel):\n",
        "    tasks: List[Task]\n",
        "\n",
        "prompt_orchestrator = f\"\"\"\n",
        "You are a master orchestrator. Your job is to break down a complex user query into a JSON array of objects.\n",
        "Each object represents one sub-task and must have a \"query_type\" and relevant parameters.\n",
        "\n",
        "The possible \"query_type\" values are:\n",
        "1. \"{QueryTypeEnum.BILLING_INQUIRY.value}\": Requires \"invoice_number\".\n",
        "2. \"{QueryTypeEnum.PRODUCT_RETURN.value}\": Requires \"product_name\" and \"reason_for_return\".\n",
        "3. \"{QueryTypeEnum.STATUS_UPDATE.value}\": Requires \"order_id\".\n",
        "\n",
        "User Query:\n",
        "---\n",
        "{{query}}\n",
        "---\n",
        "\"\"\".strip()\n",
        "\n",
        "\n",
        "def orchestrator(query):\n",
        "    \"\"\"Breaks down a complex query into a list of tasks.\"\"\"\n",
        "    response = client.models.generate_content(\n",
        "        model=\"gemini-2.5-flash\",\n",
        "        contents=prompt_orchestrator.format(query=query),\n",
        "        config={\n",
        "            \"response_mime_type\": \"application/json\",\n",
        "            \"response_schema\": TaskList\n",
        "        }\n",
        "    )\n",
        "    return response.parsed.tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQLWg2jZj7H2"
      },
      "source": [
        "## Billing Worker Implementation\n",
        "\n",
        "The billing worker specializes in handling invoice-related inquiries. It extracts the specific concern from the user's query, simulates opening an investigation, and returns structured information about the action taken:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "od132EoqX6MU"
      },
      "outputs": [],
      "source": [
        "# Worker for Billing Inquiry\n",
        "prompt_billing_worker_extractor = \"\"\"\n",
        "You are a specialized assistant. A user has a query regarding invoice '{invoice_number}'.\n",
        "From the full user query provided below, extract the specific concern or question the user has voiced about this particular invoice.\n",
        "Respond with ONLY the extracted concern/question. If no specific concern is mentioned beyond a general inquiry about the invoice, state 'General inquiry regarding the invoice'.\n",
        "\n",
        "Full User Query:\n",
        "---\n",
        "{original_user_query}\n",
        "---\n",
        "\n",
        "Extracted concern about invoice {invoice_number}:\n",
        "\"\"\".strip()\n",
        "\n",
        "\n",
        "def handle_billing_worker(invoice_number, original_user_query):\n",
        "    \"\"\"\n",
        "    Handles a billing inquiry.\n",
        "    1. Uses an LLM to extract the specific concern about the invoice from the original query.\n",
        "    2. Simulates opening an investigation.\n",
        "    3. Returns structured data about the action taken.\n",
        "    \"\"\"\n",
        "    extraction_prompt = prompt_billing_worker_extractor.format(\n",
        "        invoice_number=invoice_number, original_user_query=original_user_query\n",
        "    )\n",
        "    response = client.models.generate_content(model=\"gemini-2.5-flash\", contents=extraction_prompt)\n",
        "    extracted_concern = response.text\n",
        "\n",
        "    # Simulate backend action: opening an investigation\n",
        "    print(f\"  [Billing Worker] Action: Investigating invoice {invoice_number} for concern: '{extracted_concern}'\")\n",
        "    investigation_id = f\"INV_CASE_{random.randint(1000, 9999)}\"\n",
        "    eta_days = 2\n",
        "\n",
        "    return {\n",
        "        \"task\": \"Billing Inquiry\",\n",
        "        \"invoice_number\": invoice_number,\n",
        "        \"user_concern\": extracted_concern,\n",
        "        \"action_taken\": f\"An investigation (Case ID: {investigation_id}) has been opened regarding your concern.\",\n",
        "        \"resolution_eta\": f\"{eta_days} business days\",\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKqsMwiXj7H3"
      },
      "source": [
        "## Product Return Worker\n",
        "\n",
        "The return worker handles product return requests by generating RMA (Return Merchandise Authorization) numbers and providing detailed shipping instructions for customers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "FNIgq12SYGr0"
      },
      "outputs": [],
      "source": [
        "# Worker for Product Return\n",
        "def handle_return_worker(product_name, reason_for_return):\n",
        "    \"\"\"\n",
        "    Handles a product return request.\n",
        "    1. Simulates generating an RMA number and providing return instructions.\n",
        "    2. Returns structured data.\n",
        "    \"\"\"\n",
        "    # Simulate backend action: generating RMA and getting instructions\n",
        "    rma_number = f\"RMA-{random.randint(10000, 99999)}\"\n",
        "    shipping_instructions = (\n",
        "        \"Please pack the '{product_name}' securely in its original packaging if possible. \"\n",
        "        \"Include all accessories and manuals. Write the RMA number ({rma_number}) clearly on the outside of the package. \"\n",
        "        \"Ship to: Returns Department, 123 Automation Lane, Tech City, TC 98765.\"\n",
        "    ).format(product_name=product_name, rma_number=rma_number)\n",
        "    print(f\"  [Return Worker] Action: Generated RMA {rma_number} for {product_name} (Reason: {reason_for_return})\")\n",
        "\n",
        "    return {\n",
        "        \"task\": \"Product Return\",\n",
        "        \"product_name\": product_name,\n",
        "        \"reason_for_return\": reason_for_return,\n",
        "        \"rma_number\": rma_number,\n",
        "        \"shipping_instructions\": shipping_instructions,\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEf-wrHtj7H3"
      },
      "source": [
        "## Order Status Worker\n",
        "\n",
        "The status worker retrieves and formats order status information, including shipping details, tracking numbers, and delivery estimates:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "8KrXP3bJYMNA"
      },
      "outputs": [],
      "source": [
        "# Worker for Status Update\n",
        "def handle_status_worker(order_id):\n",
        "    \"\"\"\n",
        "    Handles an order status update request.\n",
        "    1. Simulates fetching order status from a backend system.\n",
        "    2. Returns structured data.\n",
        "    \"\"\"\n",
        "    # Simulate backend action: fetching order status\n",
        "    # Possible statuses and details to make it more dynamic\n",
        "    possible_statuses = [\n",
        "        {\"status\": \"Processing\", \"carrier\": \"N/A\", \"tracking\": \"N/A\", \"delivery_estimate\": \"3-5 business days\"},\n",
        "        {\n",
        "            \"status\": \"Shipped\",\n",
        "            \"carrier\": \"SuperFast Shipping\",\n",
        "            \"tracking\": f\"SF{random.randint(100000, 999999)}\",\n",
        "            \"delivery_estimate\": \"Tomorrow\",\n",
        "        },\n",
        "        {\n",
        "            \"status\": \"Delivered\",\n",
        "            \"carrier\": \"Local Courier\",\n",
        "            \"tracking\": f\"LC{random.randint(10000, 99999)}\",\n",
        "            \"delivery_estimate\": \"Delivered yesterday\",\n",
        "        },\n",
        "        {\n",
        "            \"status\": \"Delayed\",\n",
        "            \"carrier\": \"Standard Post\",\n",
        "            \"tracking\": f\"SP{random.randint(10000, 99999)}\",\n",
        "            \"delivery_estimate\": \"Expected in 2-3 additional days\",\n",
        "        },\n",
        "    ]\n",
        "    # For a given order_id, we could hash it to pick a status or just pick one randomly for this example\n",
        "    # This ensures that for the same order_id in a single run, we'd get the same fake status if we implement a simple hash.\n",
        "    # For now, let's pick randomly for demonstration.\n",
        "    status_details = random.choice(possible_statuses)\n",
        "    print(f\"  [Status Worker] Action: Fetched status for order {order_id}: {status_details['status']}\")\n",
        "\n",
        "    return {\n",
        "        \"task\": \"Status Update\",\n",
        "        \"order_id\": order_id,\n",
        "        \"current_status\": status_details[\"status\"],\n",
        "        \"carrier\": status_details[\"carrier\"],\n",
        "        \"tracking_number\": status_details[\"tracking\"],\n",
        "        \"expected_delivery\": status_details[\"delivery_estimate\"],\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UM_yBGWWj7IA"
      },
      "source": [
        "## Response Synthesizer\n",
        "\n",
        "The synthesizer takes the structured results from all workers and combines them into a single, coherent, and customer-friendly response message:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "lJBok1WLYUMe"
      },
      "outputs": [],
      "source": [
        "# Synthesizer\n",
        "prompt_synthesizer = \"\"\"\n",
        "You are a master communicator. Combine several distinct pieces of information from our support team into a single, well-formatted, and friendly email to a customer.\n",
        "\n",
        "Here are the points to include, based on the actions taken for their query:\n",
        "---\n",
        "{formatted_results}\n",
        "---\n",
        "\n",
        "Combine these points into one cohesive response. Start with a friendly greeting (e.g., \"Dear Customer,\" or \"Hi there,\") and end with a polite closing (e.g., \"Sincerely,\" or \"Best regards,\").\n",
        "Ensure the tone is helpful and professional.\n",
        "\"\"\".strip()\n",
        "\n",
        "\n",
        "def synthesizer(results):\n",
        "    \"\"\"Combines structured results from workers into a single user-facing message.\"\"\"\n",
        "    bullet_points = []\n",
        "    for res in results:\n",
        "        point = f\"Regarding your {res['task']}:\\n\"\n",
        "        if res[\"task\"] == \"Billing Inquiry\":\n",
        "            point += f\"  - Invoice Number: {res['invoice_number']}\\n\"\n",
        "            point += f'  - Your Stated Concern: \"{res[\"user_concern\"]}\"\\n'\n",
        "            point += f\"  - Our Action: {res['action_taken']}\\n\"\n",
        "            point += f\"  - Expected Resolution: We will get back to you within {res['resolution_eta']}.\"\n",
        "        elif res[\"task\"] == \"ProductReturn\":\n",
        "            point += f\"  - Product: {res['product_name']}\\n\"\n",
        "            point += f'  - Reason for Return: \"{res[\"reason_for_return\"]}\"\\n'\n",
        "            point += f\"  - Return Authorization (RMA): {res['rma_number']}\\n\"\n",
        "            point += f\"  - Instructions: {res['shipping_instructions']}\"\n",
        "        elif res[\"task\"] == \"Status Update\":\n",
        "            point += f\"  - Order ID: {res['order_id']}\\n\"\n",
        "            point += f\"  - Current Status: {res['current_status']}\\n\"\n",
        "            if res[\"carrier\"] != \"N/A\":\n",
        "                point += f\"  - Carrier: {res['carrier']}\\n\"\n",
        "            if res[\"tracking_number\"] != \"N/A\":\n",
        "                point += f\"  - Tracking Number: {res['tracking_number']}\\n\"\n",
        "            point += f\"  - Delivery Estimate: {res['expected_delivery']}\"\n",
        "        bullet_points.append(point)\n",
        "\n",
        "    formatted_results = \"\\n\\n\".join(bullet_points)\n",
        "    prompt = prompt_synthesizer.format(formatted_results=formatted_results)\n",
        "    response = client.models.generate_content(model=\"gemini-2.5-flash\", contents=prompt)\n",
        "    return response.text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAUQR4ONj7IA"
      },
      "source": [
        "## Main Orchestrator-Worker Pipeline\n",
        "\n",
        "This function coordinates the entire orchestrator-worker workflow: it runs the orchestrator to break down the query, dispatches the appropriate workers, and synthesizes the final response:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "2BO01w-xXR8q"
      },
      "outputs": [],
      "source": [
        "def process_user_query(user_query):\n",
        "    \"\"\"Processes a query using the Orchestrator-Worker-Synthesizer pattern.\"\"\"\n",
        "\n",
        "    print(f\"User query:\\n---\\n{user_query}\\n---\")\n",
        "\n",
        "    # 1. Run orchestrator\n",
        "    tasks_list = orchestrator(user_query)\n",
        "    if not tasks_list:\n",
        "        print(\"\\nOrchestrator did not return any tasks. Exiting.\")\n",
        "        return\n",
        "\n",
        "    print(\"\\nDeconstructed tasks from Orchestrator:\")\n",
        "    for task in tasks_list:\n",
        "        print(task.model_dump_json(indent=2))\n",
        "\n",
        "    # 2. Run workers\n",
        "    worker_results = []\n",
        "    if tasks_list:\n",
        "        print(f\"\\nDispatching {len(tasks_list)} workers...\")\n",
        "        for task in tasks_list:\n",
        "            if task.query_type == QueryTypeEnum.BILLING_INQUIRY:\n",
        "                worker_results.append(handle_billing_worker(task.invoice_number, user_query))\n",
        "            elif task.query_type == QueryTypeEnum.PRODUCT_RETURN:\n",
        "                # Ensure reason_for_return is present, provide a default if not (though orchestrator should capture it)\n",
        "                worker_results.append(handle_return_worker(task.product_name, task.reason_for_return))\n",
        "            elif task.query_type == QueryTypeEnum.STATUS_UPDATE:\n",
        "                worker_results.append(handle_status_worker(task.order_id))\n",
        "            else:\n",
        "                print(f\"Warning: Unknown query_type '{task.query_type}' found in orchestrator tasks.\")\n",
        "\n",
        "        if worker_results:\n",
        "            print(f\"Ran {len(worker_results)} workers sequentially.\")\n",
        "            print(\"\\nWorkers finished their jobs. Results:\")\n",
        "            for i, res in enumerate(worker_results):\n",
        "                print(f\"--- Worker Result {i + 1} ---\")\n",
        "                print(json.dumps(res, indent=2))\n",
        "                print(\"----------------------\")\n",
        "        else:\n",
        "            print(\"\\nNo valid worker tasks to run.\")\n",
        "    else:\n",
        "        print(\"\\nNo tasks to run for workers.\")\n",
        "\n",
        "    # 3. Run synthesizer\n",
        "    if worker_results:\n",
        "        print(\"\\nSynthesizing final response...\")\n",
        "        final_user_message = synthesizer(worker_results)\n",
        "        print(\"\\n--- Final Synthesized Response ---\")\n",
        "        print(final_user_message)\n",
        "        print(\"---------------------------------\")\n",
        "    else:\n",
        "        print(\"\\nSkipping synthesis because there were no worker results.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVNNvD4ej7IA"
      },
      "source": [
        "## Testing the Complete Workflow\n",
        "\n",
        "Let's test our orchestrator-worker pattern with a complex customer query that involves multiple tasks: a billing inquiry, a product return, and an order status update:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTHwtUtqYxuK",
        "outputId": "8303ad98-d356-487d-d580-21d2f0293be0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User query:\n",
            "---\n",
            "Hi, I'm writing to you because I have a question about invoice #INV-7890. It seems higher than I expected.\n",
            "Also, I would like to return the 'SuperWidget 5000' I bought because it's not compatible with my system.\n",
            "Finally, can you give me an update on my order #A-12345?\n",
            "---\n",
            "\n",
            "Deconstructed tasks from Orchestrator:\n",
            "{\n",
            "  \"query_type\": \"BillingInquiry\",\n",
            "  \"invoice_number\": \"INV-7890\",\n",
            "  \"product_name\": null,\n",
            "  \"reason_for_return\": null,\n",
            "  \"order_id\": null\n",
            "}\n",
            "{\n",
            "  \"query_type\": \"ProductReturn\",\n",
            "  \"invoice_number\": null,\n",
            "  \"product_name\": \"SuperWidget 5000\",\n",
            "  \"reason_for_return\": \"not compatible with my system\",\n",
            "  \"order_id\": null\n",
            "}\n",
            "{\n",
            "  \"query_type\": \"StatusUpdate\",\n",
            "  \"invoice_number\": null,\n",
            "  \"product_name\": null,\n",
            "  \"reason_for_return\": null,\n",
            "  \"order_id\": \"A-12345\"\n",
            "}\n",
            "\n",
            "Dispatching 3 workers...\n",
            "  [Billing Worker] Action: Investigating invoice INV-7890 for concern: 'It seems higher than I expected.'\n",
            "  [Return Worker] Action: Generated RMA RMA-91987 for SuperWidget 5000 (Reason: not compatible with my system)\n",
            "  [Status Worker] Action: Fetched status for order A-12345: Processing\n",
            "Ran 3 workers sequentially.\n",
            "\n",
            "Workers finished their jobs. Results:\n",
            "--- Worker Result 1 ---\n",
            "{\n",
            "  \"task\": \"Billing Inquiry\",\n",
            "  \"invoice_number\": \"INV-7890\",\n",
            "  \"user_concern\": \"It seems higher than I expected.\",\n",
            "  \"action_taken\": \"An investigation (Case ID: INV_CASE_2400) has been opened regarding your concern.\",\n",
            "  \"resolution_eta\": \"2 business days\"\n",
            "}\n",
            "----------------------\n",
            "--- Worker Result 2 ---\n",
            "{\n",
            "  \"task\": \"Product Return\",\n",
            "  \"product_name\": \"SuperWidget 5000\",\n",
            "  \"reason_for_return\": \"not compatible with my system\",\n",
            "  \"rma_number\": \"RMA-91987\",\n",
            "  \"shipping_instructions\": \"Please pack the 'SuperWidget 5000' securely in its original packaging if possible. Include all accessories and manuals. Write the RMA number (RMA-91987) clearly on the outside of the package. Ship to: Returns Department, 123 Automation Lane, Tech City, TC 98765.\"\n",
            "}\n",
            "----------------------\n",
            "--- Worker Result 3 ---\n",
            "{\n",
            "  \"task\": \"Status Update\",\n",
            "  \"order_id\": \"A-12345\",\n",
            "  \"current_status\": \"Processing\",\n",
            "  \"carrier\": \"N/A\",\n",
            "  \"tracking_number\": \"N/A\",\n",
            "  \"expected_delivery\": \"3-5 business days\"\n",
            "}\n",
            "----------------------\n",
            "\n",
            "Synthesizing final response...\n",
            "\n",
            "--- Final Synthesized Response ---\n",
            "Hi there,\n",
            "\n",
            "Thank you for reaching out to us. We're happy to provide you with an update on your recent inquiries.\n",
            "\n",
            "**Regarding your Billing Inquiry (Invoice INV-7890):**\n",
            "We understand your concern that your invoice seems higher than expected. We've initiated a thorough investigation into this for you (Case ID: INV_CASE_2400). Our team is actively looking into the details, and we anticipate getting back to you with a resolution within **2 business days**.\n",
            "\n",
            "**Regarding your Order Status Update (Order ID: A-12345):**\n",
            "We're pleased to confirm that your order is currently **processing**. You can expect delivery within **3-5 business days**.\n",
            "\n",
            "We hope this update is helpful. If you have any further questions in the meantime, please don't hesitate to reply to this email.\n",
            "\n",
            "Best regards,\n",
            "\n",
            "The [Your Company/Support Team Name] Team\n",
            "---------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Test with customer query\n",
        "complex_customer_query = \"\"\"\n",
        "Hi, I'm writing to you because I have a question about invoice #INV-7890. It seems higher than I expected.\n",
        "Also, I would like to return the 'SuperWidget 5000' I bought because it's not compatible with my system.\n",
        "Finally, can you give me an update on my order #A-12345?\n",
        "\"\"\".strip()\n",
        "\n",
        "process_user_query(complex_customer_query)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}