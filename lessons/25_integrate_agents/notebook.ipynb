{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "564e6695",
      "metadata": {},
      "source": [
        "# Lesson 25: Integrating Multiple AI Agents with MCP\n",
        "\n",
        "Throughout the course, we've built two agents: Nova for research and Brown for writing. Now it's time to integrate them into a unified system. In this lesson, we will explore how to use both the Nova research agent and the Brown writing workflow together by leveraging the Model Context Protocol (MCP).\n",
        "\n",
        "We've already seen how each agent works as an MCP server in previous lessons. Nova exposes 11 tools for research tasks, while Brown provides 3 tools for article generation and editing. The beauty of MCP is that it makes integration straightforward. We'll explore two approaches:\n",
        "\n",
        "1. **Multi-Server MCP Client**: A single MCP client that connects to multiple independent MCP servers.\n",
        "2. **Composed MCP Server**: A single MCP client that connects to a single MCP server that composes multiple MCP servers together using FastMCP's composition features.\n",
        "\n",
        "Both approaches have their use cases, and by the end of this lesson, you'll understand when to use each one.\n",
        "\n",
        "Learning objectives:\n",
        "- Learn how to connect an MCP client to multiple MCP servers simultaneously\n",
        "- Understand how to use FastMCP's composition features to create a unified MCP server\n",
        "- Compare multi-server client vs composed server approaches\n",
        "- See the practical benefits of MCP for agent integration"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec74cbeb",
      "metadata": {},
      "source": [
        "## 1. Setup\n",
        "\n",
        "First, we define some standard Magic Python commands to autoreload Python packages whenever they change:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "24fcc3b3",
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81250056",
      "metadata": {},
      "source": [
        "### Import Key Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "da957b6e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply() # Allow nested async usage in notebooks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3ff4c59",
      "metadata": {},
      "source": [
        "## 2. Understanding Multi-Agent Orchestration: The MCP Approach\n",
        "\n",
        "Before we dive into the technical implementation, let's understand the orchestration model we're using and why it matters.\n",
        "\n",
        "### The Central LLM Orchestration Pattern\n",
        "\n",
        "In this lesson, we're implementing what's known as a **Central LLM Orchestration** pattern. In this approach, a single, central LLM (e.g. the one powering your IDE assistant, like Claude in Cursor) has access to tools from multiple specialized agents. When you give it a task, the LLM dynamically decides which agent's tools to use based on the task requirements, maintaining a single conversation context and orchestrating the workflow by selecting the appropriate tools as needed.\n",
        "\n",
        "This is fundamentally different from other orchestration patterns. A **Supervisor Agent** pattern would have one agent explicitly delegate entire sub-tasks to worker agents. A **Sequential Pipeline** would force agents to always execute in a fixed order. **Peer-to-Peer Communication** would allow agents to directly message each other. Our central LLM orchestration is simpler: the LLM acts as a intelligent tool selector rather than an explicit coordinator.\n",
        "\n",
        "We'll learn more about these other patterns in another lesson.\n",
        "\n",
        "### Why This Pattern Works Well with MCP\n",
        "\n",
        "The Model Context Protocol makes this orchestration pattern particularly elegant. Both Nova and Brown expose their capabilities as MCP tools with clear descriptions, allowing the central LLM to discover all available tools through a single, standardized protocol. This unified tool interface eliminates the need for custom integration code: because both agents speak MCP, we don't need to write adapters or APIs. The client simply connects to both servers and aggregates their tools.\n",
        "\n",
        "The LLM's natural reasoning ability handles the orchestration logic. For example, it intuitively understands that it should use Nova's research tools first, review the results, and then use Brown's writing tools with that research as input. There's no need to explicitly program this workflow or create complex state machines.\n",
        "\n",
        "The flexibility is another key advantage. You can easily add or remove agents by simply updating the configuration file without touching any code. This pattern essentially treats specialized agents as \"tool libraries\" that a central reasoning engine can draw from as needed, making the system both modular and maintainable."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d263e349",
      "metadata": {},
      "source": [
        "### The Rationale: Why Choose Central LLM Orchestration?\n",
        "\n",
        "This orchestration pattern offers several compelling advantages that make it ideal for our use case of integrating Nova and Brown.\n",
        "\n",
        "The most immediate benefit is **simplicity and maintainability**. All decision-making happens in one place—the central LLM, which means the workflow is transparent and easy to understand. You can see which tools the LLM chooses in real-time as it works through a task. There's no need for complex state management or inter-agent communication protocols, which reduces the cognitive overhead of understanding and debugging the system.\n",
        "\n",
        "The pattern also enables **natural task decomposition**. The central LLM can break down complex requests on the fly without requiring predefined workflows. Even more importantly, it can adapt its strategy based on intermediate results. For example, if research reveals unexpected information, the LLM can adjust its writing approach accordingly. This adaptive behavior is especially valuable for human-in-the-loop workflows common in IDE environments. You can provide feedback at any point in the process, and the LLM incorporates it naturally without requiring explicit handoff protocols.\n",
        "\n",
        "The central LLM maintains a **single, unified context window**, which means it can reference information from Nova's research when calling Brown's tools without requiring explicit data passing between agents. This avoids the notorious \"siloed knowledge\" problem, where critical information gets trapped in one agent's context and becomes unavailable to others who need it.\n",
        "\n",
        "Perhaps most importantly for our specific use case, this pattern is **perfect for sequential, interdependent tasks**. Our workflow (research followed by writing) is inherently sequential, and the writing task depends heavily on research results. A central orchestrator can easily manage these dependencies because it sees the entire workflow and can make informed decisions about when to transition from research to writing."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3aefe0c",
      "metadata": {},
      "source": [
        "### The Tradeoffs: Understanding the Limitations\n",
        "\n",
        "While central LLM orchestration with MCP is powerful, it's important to understand its limitations and recognize when you might need a different approach.\n",
        "\n",
        "The first limitation arises from **tool overload**. As the number of available tools grows beyond approximately 15-20, LLMs begin to struggle with reliable tool selection. They may choose suboptimal tools or miss relevant ones entirely. This degradation in performance is well-documented in the research on agent systems. Our system, with 14 tools total, is comfortably within the limit, but if you were to add many more specialized agents, you'd eventually hit this ceiling and need to consider a different pattern.\n",
        "\n",
        "Another significant limitation is the pattern's inherently **sequential execution model**. If you need to research 50 companies simultaneously, a single LLM executing tools one at a time would be painfully inefficient. In such scenarios, a Supervisor-Worker pattern with parallel execution would be better.\n",
        "\n",
        "The pattern also struggles with **complex inter-agent dependencies**. If agents need to negotiate with each other, engage in debate, or iteratively refine each other's work through back-and-forth exchanges, direct agent-to-agent communication would be more natural. Our pattern handles simple, linear dependencies well (Nova's output feeds into Brown) but complex multi-way interactions would become awkward and difficult to manage.\n",
        "\n",
        "However, central LLM orchestration with MCP is the **right default choice** for most agent integration scenarios. It's simple, maintainable, and leverages the LLM's natural reasoning abilities without adding unnecessary complexity. You should only consider more elaborate patterns when you hit clear scaling limits (e.g. too many tools causing selection problems) or have fundamentally different requirements like massive parallelism or complex agent negotiations.\n",
        "\n",
        "We'll learn more about these other patterns in another lesson.\n",
        "\n",
        "### Integrating Nova and Brown\n",
        "\n",
        "In the previous lessons, we built two specialized agents: Nova for research (which ingests article guidelines, performs web research, scrapes sources, and compiles comprehensive research files) and Brown for writing (which takes research and guidelines to generate, review, and edit articles with human-in-the-loop feedback). These agents were designed to work in sequence—Nova gathers the research, and Brown uses that research to write the article—but we've been running them separately. Now we'll orchestrate them together using the central LLM orchestration pattern. Because both agents are already exposed as MCP servers, integration is straightforward: we simply connect to both servers and let the central LLM decide which tools to use and when.\n",
        "\n",
        "We'll explore two different approaches for achieving this integration.\n",
        "1. The first is a **Multi-Server MCP Client**, where a single client connects directly to multiple independent MCP servers simultaneously—the client aggregates all tools from both servers and presents them to the LLM.\n",
        "2. The second is a **Composed MCP Server**, where we create a new server that internally connects to both Nova and Brown, exposing their combined capabilities as a single unified endpoint.\n",
        "\n",
        "Both approaches achieve the same goal but differ in their deployment and configuration patterns."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8450a342",
      "metadata": {},
      "source": [
        "## 3. Approach 1: Multi-Server MCP Client\n",
        "\n",
        "The first approach is to create an MCP client that connects to multiple MCP servers simultaneously. FastMCP's `Client` class supports this out of the box by accepting a configuration object that specifies multiple servers."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e1416b6",
      "metadata": {},
      "source": [
        "### 3.1 Multi-Server Configuration File\n",
        "\n",
        "Let's look at the configuration file that defines both Nova and Brown servers.\n",
        "\n",
        "Source: _lessons/agents_integration/mcp_client/mcp_servers_config.json_\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"mcpServers\": {\n",
        "    \"nova-research-agent\": {\n",
        "      \"transport\": \"stdio\",\n",
        "      \"command\": \"uv\",\n",
        "      \"args\": [\n",
        "        \"--directory\",\n",
        "        \"/absolute/path/to/research_agent_part_2/mcp_server\",\n",
        "        \"run\",\n",
        "        \"-m\",\n",
        "        \"src.server\",\n",
        "        \"--transport\",\n",
        "        \"stdio\"\n",
        "      ]\n",
        "    },\n",
        "    \"brown-writing-workflow\": {\n",
        "      \"transport\": \"stdio\",\n",
        "      \"command\": \"uv\",\n",
        "      \"args\": [\n",
        "        \"--directory\",\n",
        "        \"/absolute/path/to/writing_workflow\",\n",
        "        \"run\",\n",
        "        \"python\",\n",
        "        \"-m\",\n",
        "        \"brown.mcp.server\"\n",
        "      ]\n",
        "    }\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "This configuration tells the MCP client how to launch both servers. Each server:\n",
        "- Has a unique name (`nova-research-agent`, `brown-writing-workflow`)\n",
        "- Uses the `stdio` transport (communicates via stdin/stdout)\n",
        "- Specifies the command and arguments to start the server"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90964e2b",
      "metadata": {},
      "source": [
        "### 3.2 Creating the Multi-Server Client\n",
        "\n",
        "Now let's see how the client code loads this configuration and connects to both servers.\n",
        "\n",
        "Source: _lessons/agents_integration/mcp_client/src/client.py_\n",
        "\n",
        "```python\n",
        "import json\n",
        "from pathlib import Path\n",
        "from fastmcp import Client\n",
        "\n",
        "# Load configuration from JSON file\n",
        "config_path = Path(\"mcp_servers_config.json\")\n",
        "with open(config_path) as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "server_names = list(config[\"mcpServers\"].keys())\n",
        "logging.info(f\"Found {len(server_names)} MCP servers in configuration: {', '.join(server_names)}\")\n",
        "\n",
        "# Create a single client with multi-server configuration\n",
        "client = Client(config)\n",
        "\n",
        "# Connect and fetch capabilities from all servers\n",
        "async with client:\n",
        "    tools = await client.list_tools()\n",
        "    resources = await client.list_resources()\n",
        "    prompts = await client.list_prompts()\n",
        "    \n",
        "    logging.info(\n",
        "        f\"Total capabilities: {len(tools)} tools, {len(resources)} resources, {len(prompts)} prompts\"\n",
        "    )\n",
        "```\n",
        "\n",
        "The key insight here is that `Client(config)` accepts a multi-server configuration. When you call `list_tools()`, `list_resources()`, or `list_prompts()`, the client automatically aggregates capabilities from all connected servers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b510072a",
      "metadata": {},
      "source": [
        "### 3.3 How Capabilities Are Named\n",
        "\n",
        "When you have multiple servers, how do you distinguish which tool belongs to which server? FastMCP handles this by prefixing the tool names with the server name.\n",
        "\n",
        "For example:\n",
        "- Nova's `extract_guidelines_urls` tool becomes `nova-research-agent_extract_guidelines_urls`\n",
        "- Brown's `generate_article` tool becomes `brown-writing-workflow_generate_article`\n",
        "\n",
        "The client code groups capabilities by extracting these prefixes:\n",
        "\n",
        "Source: _lessons/agents_integration/mcp_client/src/utils/command_utils.py_\n",
        "\n",
        "```python\n",
        "def handle_command(processed_input, tools, resources, prompts, server_names):\n",
        "    # Extract server prefixes from tool names\n",
        "    server_prefixes = set()\n",
        "    for tool in tools:\n",
        "        if \"_\" in tool.name:\n",
        "            prefix = tool.name.split(\"_\")[0]\n",
        "            server_prefixes.add(prefix)\n",
        "    \n",
        "    # Group tools by prefix\n",
        "    if processed_input.input_type == InputType.COMMAND_INFO_TOOLS:\n",
        "        for prefix in sorted(server_prefixes):\n",
        "            prefix_tools = [t for t in tools if t.name.startswith(f\"{prefix}_\")]\n",
        "            if prefix_tools:\n",
        "                print_header(f\"{prefix} - Tools ({len(prefix_tools)})\")\n",
        "                for i, tool in enumerate(prefix_tools, 1):\n",
        "                    print_item(tool.name, tool.description, i)\n",
        "```\n",
        "\n",
        "This approach makes it easy to see which capabilities come from which server.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa268db3",
      "metadata": {},
      "source": [
        "### 3.4 Running the Multi-Server Client\n",
        "\n",
        "When you run the multi-server client, you'll see output like this:\n",
        "\n",
        "```bash\n",
        "$ cd agents_integration/mcp_client\n",
        "$ uv run -m src.client\n",
        "```\n",
        "\n",
        "```terminal\n",
        "INFO:root:Loading MCP server configuration from: mcp_servers_config.json\n",
        "INFO:root:Found 2 MCP servers in configuration: nova-research-agent, brown-writing-workflow\n",
        "INFO:root:Connecting to MCP servers...\n",
        "INFO:root:Fetching capabilities from all servers...\n",
        "INFO:root:Total capabilities: 14 tools, 4 resources, 4 prompts\n",
        "\n",
        "============================================================\n",
        "Brown Writing Workflow\n",
        "============================================================\n",
        "\n",
        "  - 3 tools available\n",
        "  - 2 resources available\n",
        "  - 3 prompts available\n",
        "\n",
        "============================================================\n",
        "Nova Research Agent\n",
        "============================================================\n",
        "\n",
        "  - 11 tools available\n",
        "  - 2 resources available\n",
        "  - 1 prompts available\n",
        "\n",
        "Available Commands: /tools, /resources, /prompts, /quit\n",
        "\n",
        ">\n",
        "```\n",
        "\n",
        "When you type `/tools`, you'll see all tools from both servers:\n",
        "\n",
        "```terminal\n",
        "============================================================\n",
        "brown-writing-workflow - Tools (3)\n",
        "============================================================\n",
        "\n",
        "1. brown-writing-workflow_generate_article\n",
        "   Generate an article from scratch using Brown's article generation workflow.\n",
        "\n",
        "2. brown-writing-workflow_edit_article\n",
        "   Edit an entire article based on human feedback and expected requirements.\n",
        "\n",
        "3. brown-writing-workflow_edit_selected_text\n",
        "   Edit a selected section of an article based on human feedback.\n",
        "\n",
        "============================================================\n",
        "nova-research-agent - Tools (11)\n",
        "============================================================\n",
        "\n",
        "1. nova-research-agent_extract_guidelines_urls\n",
        "   Extract URLs and local file references from article guidelines.\n",
        "\n",
        "2. nova-research-agent_process_local_files\n",
        "   Process local files referenced in the article guidelines.\n",
        "\n",
        "3. nova-research-agent_scrape_and_clean_other_urls\n",
        "   Scrape and clean other URLs from GUIDELINES_FILENAMES_FILE.\n",
        "\n",
        "... (and 8 more tools)\n",
        "```\n",
        "\n",
        "This demonstrates that the client successfully connected to both servers and aggregated their capabilities.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b5c75d1",
      "metadata": {},
      "source": [
        "### 3.5 Running the Multi-Server Client in the Notebook\n",
        "\n",
        "Now let's run the multi-server client directly from this notebook. The client will connect to both Nova and Brown servers and display their capabilities.\n",
        "\n",
        "*Note*: The client is interactive, so you can type commands like `/tools`, `/resources`, `/prompts`, or `/quit` when prompted. Type `/quit` to exit the client."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3da31d0",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "from agents_integration.mcp_client.src.client import main as client_main\n",
        "\n",
        "\n",
        "async def run_client():\n",
        "    _argv_backup = sys.argv[:]\n",
        "    sys.argv = [\"client\", \"--config\", \"mcp_servers_config.json\"]\n",
        "    try:\n",
        "        await client_main()\n",
        "    finally:\n",
        "        sys.argv = _argv_backup\n",
        "\n",
        "\n",
        "# Start client with in-memory server\n",
        "await run_client()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8080511",
      "metadata": {},
      "source": [
        "After running this cell, you should see:\n",
        "1. Both servers starting up (Nova Research MCP Server and Brown MCP Server)\n",
        "2. The total capabilities summary (14 tools, 4 resources, 4 prompts)\n",
        "3. A welcome message for each server showing their individual capabilities\n",
        "4. An interactive prompt where you can type commands\n",
        "\n",
        "Try typing:\n",
        "- `/tools` to see all tools from both servers\n",
        "- `/resources` to see all resources\n",
        "- `/prompts` to see all prompts\n",
        "- `/quit` to exit"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a13136f6",
      "metadata": {},
      "source": [
        "## 4. Approach 2: Composed MCP Server\n",
        "\n",
        "The second approach is to create a new MCP server that composes the Nova and Brown servers together. Instead of the client connecting to multiple servers, you create a single composed server that internally proxies requests to the underlying servers.\n",
        "\n",
        "This approach is useful when you want to:\n",
        "- Package multiple agents as a single deployable unit\n",
        "- Simplify the client-side configuration (client only needs to know about one server)\n",
        "- Add a layer of coordination or orchestration between agents\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71c5c484",
      "metadata": {},
      "source": [
        "### 4.1 Server Composition Configuration\n",
        "\n",
        "First, we define which servers to compose:\n",
        "\n",
        "Source: _lessons/agents_integration/mcp_server/mcp_servers_to_compose.json_\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"mcpServers\": {\n",
        "    \"nova-research-agent\": {\n",
        "      \"transport\": \"stdio\",\n",
        "      \"command\": \"uv\",\n",
        "      \"args\": [\n",
        "        \"--directory\",\n",
        "        \"/absolute/path/to/research_agent_part_2/mcp_server\",\n",
        "        \"run\",\n",
        "        \"-m\",\n",
        "        \"src.server\",\n",
        "        \"--transport\",\n",
        "        \"stdio\"\n",
        "      ]\n",
        "    },\n",
        "    \"brown-writing-workflow\": {\n",
        "      \"transport\": \"stdio\",\n",
        "      \"command\": \"uv\",\n",
        "      \"args\": [\n",
        "        \"--directory\",\n",
        "        \"/absolute/path/to/writing_workflow\",\n",
        "        \"run\",\n",
        "        \"python\",\n",
        "        \"-m\",\n",
        "        \"brown.mcp.server\"\n",
        "      ]\n",
        "    }\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "This looks identical to the multi-server client config, but it's used differently.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca45bcd4",
      "metadata": {},
      "source": [
        "### 4.2 Creating the Composed Server\n",
        "\n",
        "Now let's see how to create a composed server using FastMCP's composition features.\n",
        "\n",
        "Source: _lessons/agents_integration/mcp_server/src/main.py_\n",
        "\n",
        "```python\n",
        "import json\n",
        "import logging\n",
        "from pathlib import Path\n",
        "from fastmcp import Client, FastMCP\n",
        "\n",
        "def load_server_config() -> dict:\n",
        "    \"\"\"Load the MCP servers configuration from JSON file.\"\"\"\n",
        "    config_path = Path(__file__).parent.parent / \"mcp_servers_to_compose.json\"\n",
        "    with open(config_path) as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def create_composed_server() -> FastMCP:\n",
        "    \"\"\"Create a composed MCP server by mounting Nova and Brown servers.\"\"\"\n",
        "    # Create the main composed server\n",
        "    mcp = FastMCP(\n",
        "        name=\"Nova+Brown Composed Server\",\n",
        "        version=\"0.1.0\",\n",
        "    )\n",
        "    \n",
        "    # Load configuration\n",
        "    config = load_server_config()\n",
        "    servers_config = config.get(\"mcpServers\", {})\n",
        "    \n",
        "    # Create proxies and mount each server\n",
        "    for server_name, server_config in servers_config.items():\n",
        "        # Wrap the server config in the structure expected by Client\n",
        "        client_config = {\"mcpServers\": {server_name: server_config}}\n",
        "        \n",
        "        # Create a client for this server\n",
        "        client = Client(client_config)\n",
        "        \n",
        "        # Create a proxy from the client\n",
        "        proxy = FastMCP.as_proxy(client)\n",
        "        \n",
        "        # Extract prefix: nova-research-agent -> nova\n",
        "        prefix = server_name.split(\"-\")[0]\n",
        "        \n",
        "        # Mount the proxy with the prefix\n",
        "        mcp.mount(proxy, prefix=prefix)\n",
        "    \n",
        "    return mcp\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    composed_server = create_composed_server()\n",
        "    composed_server.run()\n",
        "```\n",
        "\n",
        "Let's break down the key steps:\n",
        "\n",
        "1. First we create a FastMCP instance. This is our composed server.\n",
        "2. For each server to compose:\n",
        "   - Create a `Client` that connects to that server\n",
        "   - Use `FastMCP.as_proxy(client)` to create a proxy object\n",
        "   - Use `mcp.mount(proxy, prefix=prefix)` to mount it with a prefix\n",
        "\n",
        "The `mount()` method is the magic here. It takes all capabilities from the proxy and adds them to the composed server with the specified prefix. This is how `extract_guidelines_urls` becomes `nova_extract_guidelines_urls`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb22fe33",
      "metadata": {},
      "source": [
        "### 4.3 Running the Composed Server\n",
        "\n",
        "To use the composed server, you need a client config that points to it:\n",
        "\n",
        "Source: _lessons/agents_integration/mcp_client/mcp_composed_server_config.json_\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"mcpServers\": {\n",
        "    \"nova-brown-composed\": {\n",
        "      \"transport\": \"stdio\",\n",
        "      \"command\": \"uv\",\n",
        "      \"args\": [\n",
        "        \"--directory\",\n",
        "        \"/absolute/path/to/agents_integration/mcp_server\",\n",
        "        \"run\",\n",
        "        \"python\",\n",
        "        \"-m\",\n",
        "        \"src.main\"\n",
        "      ]\n",
        "    }\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "Now when you run the client with this config:\n",
        "\n",
        "```bash\n",
        "$ cd agents_integration/mcp_client\n",
        "$ uv run -m src.client --config mcp_composed_server_config.json\n",
        "```\n",
        "\n",
        "You'll see:\n",
        "\n",
        "```terminal\n",
        "INFO:root:Loading MCP server configuration from: mcp_composed_server_config.json\n",
        "INFO:root:Found 1 MCP servers in configuration: nova-brown-composed\n",
        "INFO:root:Connecting to MCP servers...\n",
        "INFO:__main__:Starting composed MCP server...\n",
        "INFO:__main__:Loading server configuration...\n",
        "INFO:__main__:Found 2 servers to compose: ['nova-research-agent', 'brown-writing-workflow']\n",
        "INFO:__main__:Creating proxy for nova-research-agent...\n",
        "INFO:__main__:Mounting nova-research-agent with prefix 'nova'...\n",
        "INFO:__main__:Creating proxy for brown-writing-workflow...\n",
        "INFO:__main__:Mounting brown-writing-workflow with prefix 'brown'...\n",
        "INFO:__main__:Composed server created successfully!\n",
        "INFO:__main__:Running composed server...\n",
        "\n",
        "INFO:root:Fetching capabilities from all servers...\n",
        "INFO:root:Total capabilities: 14 tools, 4 resources, 4 prompts\n",
        "\n",
        "============================================================\n",
        "Brown\n",
        "============================================================\n",
        "\n",
        "  - 3 tools available\n",
        "  - 2 resources available\n",
        "  - 3 prompts available\n",
        "\n",
        "============================================================\n",
        "Nova\n",
        "============================================================\n",
        "\n",
        "  - 11 tools available\n",
        "  - 2 resources available\n",
        "  - 1 prompts available\n",
        "\n",
        "Available Commands: /tools, /resources, /prompts, /quit\n",
        "```\n",
        "\n",
        "Notice the difference in prefixes:\n",
        "- Multi-server client: `nova-research-agent_`, `brown-writing-workflow_`\n",
        "- Composed server: `nova_`, `brown_`\n",
        "\n",
        "This is because the composed server uses cleaner prefixes specified in the `mount()` call."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd476dd4",
      "metadata": {},
      "source": [
        "### 4.4 Listing Tools from the Composed Server\n",
        "\n",
        "When you type `/tools`, you'll see:\n",
        "\n",
        "```terminal\n",
        "============================================================\n",
        "brown - Tools (3)\n",
        "============================================================\n",
        "\n",
        "1. brown_generate_article\n",
        "   Generate an article from scratch using Brown's article generation workflow.\n",
        "\n",
        "2. brown_edit_article\n",
        "   Edit an entire article based on human feedback.\n",
        "\n",
        "3. brown_edit_selected_text\n",
        "   Edit a selected section of an article based on human feedback.\n",
        "\n",
        "============================================================\n",
        "nova - Tools (11)\n",
        "============================================================\n",
        "\n",
        "1. nova_extract_guidelines_urls\n",
        "   Extract URLs and local file references from article guidelines.\n",
        "\n",
        "2. nova_process_local_files\n",
        "   Process local files referenced in the article guidelines.\n",
        "\n",
        "3. nova_scrape_and_clean_other_urls\n",
        "   Scrape and clean other URLs from GUIDELINES_FILENAMES_FILE.\n",
        "\n",
        "... (and 8 more tools)\n",
        "```\n",
        "\n",
        "From the client's perspective, it's connecting to a single server (`nova-brown-composed`), but that server internally proxies to both Nova and Brown.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c351f5a2",
      "metadata": {},
      "source": [
        "### 4.5 Running the Composed Server Client in the Notebook\n",
        "\n",
        "Now let's run the client with the composed server configuration. This time, the client connects to a single composed server that internally proxies to both Nova and Brown.\n",
        "\n",
        "*Note*: This is also interactive. Type `/quit` to exit when you're done exploring."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d2bd785",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "from agents_integration.mcp_client.src.client import main as client_main\n",
        "\n",
        "\n",
        "async def run_client():\n",
        "    _argv_backup = sys.argv[:]\n",
        "    sys.argv = [\"client\", \"--config\", \"mcp_composed_server_config.json\"]\n",
        "    try:\n",
        "        await client_main()\n",
        "    finally:\n",
        "        sys.argv = _argv_backup\n",
        "\n",
        "\n",
        "# Start client with in-memory server\n",
        "await run_client()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e8d0f6b",
      "metadata": {},
      "source": [
        "After running this cell, you should see:\n",
        "1. The composed server starting (Nova+Brown Composed Server)\n",
        "2. Log messages showing the composition process (creating proxies, mounting servers)\n",
        "3. Both Nova and Brown servers starting up in the background\n",
        "4. The total capabilities summary (same 14 tools, 4 resources, 4 prompts)\n",
        "5. Welcome messages with cleaner prefixes (\"Brown\" and \"Nova\" instead of full server names)\n",
        "6. An interactive prompt for commands\n",
        "\n",
        "Notice the differences compared to the multi-server client:\n",
        "- Only one server name in the config (`nova-brown-composed`)\n",
        "- Cleaner tool prefixes (`nova_` and `brown_` instead of `nova-research-agent_` and `brown-writing-workflow_`)\n",
        "- Extra log messages showing the composition process\n",
        "\n",
        "Try the same commands:\n",
        "- `/tools` to see tools with cleaner prefixes\n",
        "- `/resources` to see resources\n",
        "- `/prompts` to see prompts\n",
        "- `/quit` to exit"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b09861a0",
      "metadata": {},
      "source": [
        "## 5. Multi-Server Client vs Composed Server: When to Use Each\n",
        "\n",
        "Both approaches achieve the same goal: using Nova and Brown together. But they have different use cases and trade-offs.\n",
        "\n",
        "| Aspect | Multi-Server Client | Composed Server |\n",
        "|--------|---------------------|-----------------|\n",
        "| **How it works** | The client connects to multiple independent servers simultaneously | A single server internally proxies to multiple underlying servers |\n",
        "| **Process management** | Each server runs independently with its own process | Single server process |\n",
        "| **Client configuration** | Client needs to know about all servers | Single endpoint (simpler client config) |\n",
        "| **Flexibility** | Easy to add/remove servers without changing code | Requires code changes to modify composition |\n",
        "| **Coordination** | No opportunity for server-side coordination | Opportunity to add coordination logic between agents |\n",
        "| **Deployment** | Good for development and testing | Better for production deployments |\n",
        "| **Fault tolerance** | One server can fail without affecting others | If composed server fails, all agents become unavailable |\n",
        "| **Use when** | • Developing or testing agents<br>• Quickly combining existing agents<br>• Need flexibility to add/remove agents dynamically | • Deploying agents as a unified system<br>• Need coordination logic between agents<br>• Want simpler client experience<br>• Building a product that packages multiple agents |\n",
        "| **Practical example** | **Development**: Use while building and testing Nova and Brown separately. Easy to restart one server without affecting the other | **Production**: Deploy and use from Cursor or Claude Desktop. Users configure one MCP server and get access to both agents |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5dff58cc",
      "metadata": {},
      "source": [
        "## 6. Conclusion\n",
        "\n",
        "In this lesson, we explored how to integrate the Nova research agent and Brown writing workflow using MCP. We learned:\n",
        "\n",
        "1. **Multi-Server MCP Client**: How to connect a single client to multiple MCP servers using FastMCP's multi-server configuration\n",
        "2. **Composed MCP Server**: How to use FastMCP's composition features (`as_proxy()` and `mount()`) to create a unified server\n",
        "4. **Use Cases**: When to use multi-server client vs composed server\n",
        "\n",
        "The key insight is that MCP makes agent integration straightforward. Because both Nova and Brown are already MCP servers, we don't need to write custom integration code. We simply leverage MCP's standardized protocol and FastMCP's composition features.\n",
        "\n",
        "This lesson focused on the technical mechanics of integration. In a real-world scenario, you would use these integrated agents within an IDE like Cursor. You could:\n",
        "- Use Nova to research a topic\n",
        "- Review the research file\n",
        "- Use Brown to generate an article from that research\n",
        "- Iterate on the article with Brown's editing tools\n",
        "- All from within your editor, with beautiful diff views and human-in-the-loop feedback\n",
        "\n",
        "In the next lesson, you'll see how to use both agents from Cursor to work on an article end-to-end. In Part 3 of this course, we'll explore production deployment, including how to deploy these composed servers remotely, add monitoring with Opik, and implement security measures."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
