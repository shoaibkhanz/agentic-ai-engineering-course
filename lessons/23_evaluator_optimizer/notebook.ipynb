{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 23: Evaluator-Optimizer Pattern â€” Reviewing and Editing the Brown Agent\n",
    "\n",
    "In this lesson, we'll explore how to implement the evaluator-optimizer pattern to review and edit generated articles. Building on the foundation from Lesson 22, we'll add a quality assurance layer that ensures the generated content meets all requirements.\n",
    "\n",
    "Learning Objectives:\n",
    "\n",
    "- Understand the evaluator-optimizer pattern and its real-world applications\n",
    "- Implement an article reviewing system that checks content against multiple profiles\n",
    "- Extend the article writer to handle review feedback\n",
    "- Configure the entire system from a single YAML file\n",
    "- Glue everything together into a robust LangGraph workflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [!NOTE]\n",
    "> ðŸ’¡ Remember that you can also run `brown` as a standalone Python package by going to `lessons/writing_workflow/` and following the instructions from there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "First, we define some standard Magic Python commands to autoreload Python packages whenever they change:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up Python Environment\n",
    "\n",
    "To set up your Python virtual environment using `uv` and load it into the Notebook, follow the step-by-step instructions from the `Course Admin` lesson from the beginning of the course.\n",
    "\n",
    "**TL/DR:** Be sure the correct kernel pointing to your `uv` virtual environment is selected.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Gemini API\n",
    "\n",
    "To run this lesson, you'll need several API keys configured:\n",
    "\n",
    "1. **Gemini API Key**, `GOOGLE_API_KEY` variable: Get your key from [Google AI Studio](https://aistudio.google.com/app/apikey)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables loaded from `/Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.env`\n",
      "Environment variables loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "from utils import env\n",
    "\n",
    "env.load(required_env_vars=[\"GOOGLE_API_KEY\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Key Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "  Using Pretty Prints\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "from utils import pretty_print\n",
    "\n",
    "nest_asyncio.apply()  # Allow nested async usage in notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Required Files\n",
    "\n",
    "We need to download the configuration files and input data that Brown uses for article generation and editing.\n",
    "\n",
    "First, let's download the configs folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!rm -rf configs\n",
    "!curl -L -o configs.zip https://raw.githubusercontent.com/iusztinpaul/agentic-ai-engineering-course-data/main/data/configs.zip\n",
    "!unzip configs.zip\n",
    "!rm -rf configs.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's download the inputs folder containing profiles, examples, and test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!rm -rf inputs\n",
    "!curl -L -o inputs.zip https://raw.githubusercontent.com/iusztinpaul/agentic-ai-engineering-course-data/main/data/inputs.zip\n",
    "!unzip inputs.zip\n",
    "!rm -rf inputs.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify what we downloaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aritcle_guideline.md   \u001b[1m\u001b[36minputs\u001b[m\u001b[m/                notebook_guideline.md\n",
      "\u001b[1m\u001b[36mconfigs\u001b[m\u001b[m/               notebook.ipynb\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up Directory Constants\n",
    "\n",
    "Now let's define constants to reference these directories throughout the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configs directory exists: True\n",
      "Inputs directory exists: True\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "CONFIGS_DIR = Path(\"configs\")\n",
    "INPUTS_DIR = Path(\"inputs\")\n",
    "\n",
    "print(f\"Configs directory exists: {CONFIGS_DIR.exists()}\")\n",
    "print(f\"Inputs directory exists: {INPUTS_DIR.exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples directory exists: True\n",
      "Profiles directory exists: True\n"
     ]
    }
   ],
   "source": [
    "EXAMPLES_DIR = Path(\"inputs/examples/course_lessons\")\n",
    "PROFILES_DIR = Path(\"inputs/profiles\")\n",
    "\n",
    "print(f\"Examples directory exists: {EXAMPLES_DIR.exists()}\")\n",
    "print(f\"Profiles directory exists: {PROFILES_DIR.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will load a simpler example that runs faster and is easier to understand. At the end, we will load a larger sample that is closer to what we do on our end to generate professional articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples directory exists: True\n"
     ]
    }
   ],
   "source": [
    "SAMPLE_DIR = Path(\"inputs/tests/01_sample_small\")\n",
    "\n",
    "print(f\"Samples directory exists: {SAMPLE_DIR.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. How the Writing Agent Works with Review-Edit Loop\n",
    "\n",
    "Before diving into the implementation, let's understand how the writing agent now incorporates the review-editing process through the evaluator-optimizer pattern.\n",
    "\n",
    "### The Extended Workflow\n",
    "\n",
    "In Lesson 22, we learned about the three-step workflow:\n",
    "\n",
    "1. **Load Context into Memory** - Gather guidelines, research, profiles, and examples\n",
    "2. **Generate Media Items** - Use the orchestrator-worker pattern to create diagrams\n",
    "3. **Write the Article** - Generate the first draft using the ArticleWriter\n",
    "\n",
    "Now we're adding a fourth and fifth step that loops multiple times:\n",
    "\n",
    "4. **Review the Article** (Evaluator) - Check the article against all profiles and guidelines\n",
    "5. **Edit the Article** (Optimizer) - Fix all identified issues based on the reviews\n",
    "\n",
    "This review-edit pattern continues for a configurable number of iterations, gradually improving the article quality.\n",
    "\n",
    "### The Evaluator-Optimizer Pattern Explained\n",
    "\n",
    "The evaluator-optimizer pattern is a fundamental AI workflow pattern that mirrors real-world quality assurance processes:\n",
    "\n",
    "- **Evaluator**: Analyzes output and identifies issues or areas for improvement\n",
    "- **Optimizer**: Takes the feedback and makes targeted improvements\n",
    "\n",
    "In our case:\n",
    "- **Article Reviewer Node** = Evaluator (checks if article follows all the standards)\n",
    "- **Article Writer Node** = Optimizer (edits the article based on reviews)\n",
    "\n",
    "This approach is extremely similar to how a real-world writing process works:\n",
    "\n",
    "1. The writer writes the article (initial draft)\n",
    "2. A reviewer provides feedback from outside eyes\n",
    "3. The same writer edits the article based on the provided feedback\n",
    "4. Repeat steps 2-3 until satisfied"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow Visualization\n",
    "\n",
    "Let's visualize the complete workflow with the review-edit loop:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/iusztinpaul/agentic-ai-engineering-course-data/main/images/l23_writing_workflow.png\" alt=\"Workflow\" height=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Review Entities: Modeling Feedback\n",
    "\n",
    "Now let's explore the new Pydantic entities we need for the review process. In Lesson 22, we already covered the core entities like `Article`, `ArticleGuideline`, and `ArticleProfiles`. Now we need entities to represent the reviewing logic.\n",
    "\n",
    "### Why Two Types of Reviews?\n",
    "\n",
    "We support two review modes:\n",
    "\n",
    "1. **Whole Article Reviews**: Review the entire article from top to bottom\n",
    "2. **Selected Text Reviews**: Review only a specific portion of the article\n",
    "\n",
    "Most of the time, only a section of the article needs editing, not the whole thing. This targeted approach saves time and reduces API costs by only reviewing what matters.\n",
    "\n",
    "### The Review Entities\n",
    "\n",
    "From `brown.entities.reviews`, we have these core entities:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. The Review Entity\n",
    "\n",
    "A `Review` represents a single piece of feedback about the article:\n",
    "\n",
    "\n",
    "```python\n",
    "from pydantic import BaseModel, Field\n",
    "from brown.entities.mixins import ContextMixin\n",
    "\n",
    "\n",
    "class Review(BaseModel, ContextMixin):\n",
    "    profile: str = Field(\n",
    "        description=\"The profile type listing the constraints based on which we will write the comment.\"\n",
    "    )\n",
    "    location: str = Field(\n",
    "        description=\"The location from within the article where the comment is made. For example, the title of a section.\"\n",
    "    )\n",
    "    comment: str = Field(\n",
    "        description=\"The comment made by the reviewer stating the issue relative to the profile.\"\n",
    "    )\n",
    "\n",
    "    def to_context(self) -> str:\n",
    "        return f\"\"\"\n",
    "<{self.xml_tag}>\n",
    "    <profile>{self.profile}</profile>\n",
    "    <location>{self.location}</location>\n",
    "    <comment>{self.comment}</comment>\n",
    "</{self.xml_tag}>\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**Key Fields:**\n",
    "\n",
    "- **profile**: Which requirement was violated (e.g., \"tonality_profile\", \"article_guideline\", \"structured_profile\")\n",
    "- **location**: Where in the article the issue exists, usually the title of the article section (e.g., \"Introduction - Second paragraph\")\n",
    "- **comment**: Detailed explanation of what's wrong and why it deviates from the requirement\n",
    "\n",
    "**Example Review:**\n",
    "\n",
    "```python\n",
    "Review(\n",
    "    profile=\"tonality_profile\",\n",
    "    location=\"Introduction - First paragraph\",\n",
    "    comment=\"The tone is overly formal. The tonality profile specifies a conversational, friendly tone. The current opening reads like an academic paper rather than an engaging blog post.\"\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. The ArticleReviews Entity\n",
    "\n",
    "`ArticleReviews` bundles multiple reviews for the whole article:\n",
    "\n",
    "\n",
    "```python\n",
    "class ArticleReviews(BaseModel, ContextMixin):\n",
    "    article: Article\n",
    "    reviews: list[Review]\n",
    "\n",
    "    def to_context(self, include_article: bool = False) -> str:\n",
    "        reviews_str = \"\\n\".join([review.to_context() for review in self.reviews])\n",
    "        return f\"\"\"\n",
    "<{self.xml_tag}>\n",
    "    {f\"<article>{self.article}</article>\" if include_article else \"\"}\n",
    "    <reviews>\n",
    "    {reviews_str}\n",
    "    </reviews>\n",
    "</{self.xml_tag}>\n",
    "\"\"\"\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"Reviews(len_reviews={len(self.reviews)})\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. The SelectedText Entity\n",
    "\n",
    "Before understanding `SelectedTextReviews`, we need to see the `SelectedText` entity from `brown.entities.articles` to understand how we will model the selected text relative to how we did for the whole article:\n",
    "\n",
    "\n",
    "```python\n",
    "class SelectedText(BaseModel, ContextMixin):\n",
    "    article: Article\n",
    "    content: str\n",
    "    first_line_number: int\n",
    "    last_line_number: int\n",
    "\n",
    "    def to_context(self) -> str:\n",
    "        return f\"\"\"\n",
    "<{self.xml_tag}>\n",
    "    <content>{self.content}</content>\n",
    "    <first_line_number>{self.first_line_number}</first_line_number>\n",
    "    <last_line_number>{self.last_line_number}</last_line_number>\n",
    "</{self.xml_tag}>\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**Key Features:**\n",
    "\n",
    "- Contains the full `article` for context\n",
    "- `content`: The specific text selection to review/edit\n",
    "- Line numbers help locate the selection within the full article\n",
    "- This enables targeted reviews of specific sections\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. The SelectedTextReviews Entity\n",
    "\n",
    "`SelectedTextReviews` handles reviews for just a portion of the article:\n",
    "\n",
    "\n",
    "```python\n",
    "class SelectedTextReviews(BaseModel, ContextMixin):\n",
    "    article: Article\n",
    "    selected_text: SelectedText\n",
    "    reviews: list[Review]\n",
    "\n",
    "    def to_context(self, include_article: bool = False) -> str:\n",
    "        reviews_str = \"\\n\".join([review.to_context() for review in self.reviews])\n",
    "        return f\"\"\"\n",
    "<{self.xml_tag}>\n",
    "    {f\"<article>{self.article.to_context()}</article>\" if include_article else \"\"}\n",
    "    <selected_text>{self.selected_text.to_context()}</selected_text>\n",
    "    <reviews>\n",
    "    {reviews_str}\n",
    "    </reviews>\n",
    "</{self.xml_tag}>\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**Use Case:**\n",
    "\n",
    "When a user identifies a specific problematic section, we can:\n",
    "1. Create a `SelectedText` entity pointing to that section\n",
    "2. Review only that selection (faster, cheaper)\n",
    "3. Edit only that selection\n",
    "4. Replace the selection in the full article\n",
    "\n",
    "This is particularly useful for human-in-the-loop workflows where humans can highlight specific sections for improvement. More on this in Lesson 24.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity Relationships\n",
    "\n",
    "Let's visualize how these entities relate:\n",
    "\n",
    "```\n",
    "Article\n",
    "  â””â”€â”€ ArticleReviews\n",
    "       â””â”€â”€ reviews: list[Review]\n",
    "\n",
    "Article + SelectedText\n",
    "  â””â”€â”€ SelectedTextReviews  \n",
    "       â”œâ”€â”€ selected_text: SelectedText\n",
    "       â””â”€â”€ reviews: list[Review]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The Article Reviewer Node: The Evaluator\n",
    "\n",
    "Now let's explore the `ArticleReviewer` node, which acts as the **evaluator** in our evaluator-optimizer pattern. This node analyzes articles against all requirements and generates detailed feedback.\n",
    "\n",
    "Remember that the core expectations are that the article follows the article guidelines and that all the writing profiles are respected.\n",
    "\n",
    "### Node Abstraction Recap\n",
    "\n",
    "First, a quick reminder that we leverage the same `Node` abstraction from Lesson 22 to implement all our nodes.\n",
    "\n",
    "\n",
    "```python\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Any\n",
    "\n",
    "from brown.nodes.base import Node, Toolkit\n",
    "\n",
    "\n",
    "class Node(ABC):\n",
    "    def __init__(self, model: Runnable, toolkit: Toolkit) -> None:\n",
    "        self.toolkit = toolkit\n",
    "        self.model = self._extend_model(model)\n",
    "\n",
    "    def _extend_model(self, model: Runnable) -> Runnable:\n",
    "        # Can be overridden to bind tools, structured output, etc.\n",
    "        return model\n",
    "\n",
    "    @abstractmethod\n",
    "    async def ainvoke(self) -> Any:\n",
    "        pass\n",
    "```\n",
    "\n",
    "All workflow nodes inherit from this base class, providing a consistent interface throughout the system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ArticleReviewer Class Structure\n",
    "\n",
    "Let's examine the `ArticleReviewer` class from `brown.nodes.article_reviewer`:\n",
    "\n",
    "**1. The Class and Initialization:**\n",
    "\n",
    "```python\n",
    "class ArticleReviewer(Node):\n",
    "    system_prompt_template = \"\"\"...\"\"\"  # We'll see this shortly\n",
    "    selected_text_system_prompt_template = \"\"\"...\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        to_review: Article | SelectedText,\n",
    "        article_guideline: ArticleGuideline,\n",
    "        model: Runnable,\n",
    "        article_profiles: ArticleProfiles,\n",
    "    ) -> None:\n",
    "        self.to_review = to_review\n",
    "        self.article_guideline = article_guideline\n",
    "        self.article_profiles = article_profiles\n",
    "\n",
    "        super().__init__(model, toolkit=Toolkit(tools=[]))\n",
    "```\n",
    "\n",
    "**Key Design Decisions:**\n",
    "\n",
    "- `to_review` can be either a full `Article` or just `SelectedText` (polymorphic design)\n",
    "- Takes all the requirements: guideline, profiles\n",
    "- No tools needed (empty toolkit), as reviewing is a pure generation task and no tools are required\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Model Extension with Structured Output:**\n",
    "\n",
    "```python\n",
    "    def _extend_model(self, model: Runnable) -> Runnable:\n",
    "        model = cast(BaseChatModel, super()._extend_model(model))\n",
    "        model = model.with_structured_output(ReviewsOutput)\n",
    "        \n",
    "        return model\n",
    "```\n",
    "\n",
    "The reviewer uses structured output to ensure we get properly formatted reviews. First, we need an intermediate Pydantic model:\n",
    "\n",
    "```python\n",
    "class ReviewsOutput(BaseModel):\n",
    "    reviews: list[Review]\n",
    "```\n",
    "\n",
    "**Why an intermediate model?**\n",
    "\n",
    "The LLM outputs `ReviewsOutput`, but the node returns either `ArticleReviews` or `SelectedTextReviews` (which include the article/selected_text). This separation keeps the LLM output schema simple to avoid any potential LLM inference errors, while allowing richer node outputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. The ainvoke Method:**\n",
    "\n",
    "```python\n",
    "    async def ainvoke(self) -> ArticleReviews | SelectedTextReviews:\n",
    "        # Build the main system prompt with all requirements\n",
    "        system_prompt = self.system_prompt_template.format(\n",
    "            human_feedback=self.human_feedback.to_context() if self.human_feedback else \"\",\n",
    "            article=self.article.to_context(),\n",
    "            article_guideline=self.article_guideline.to_context(),\n",
    "            character_profile=self.article_profiles.character.to_context(),\n",
    "            article_profile=self.article_profiles.article.to_context(),\n",
    "            structure_profile=self.article_profiles.structure.to_context(),\n",
    "            mechanics_profile=self.article_profiles.mechanics.to_context(),\n",
    "            terminology_profile=self.article_profiles.terminology.to_context(),\n",
    "            tonality_profile=self.article_profiles.tonality.to_context(),\n",
    "        )\n",
    "        \n",
    "        user_input_content = self.build_user_input_content(inputs=[system_prompt])\n",
    "        inputs = [{\"role\": \"user\", \"content\": user_input_content}]\n",
    "        \n",
    "        # If reviewing selected text, add additional instructions\n",
    "        if self.is_selected_text:\n",
    "            inputs.extend([\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": self.selected_text_system_prompt_template.format(\n",
    "                        selected_text=self.to_review.to_context()\n",
    "                    ),\n",
    "                }\n",
    "            ])\n",
    "        \n",
    "        # Generate reviews\n",
    "        reviews = await self.model.ainvoke(inputs)\n",
    "        if not isinstance(reviews, ReviewsOutput):\n",
    "            raise InvalidOutputTypeException(ReviewsOutput, type(reviews))\n",
    "        \n",
    "        # Return appropriate review type\n",
    "        if self.is_selected_text:\n",
    "            return SelectedTextReviews(\n",
    "                article=self.article,\n",
    "                selected_text=cast(SelectedText, self.to_review),\n",
    "                reviews=reviews.reviews,\n",
    "            )\n",
    "        else:\n",
    "            return ArticleReviews(\n",
    "                article=self.article,\n",
    "                reviews=reviews.reviews,\n",
    "            )\n",
    "```\n",
    "\n",
    "**Flow:**\n",
    "\n",
    "1. Format the system prompt with all requirements\n",
    "2. If reviewing selected text, add special instructions\n",
    "3. Generate structured reviews from the LLM\n",
    "4. Package the output entity into the appropriate review type\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. The System Prompt (Main Review Logic):**\n",
    "\n",
    "Here's the system prompt which is carefully designed to create thorough, actionable reviews based on the article guideline and writing profiles:\n",
    "\n",
    "```python\n",
    "class ArticleReviewer(Node):\n",
    "    system_prompt_template = \"\"\"\n",
    "You are Brown, an expert article writer, editor and reviewer specialized in reviewing technical, educative and informational articles.\n",
    "\n",
    "Your task is to review a given article against a set of expected requirements and provide detailed feedback \n",
    "about any deviations. You will act as a quality assurance reviewer, identifying specific issues and suggesting \n",
    "how the article fails to meet the expected requirements.\n",
    "\n",
    "These reviews will further be used to edit the article, ensuring it follows all the requirements.\n",
    "\n",
    "## Requirements\n",
    "\n",
    "The requirements are a set of rules, guidelines or profiles that the article should follow. Here they are:\n",
    "\n",
    "- **article guideline:** the user intent describing how the article should look like. Specific to this particular article.\n",
    "- **article profile:** rules specific to writing articles. Generic for all articles.\n",
    "- **character profile:** the character you will impersonate while writing. Generic for all content.\n",
    "- **structure profile:** Structure rules guiding the final output format. Generic for all content.\n",
    "- **mechanics profile:** Mechanics rules guiding the writing process. Generic for all content.\n",
    "- **terminology profile:** Terminology rules guiding word choice and phrasing. Generic for all content.\n",
    "- **tonality profile:** Tonality rules guiding the writing style. Generic for all content.\n",
    "\n",
    "## Article to Review\n",
    "\n",
    "Here is the article that needs to be reviewed:\n",
    "\n",
    "{article}\n",
    "\n",
    "## Article Guideline\n",
    "\n",
    "The <article_guideline> represents the user intent, describing how the actual article should look like.\n",
    "\n",
    "The <article_guideline> will ALWAYS contain:\n",
    "- all the sections of the article expected to be written, in the correct order\n",
    "- a level of detail for each section, describing what each section should contain. Depending on how much detail you have in a\n",
    "particular section of the <article_guideline>, you will use more or less information from the <research> tags to write the section.\n",
    "\n",
    "The <article_guideline> can ALSO contain:\n",
    "- length constraints for each section, such as the number of characters, words or reading time. If present, you will respect them.\n",
    "- important (golden) references as URLs or titles present in the <research> tags. If present, always prioritize them over anything else \n",
    "from the <research>.\n",
    "- information about anchoring the article into a series such as a course or a book. Extremely important when the article is part of \n",
    "something bigger and we have to anchor the article into the learning journey of the reader. For example, when introducing concepts\n",
    "in previous articles that we don't want to reintroduce into the current one.\n",
    "- concrete information about writing the article. If present, you will ALWAYS priotize the instructions from the <article_guideline> \n",
    "over any other instructions.\n",
    "\n",
    "Here is the article guideline:\n",
    "{article_guideline}\n",
    "\n",
    "## Character Profile\n",
    "\n",
    "To make the writing more personable, we impersonated the following character profile when writing the article:\n",
    "{character_profile}\n",
    "\n",
    "## Terminology Profile\n",
    "\n",
    "Here is the terminology profile, describing how to choose the right words and phrases:\n",
    "to the target audience:\n",
    "{terminology_profile}\n",
    "\n",
    "## Tonality Profile\n",
    "\n",
    "Here is the tonality profile, describing the tone, voice and style of the writing:\n",
    "{tonality_profile}\n",
    "\n",
    "## Mechanics Profile\n",
    "\n",
    "Here is the mechanics profile, describing how the sentences and words should be written:\n",
    "{mechanics_profile}\n",
    "\n",
    "## Structure Profile\n",
    "\n",
    "Here is the structure profile, describing general rules on how to structure text, such as the sections, paragraphs, lists,\n",
    "code blocks, or media items:\n",
    "{structure_profile}\n",
    "\n",
    "## Article Profile\n",
    "\n",
    "Here is the article profile, describing particularities on how the end-to-end article should look like:\n",
    "{article_profile}\n",
    "\n",
    "## Reviewing Process\n",
    "\n",
    "You will review the article against all the requirements above, creating a one-to-many relationship between each requirement and the \n",
    "number of required reviews. In other words, for each requirement, you will create 0 to N reviews. If the article follows the \n",
    "requirement 100%, you will not create any reviews for it. If it doesn't follow the requirement, you will create as many reviews \n",
    "as required to ensure the article follows the requirement.\n",
    "\n",
    "Remember that these reviews will further be used to edit the article, ensuring it follows all the requirements. Thus, it's\n",
    "important to make a thorough review, covering all the requirements and not missing any detail.\n",
    "\n",
    "## Reviewing Rules\n",
    "\n",
    "- **The first most important rule:** The requirements can contain some special sections labeled as \"rules\" or \n",
    "\"correction rules\". You should look for <(.*)?rules(.*)?> XML tags like <correction_media_rules>, \n",
    "<abbreviations_or_acronyms_never_to_expand_rules>, <correction_reference_rules>. These are special highlights that \n",
    "should always be prioritized over other rules during the review process. They should be respected at all costs when \n",
    "writing the article. You will always prioritize these rules over other rules from the requirements making them your \n",
    "No.1 focus.\n",
    "- **The second most important rule:** The adherence to the <article_guideline>.\n",
    "- **The third most important rule:** The adherence to the <article_profile>.\n",
    "- **The fourth most important rule:** The adherence to the rest of the requirements.\n",
    "\n",
    "Other more generic rules:\n",
    "- Be thorough but fair - only flag genuine issues\n",
    "- Emphasize WHY something is wrong, not just WHAT is wrong\n",
    "- Focus on significant deviations, not minor nitpicks \n",
    "\n",
    "## Output Format\n",
    "\n",
    "For each issue you identify, create a review with:\n",
    "- **profile**: The requirement where the issue was found (e.g., \"human_feedback\", \"article_guideline\", \"character_profile\", \n",
    "\"article_profile\", \"structure_profile\", \"mechanics_profile\", \"terminology_profile\", \"tonality_profile\")\n",
    "- **location**: The section title where the issue was found and the paragraph number. For example, \"Introduction - First paragraph\" \n",
    "or \"Implementing GraphRAG - Third paragraph\"\n",
    "- **comment**: A detailed explanation of why it's wrong, what's wrong and how it deviates from the requirement.\n",
    "\n",
    "## Chain of Thoughts\n",
    "\n",
    "1. Read and analyze the article.\n",
    "2. Read and analyze the <human_feedback>.\n",
    "3. Read and analyze all the requirements considering the <human_feedback> as a guiding force.\n",
    "4. Carefully compare the article against the requirements as instructed by the rules above.\n",
    "5. For each requirement, create 0 to N reviews\n",
    "6. Return the reviews of the article.\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**Key Prompt Engineering Techniques:**\n",
    "\n",
    "1. **Clear Role**: Expert reviewer with specific expertise\n",
    "2. **Explicit Priority System**: Rules are ranked (special rules > guideline > article profile > other profiles)\n",
    "3. **Output**: Clear instructions on what we want the LLM to fill for each attribute\n",
    "5. **Chain of Thought**: Explicit reasoning steps that glue together all the other sections\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. The Selected Text System Prompt:**\n",
    "\n",
    "When reviewing only a selected portion, we append additional instructions:\n",
    "\n",
    "```python\n",
    "class ArticleReviewer(Node):\n",
    "    system_prompt_template = \"\"\"...\"\"\"\n",
    "    \n",
    "    selected_text_system_prompt_template = \"\"\"\n",
    "You already reviewed and edited the whole article. Now we want to further review only a specific portion\n",
    "of the article, which we label as the <selected_text>. Despite reviewing the selected text, instead of the\n",
    "article as a whole, you will follow the exact same instructions from above as if you were reviewing the article as a whole.\n",
    "\n",
    "## Selected Text to Review\n",
    "\n",
    "Here is the selected text that needs to be reviewed:\n",
    "\n",
    "{selected_text}\n",
    "\n",
    "As pointed out before, the selected text is part of the larger <article> that is already reviewed.\n",
    "You will use the full <article> as context and anchoring the reviewing process within the bigger picture.\n",
    "\n",
    "The <first_line_number> and <last_line_number> numbers from the <selected_text> indicate the first and \n",
    "last line/row numbers of the selected text from the <article>. Use them to locate the selected text within the <article>.\n",
    "\n",
    "## Chain of Thoughts\n",
    "\n",
    "Here is the new chain of thoughts logic you will follow when reviewing the selected text. You can ignore the\n",
    "previous chain of thoughts:\n",
    "\n",
    "1. Read and analyze the article.\n",
    "2. Locate the <selected_text> within the <article> based on the <first_line_number> and <last_line_number>.\n",
    "3. Read and analyze the <human_feedback>.\n",
    "4. Read and analyze all the requirements considering the <human_feedback> as a guiding force.\n",
    "5. Carefully compare the selected text against the requirements as instructed by the rules above.\n",
    "6. For each requirement, create 0 to N reviews\n",
    "7. Return the reviews of the selected text.\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "This allows focused reviews on specific sections while maintaining context of the full article. As this system prompt is passed together with the `system_prompt_template` system prompt it has to act only as an extension on explaining what to do with a selected text.\n",
    "\n",
    "The special trick here is that it adds a new `Chain of Thoughts` section that overrides the one from the original system prompts adding specialized instructions on how to reason across the new task, while still having all the context from both system prompts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Reviewing a Whole Article\n",
    "\n",
    "Now let's see the `ArticleReviewer` in action by reviewing a sample article.\n",
    "\n",
    "First load the sample article guideline and the standard profiles:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-11-26 13:13:45.120\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mbrown.config\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m10\u001b[0m - \u001b[1mLoading environment file from `.env`\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m------------------------------------- Sample article guideline -------------------------------------\u001b[0m\n",
      "  ## Outline\n",
      "\n",
      "1. Introduction: The Critical Decision Every AI Engineer Faces\n",
      "2. Understanding the Spectrum: From Workflows to Agents\n",
      "3. Choosing Your Path\n",
      "4. Conclusion: The Challenges of Every AI Engineer\n",
      "\n",
      "## Section 1 - Introduction: The Critical Decision Every AI Engineer Faces\n",
      "\n",
      "- **The Problem:** When building AI applications, engineers face a critical architectural decision early in their development process. Should they create a predictable, step-by-step workflow where they control every action, or should they build an autonomous agent that can think and decide for itself? This is one of the key decisions that will impact everything from the product such as development time and costs to reliability and user experience.\n",
      "- Quick walkthrough of what we'll learn by the end of this lesson\n",
      "\n",
      "- **Section length:** 100 words\n",
      "\n",
      "## Section 2 - Understanding the Spectrum: From Workflows to Agents\n",
      "\n",
      "- In this section we want to take a brief look at what LLM workflows and AI agents are. At this po\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from brown.loaders import MarkdownArticleGuidelineLoader, MarkdownArticleLoader, MarkdownArticleProfilesLoader\n",
    "from brown.models import SupportedModels, get_model\n",
    "from brown.nodes import ArticleReviewer\n",
    "\n",
    "# Load the article guideline\n",
    "guideline_loader = MarkdownArticleGuidelineLoader(uri=Path(\"article_guideline.md\"))\n",
    "article_guideline = guideline_loader.load(working_uri=SAMPLE_DIR)\n",
    "\n",
    "# Load the article profiles\n",
    "profiles_input = {\n",
    "    \"article\": PROFILES_DIR / \"article_profile.md\",\n",
    "    \"character\": PROFILES_DIR / \"character_profiles\" / \"paul_iusztin.md\",\n",
    "    \"mechanics\": PROFILES_DIR / \"mechanics_profile.md\",\n",
    "    \"structure\": PROFILES_DIR / \"structure_profile.md\",\n",
    "    \"terminology\": PROFILES_DIR / \"terminology_profile.md\",\n",
    "    \"tonality\": PROFILES_DIR / \"tonality_profile.md\",\n",
    "}\n",
    "profiles_loader = MarkdownArticleProfilesLoader(uri=profiles_input)\n",
    "article_profiles = profiles_loader.load(working_uri=SAMPLE_DIR)\n",
    "\n",
    "pretty_print.wrapped(article_guideline.content[:1000], title=\"Sample article guideline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, load a sample article to review (that we already generated based on the same logic from lesson 22):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m------------------------- Sample article to review (first 3000 characters) -------------------------\u001b[0m\n",
      "  # AI Agents vs. LLM Workflows: The Critical Decision Every AI Engineer Faces\n",
      "### A pragmatic guide to choosing the right architecture for your AI application.\n",
      "\n",
      "When building AI applications, engineers face a critical architectural decision early in their development process. Should you create a predictable, step-by-step workflow where you control every action, or should you build an autonomous agent that can think and decide for itself? This is one of the key decisions that will impact everything from development time and costs to reliability and user experience.\n",
      "\n",
      "Choose the wrong approach, and you might end up with an overly rigid system that breaks when users deviate from expected patterns. Or you could build an unpredictable agent that works brilliantly 80% of the time but fails catastrophically when it matters most. Either path can lead to months of wasted development time, frustrated users, and executives questioning the skyrocketing operational costs.\n",
      "\n",
      "In the real world of 2024 and 2025, billion-dollar AI startups succeed or fail based primarily on this architectural decision. The successful companies, teams, and AI engineers know when to use workflows versus agents and, more importantly, how to combine both approaches effectively.\n",
      "\n",
      "This lesson will provide a framework to help you make this critical decision with confidence. We will systematically explore the spectrum from rigid workflows to autonomous agents, helping you understand the trade-offs. By the end, you will have the knowledge to architect AI systems that are not only powerful but also robust, efficient, and safe.\n",
      "\n",
      "## Understanding the Spectrum: From Workflows to Agents\n",
      "\n",
      "Before we can choose between workflows and agents, we need a clear understanding of what they are. Rather than focusing on the technical specifics, let's look at their core properties and how they function in practice.\n",
      "\n",
      "An LLM workflow is a sequence of tasks that involves LLM calls or other operations, such as reading from a database or writing to a file system. It is largely predefined and orchestrated by developer-written code. The steps are defined in advance, resulting in deterministic or rule-based paths with predictable execution and explicit control flow. Think of it like a factory assembly line, where each station performs a specific, repeatable task in a set order.\n",
      "\n",
      "```mermaid\n",
      "graph TD\n",
      "    A[\"Input\"] --> B[\"tool_calling_llm\"]\n",
      "    B --> C[\"Tools\"]\n",
      "    C --> D[\"End\"]\n",
      "```\n",
      "Image 1: A simple LLM workflow illustrating a sequence of predefined tasks, where an input leads to a tool-calling LLM, which interacts with tools before the workflow ends.\n",
      "\n",
      "On the other end of the spectrum, we have AI agents. These are systems where an LLM plays a central role in dynamically deciding the sequence of steps, reasoning, and actions to achieve a goal. The steps are not defined in advance but are planned based on the task and the current state of the environment. This makes them adaptive and capable of handling novelty. You ca...\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "article_loader = MarkdownArticleLoader(uri=Path(\"article.md\"))\n",
    "article = article_loader.load(working_uri=SAMPLE_DIR)\n",
    "\n",
    "pretty_print.wrapped(f\"{article.content[:3000]}...\", title=\"Sample article to review (first 3000 characters)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the article reviewer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviewing article...\n",
      "\u001b[93m----------------------------------------- Article reviews -----------------------------------------\u001b[0m\n",
      "  Generated 30 reviews:\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m--------------------------------------------- Review 1 ---------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"article_guideline\",\n",
      "  \"Location\": \"Introduction: The Critical Decision Every AI Engineer Faces - Overall section\",\n",
      "  \"Comment\": \"The 'Introduction' section exceeds the specified length of 100 words. It currently has approximately 128 words, which is 28 words over the limit. The introduction should be concise and around 100 word...\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m--------------------------------------------- Review 2 ---------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"mechanics_profile\",\n",
      "  \"Location\": \"Introduction: The Critical Decision Every AI Engineer Faces - First paragraph\",\n",
      "  \"Comment\": \"The first paragraph of the Introduction uses the pronoun 'you' to refer to the team creating the course (e.g., 'Should you create... or should you build...'). As per the 'Point of View' rule in the me...\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m--------------------------------------------- Review 3 ---------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"mechanics_profile\",\n",
      "  \"Location\": \"Understanding the Spectrum: From Workflows to Agents - Fourth paragraph\",\n",
      "  \"Comment\": \"The fourth paragraph in this section contains the sentence 'Before we can choose between workflows and agents, we need a clear understanding of what they are.' The 'Point of View' rule in the mechanic...\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m--------------------------------------------- Review 4 ---------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"article_guideline\",\n",
      "  \"Location\": \"Understanding the Spectrum: From Workflows to Agents - Overall section\",\n",
      "  \"Comment\": \"The 'Understanding the Spectrum' section exceeds the specified length of 200 words (without the mermaid diagram code). The textual content of this section is approximately 254 words, which is 54 words...\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m--------------------------------------------- Review 5 ---------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"article_guideline\",\n",
      "  \"Location\": \"Choosing Your Path - Overall section\",\n",
      "  \"Comment\": \"The 'Choosing Your Path' section significantly exceeds the specified length of 200 words. The textual content of this section is approximately 439 words, which is 239 words over the limit. The section...\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m--------------------------------------------- Review 6 ---------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"article_guideline\",\n",
      "  \"Location\": \"Choosing Your Path - First paragraph\",\n",
      "  \"Comment\": \"The article guideline for Section 3 explicitly states: 'Attach an image from the research showing the gradient between LLM workflows and AI agents.' The article provides a Mermaid diagram instead of a...\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m--------------------------------------------- Review 7 ---------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"structure_profile\",\n",
      "  \"Location\": \"Choosing Your Path - First paragraph\",\n",
      "  \"Comment\": \"The Mermaid diagram (Image 3) caption is incorrectly formatted. The 'diagram description' part should be concise and to the point. The current caption 'A spectrum illustrating the trade-off between ap...\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m--------------------------------------------- Review 8 ---------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"article_guideline\",\n",
      "  \"Location\": \"Choosing Your Path - Seventh paragraph\",\n",
      "  \"Comment\": \"The article guideline for Section 3 explicitly states: 'Generate a mermaid to illustrate the AI generation and human verification loop.' The article includes a Mermaid diagram (Image 4) but does not a...\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m--------------------------------------------- Review 9 ---------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"structure_profile\",\n",
      "  \"Location\": \"Choosing Your Path - Seventh paragraph\",\n",
      "  \"Comment\": \"The Mermaid diagram (Image 4) caption is incorrectly formatted. The 'diagram description' part should be concise and to the point. The current caption 'A flowchart illustrating the cyclical AI generat...\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m-------------------------------------------- Review 10 --------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"article_guideline\",\n",
      "  \"Location\": \"Conclusion: The Challenges of Every AI Engineer - Overall section\",\n",
      "  \"Comment\": \"The 'Conclusion' section exceeds the specified length of 100 words. The textual content of this section is approximately 201 words, which is 101 words over the limit. The conclusion needs to be conden...\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m-------------------------------------------- Review 11 --------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"article_guideline\",\n",
      "  \"Location\": \"Conclusion: The Challenges of Every AI Engineer - Second paragraph\",\n",
      "  \"Comment\": \"The article guideline specifies to 'To transition from this lesson to the next, specify that in the future lesson (lesson 3) we will dig more into the foundations of AI agents and workflows, which is ...\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m-------------------------------------------- Review 12 --------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"references_rules\",\n",
      "  \"Location\": \"References - Overall section\",\n",
      "  \"Comment\": \"The 'References' section is missing the 'Golden Sources' from the article guideline at the top of the list, as required by the 'references_rules'. The guideline states: 'Always add the sources from th...\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m-------------------------------------------- Review 13 --------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"references_rules\",\n",
      "  \"Location\": \"References - Overall section\",\n",
      "  \"Comment\": \"The 'References' section does not follow the APA 7th edition format for entries where the author, publish date, or full title is missing. For example, 'Google. (n.d.). Gemini CLI. GitHub. https://gith...\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m-------------------------------------------- Review 14 --------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"correction_media_rules\",\n",
      "  \"Location\": \"Article level\",\n",
      "  \"Comment\": \"The image numbering is inconsistent throughout the article. Image 1, Image 2, Image 3, Image 4, Image 5, Image 6, Image 7, Image 8, Image 9, Image 10, and Image 11 are all present. There are no tables...\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m-------------------------------------------- Review 15 --------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"structure_profile\",\n",
      "  \"Location\": \"Image 1: A simple LLM workflow illustrating a sequence of predefined tasks, where an input leads to a tool-calling LLM, which interacts with tools before the workflow ends.\",\n",
      "  \"Comment\": \"The caption for Image 1 is too verbose and does not follow the concise description style from the example 'Image X: The description of the diagram.'. It should be shortened to 'Image 1: A simple LLM w...\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m-------------------------------------------- Review 16 --------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"structure_profile\",\n",
      "  \"Location\": \"Image 2: A simple agentic system illustrating dynamic decision-making and tool usage with an assistant orchestrating the loop.\",\n",
      "  \"Comment\": \"The caption for Image 2 is too verbose and does not follow the concise description style from the example 'Image X: The description of the diagram.'. It should be shortened to 'Image 2: A simple agent...\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m-------------------------------------------- Review 17 --------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"structure_profile\",\n",
      "  \"Location\": \"Image 5: An LLM workflow using chaining and routing.\",\n",
      "  \"Comment\": \"The caption for Image 5 is too verbose and does not follow the concise description style from the example 'Image X: The description of the diagram.'. It should be shortened to 'Image 5: An LLM workflo...\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m-------------------------------------------- Review 18 --------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"structure_profile\",\n",
      "  \"Location\": \"Image 6: An Orchestrator-Worker pattern for LLM workflows.\",\n",
      "  \"Comment\": \"The caption for Image 6 is too verbose and does not follow the concise description style from the example 'Image X: The description of the diagram.'. It should be shortened to 'Image 6: The Orchestrat...\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m-------------------------------------------- Review 19 --------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"structure_profile\",\n",
      "  \"Location\": \"Image 7: An Evaluator-Optimizer loop for LLM outputs, showing an iterative refinement process.\",\n",
      "  \"Comment\": \"The caption for Image 7 is too verbose and does not follow the concise description style from the example 'Image X: The description of the diagram.'. It should be shortened to 'Image 7: An Evaluator-O...\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m-------------------------------------------- Review 20 --------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"structure_profile\",\n",
      "  \"Location\": \"Image 8: A ReAct AI agent diagram illustrating the core components and the iterative 'Act, Observe, Reason' cycle.\",\n",
      "  \"Comment\": \"The caption for Image 8 is too verbose and does not follow the concise description style from the example 'Image X: The description of the diagram.'. It should be shortened to 'Image 8: A ReAct AI age...\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m-------------------------------------------- Review 21 --------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"structure_profile\",\n",
      "  \"Location\": \"Image 9: Document summarization and analysis workflow by Gemini in Google Workspace\",\n",
      "  \"Comment\": \"The caption for Image 9 is too verbose and does not follow the concise description style from the example 'Image X: The description of the diagram.'. It should be shortened to 'Image 9: Document summa...\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m-------------------------------------------- Review 22 --------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"structure_profile\",\n",
      "  \"Location\": \"Image 10: Operational loop of the Gemini CLI coding assistant leveraging the ReAct agent architecture.\",\n",
      "  \"Comment\": \"The caption for Image 10 is too verbose and does not follow the concise description style from the example 'Image X: The description of the diagram.'. It should be shortened to 'Image 10: Operational ...\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m-------------------------------------------- Review 23 --------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"structure_profile\",\n",
      "  \"Location\": \"Image 11: An iterative multi-step process that could power Perplexity's Deep Research agent.\",\n",
      "  \"Comment\": \"The caption for Image 11 is too verbose and does not follow the concise description style from the example 'Image X: The description of the diagram.'. It should be shortened to 'Image 11: An iterative...\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m-------------------------------------------- Review 24 --------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"terminology_profile\",\n",
      "  \"Location\": \"Choosing Your Path - Fourth paragraph\",\n",
      "  \"Comment\": \"The phrase 'paramount' is used in the fourth paragraph of the 'Choosing Your Path' section. According to the 'AI Slop Banned Words List' in the terminology profile, 'paramount' is a banned word and sh...\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m-------------------------------------------- Review 25 --------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"terminology_profile\",\n",
      "  \"Location\": \"Conclusion: The Challenges of Every AI Engineer - First paragraph\",\n",
      "  \"Comment\": \"The phrase 'paramount' is used in the 'Conclusion: The Challenges of Every AI Engineer' section. According to the 'AI Slop Banned Words List' in the terminology profile, 'paramount' is a banned word a...\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m-------------------------------------------- Review 26 --------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"terminology_profile\",\n",
      "  \"Location\": \"Choosing Your Path - Sixth paragraph\",\n",
      "  \"Comment\": \"The phrase 'dramatically' is used in the sixth paragraph of the 'Choosing Your Path' section. According to the 'AI Slop Banned Words List' in the terminology profile, 'dramatically' is a banned word a...\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m-------------------------------------------- Review 27 --------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"tonality_profile\",\n",
      "  \"Location\": \"Introduction: The Critical Decision Every AI Engineer Faces - Fourth paragraph\",\n",
      "  \"Comment\": \"The fourth paragraph in the 'Introduction' section contains the sentence 'This lesson will provide a framework to help you make this critical decision with confidence.' This phrasing is similar to the...\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m-------------------------------------------- Review 28 --------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"tonality_profile\",\n",
      "  \"Location\": \"Conclusion: The Challenges of Every AI Engineer - Last paragraph\",\n",
      "  \"Comment\": \"The last paragraph in the 'Conclusion' section contains the sentence 'Your path forward as an AI engineer is about mastering these realities. By the end of this course, you will have the knowledge to ...\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m-------------------------------------------- Review 29 --------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"tonality_profile\",\n",
      "  \"Location\": \"Introduction: The Critical Decision Every AI Engineer Faces - Third paragraph\",\n",
      "  \"Comment\": \"The third paragraph in the 'Introduction' section uses the phrase 'billion-dollar AI startups succeed or fail based primarily on this architectural decision'. This type of phrasing, using strong, almo...\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m-------------------------------------------- Review 30 --------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"tonality_profile\",\n",
      "  \"Location\": \"Choosing Your Path - Fifth paragraph\",\n",
      "  \"Comment\": \"The fifth paragraph in 'Choosing Your Path' uses the phrase 'notoriously difficult to debug and evaluate' and 'plenty of jokes in the developer community about AI agents deleting entire codebases'. Wh...\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model = get_model(SupportedModels.GOOGLE_GEMINI_25_FLASH)\n",
    "reviewer = ArticleReviewer(\n",
    "    to_review=article,\n",
    "    article_guideline=article_guideline,\n",
    "    article_profiles=article_profiles,\n",
    "    model=model,\n",
    ")\n",
    "\n",
    "print(\"Reviewing article...\")\n",
    "article_reviews = await reviewer.ainvoke()\n",
    "\n",
    "pretty_print.wrapped(f\"Generated {len(article_reviews.reviews)} reviews:\", title=\"Article reviews\")\n",
    "for i, review in enumerate(article_reviews.reviews, 1):\n",
    "    review_dict = {\n",
    "        \"Profile\": review.profile,\n",
    "        \"Location\": review.location,\n",
    "        \"Comment\": review.comment[:200] + \"...\" if len(review.comment) > 200 else review.comment,\n",
    "    }\n",
    "    pretty_print.wrapped(review_dict, title=f\"Review {i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Reviewing Selected Text\n",
    "\n",
    "Now let's review only a specific section of the article:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m------------------------------------- Selected text to review -------------------------------------\u001b[0m\n",
      "  Selected text: 2577 characters\n",
      "Lines: 11-44\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m-------------------------- Selected text context (first 1500 characters) --------------------------\u001b[0m\n",
      "  \n",
      "<selected_text>\n",
      "    \n",
      "    <content>## Understanding the Spectrum: From Workflows to Agents\n",
      "\n",
      "Before we can choose between workflows and agents, we need a clear understanding of what they are. Rather than focusing on the technical specifics, let's look at their core properties and how they function in practice.\n",
      "\n",
      "An LLM workflow is a sequence of tasks that involves LLM calls or other operations, such as reading from a database or writing to a file system. It is largely predefined and orchestrated by developer-written code. The steps are defined in advance, resulting in deterministic or rule-based paths with predictable execution and explicit control flow. Think of it like a factory assembly line, where each station performs a specific, repeatable task in a set order.\n",
      "\n",
      "```mermaid\n",
      "graph TD\n",
      "    A[\"Input\"] --> B[\"tool_calling_llm\"]\n",
      "    B --> C[\"Tools\"]\n",
      "    C --> D[\"End\"]\n",
      "```\n",
      "Image 1: A simple LLM workflow illustrating a sequence of predefined tasks, where an input leads to a tool-calling LLM, which interacts with tools before the workflow ends.\n",
      "\n",
      "On the other end of the spectrum, we have AI agents. These are systems where an LLM plays a central role in dynamically deciding the sequence of steps, reasoning, and actions to achieve a goal. The steps are not defined in advance but are planned based on the task and the current state of the environment. This makes them adaptive and capable of handling novelty. You can think of an agent as a skilled human expert tackling an unfamiliar probl\n",
      "...\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from brown.entities.articles import SelectedText\n",
    "\n",
    "# Let's extract a specific section to review\n",
    "article_lines = article.content.split(\"\\n\")\n",
    "first_line_number = 11\n",
    "last_line_number = 44\n",
    "selected_content = \"\\n\".join(article_lines[first_line_number:last_line_number])\n",
    "\n",
    "selected_text = SelectedText(\n",
    "    article=article,\n",
    "    content=selected_content,\n",
    "    first_line_number=first_line_number,\n",
    "    last_line_number=last_line_number,\n",
    ")\n",
    "\n",
    "text = [\n",
    "    f\"Selected text: {len(selected_content)} characters\",\n",
    "    f\"Lines: {selected_text.first_line_number}-{selected_text.last_line_number}\",\n",
    "]\n",
    "pretty_print.wrapped(\"\\n\".join(text), title=\"Selected text to review\")\n",
    "pretty_print.wrapped(f\"{selected_text.to_context()[:1500]}\\n...\", title=\"Selected text context (first 1500 characters)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's review the selected text (note how we used the same `ArticleReviewer` class for both inputs containing the business logic in a single place):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviewing selected text...\n",
      "\u001b[93m-------------------------------------- Selected text reviews --------------------------------------\u001b[0m\n",
      "  Generated 4 reviews for selected text:\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m--------------------------------------------- Review 1 ---------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"mechanics_profile\",\n",
      "  \"Location\": \"Understanding the Spectrum: From Workflows to Agents - First paragraph\",\n",
      "  \"Comment\": \"The sentence 'Before we can choose between workflows and agents, we need a clear understanding of what they are.' uses 'we' to refer to both the course team and the reader. According to the mechanics_...\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m--------------------------------------------- Review 2 ---------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"mechanics_profile\",\n",
      "  \"Location\": \"Understanding the Spectrum: From Workflows to Agents - Sixth paragraph\",\n",
      "  \"Comment\": \"The sentence 'We will explore advanced workflow patterns like chaining, routing, and the orchestrator-worker model in future lessons, as well as the core components of agents like tools, memory, and t...\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m--------------------------------------------- Review 3 ---------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"terminology_profile\",\n",
      "  \"Location\": \"Understanding the Spectrum: From Workflows to Agents - Sixth paragraph\",\n",
      "  \"Comment\": \"The word 'significantly' is listed as an AI Slop Banned Word. The sentence 'Both workflows and agents require an orchestration layer, but its role differs significantly.' should be rephrased to avoid ...\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m--------------------------------------------- Review 4 ---------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"article_guideline\",\n",
      "  \"Location\": \"Understanding the Spectrum: From Workflows to Agents - Section level\",\n",
      "  \"Comment\": \"The 'Section length: 200 words (without the mermaid diagram code)' constraint for Section 2 has not been met. The current text for this section (excluding the Mermaid diagrams and their captions) is a...\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model = get_model(SupportedModels.GOOGLE_GEMINI_25_FLASH)\n",
    "reviewer = ArticleReviewer(\n",
    "    to_review=selected_text,  # Now passing SelectedText instead of Article\n",
    "    article_guideline=article_guideline,\n",
    "    article_profiles=article_profiles,\n",
    "    model=model,\n",
    ")\n",
    "\n",
    "print(\"Reviewing selected text...\")\n",
    "selected_text_reviews = await reviewer.ainvoke()\n",
    "\n",
    "pretty_print.wrapped(\n",
    "    f\"Generated {len(selected_text_reviews.reviews)} reviews for selected text:\", title=\"Selected text reviews\"\n",
    ")\n",
    "for i, review in enumerate(selected_text_reviews.reviews, 1):\n",
    "    review_dict = {\n",
    "        \"Profile\": review.profile,\n",
    "        \"Location\": review.location,\n",
    "        \"Comment\": review.comment[:200] + \"...\" if len(review.comment) > 200 else review.comment,\n",
    "    }\n",
    "    pretty_print.wrapped(review_dict, title=f\"Review {i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hooking Reviews to the Article Writer\n",
    "\n",
    "Now let's see how the `ArticleWriter` node handles reviews to act as the **optimizer** in our evaluator-optimizer pattern.\n",
    "\n",
    "### Design Philosophy\n",
    "\n",
    "To keep the \"writing\" logic contained and avoid duplicated code, the `ArticleWriter` serves dual purposes:\n",
    "\n",
    "1. **Writer**: Generates the initial article draft\n",
    "2. **Editor**: Edits the article based on reviews\n",
    "\n",
    "This mirrors real-world writing processes where the original author both writes and edits their own work based on feedback. It keeps all writing knowledge in one place.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changes to ArticleWriter __init__\n",
    "\n",
    "The `ArticleWriter` now accepts an optional `reviews` parameter:\n",
    "```python\n",
    "class ArticleWriter(Node):\n",
    "    def __init__(\n",
    "        self,\n",
    "        article_guideline: ArticleGuideline,\n",
    "        research: Research,\n",
    "        article_profiles: ArticleProfiles,\n",
    "        media_items: MediaItems,\n",
    "        article_examples: ArticleExamples,\n",
    "        model: Runnable,\n",
    "        reviews: ArticleReviews | SelectedTextReviews | None = None,  # NEW!\n",
    "    ) -> None:\n",
    "        super().__init__(model, toolkit=Toolkit(tools=[]))\n",
    "        \n",
    "        self.article_guideline = article_guideline\n",
    "        self.research = research\n",
    "        self.article_profiles = article_profiles\n",
    "        self.media_items = media_items\n",
    "        self.article_examples = article_examples\n",
    "        self.reviews = reviews  # Store reviews for editing mode\n",
    "```\n",
    "\n",
    "**Key Insight:**\n",
    "\n",
    "When `reviews=None`, the writer generates a new article from scratch. When reviews are provided, it edits the existing article based on the feedback.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changes to the ainvoke Method\n",
    "\n",
    "The `ainvoke` method now handles both writing and editing:\n",
    "```python\n",
    "async def ainvoke(self) -> Article | SelectedText:\n",
    "    # Step 1: Build the main system prompt (same as before)\n",
    "    system_prompt = self.system_prompt_template.format(\n",
    "        article_guideline=self.article_guideline.to_context(),\n",
    "        research=self.research.to_context(),\n",
    "        # ... all other context ...\n",
    "    )\n",
    "    \n",
    "    user_input_content = self.build_user_input_content(\n",
    "        inputs=[system_prompt], \n",
    "        image_urls=self.research.image_urls\n",
    "    )\n",
    "    inputs = [{\"role\": \"user\", \"content\": user_input_content}]\n",
    "    \n",
    "    # Step 2: If reviews exist, add them to the conversation\n",
    "    if self.reviews:\n",
    "        # First, provide the previously written article as the assistant's response.\n",
    "        # This is important because the editing will be done relative to the article.\n",
    "        # Thus, we have to anchor the reviews on the evaluated article.\n",
    "        inputs.extend([\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": self.reviews.article.to_context(),\n",
    "            },\n",
    "        ])\n",
    "        \n",
    "        # Then, provide the reviews as user feedback, along with a new system prompt that\n",
    "        # instructs the agent on how to edit the article. In this way, we can \"hijack\"\n",
    "        # the original system template to edit the article instead of writing it from scratch.\n",
    "        if isinstance(self.reviews, ArticleReviews):\n",
    "            reviews_prompt = self.article_reviews_prompt_template.format(\n",
    "                reviews=self.reviews.to_context(include_article=False),\n",
    "            )\n",
    "        elif isinstance(self.reviews, SelectedTextReviews):\n",
    "            reviews_prompt = self.selected_text_reviews_prompt_template.format(\n",
    "                selected_text=self.reviews.selected_text.to_context(),\n",
    "                reviews=self.reviews.to_context(include_article=False),\n",
    "            )\n",
    "        \n",
    "        inputs.extend([{\"role\": \"user\", \"content\": reviews_prompt}])\n",
    "    \n",
    "    # Step 3: Generate/edit the article\n",
    "    written_output = await self.model.ainvoke(inputs)\n",
    "    written_output = cast(str, written_output.text)\n",
    "    \n",
    "    # Step 4: Return appropriate type\n",
    "    if isinstance(self.reviews, SelectedTextReviews):\n",
    "        return SelectedText(\n",
    "            article=self.reviews.article,\n",
    "            content=written_output,\n",
    "            first_line_number=self.reviews.selected_text.first_line_number,\n",
    "            last_line_number=self.reviews.selected_text.last_line_number,\n",
    "        )\n",
    "    else:\n",
    "        return Article(content=written_output)\n",
    "```\n",
    "\n",
    "**Context engineering for editing:**\n",
    "\n",
    "1. **User**: System prompt with all context (guidelines, profiles, etc.)\n",
    "2. **Assistant**: The previously written article\n",
    "3. **User**: The reviews with specific issues to fix\n",
    "4. **Assistant**: The edited article (generated by LLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Review Prompt Templates\n",
    "\n",
    "The writer has two additional prompt templates for handling reviews. One for editing the whole article and whole for the selected text.\n",
    "\n",
    "**1. Article Reviews Prompt:**\n",
    "\n",
    "```python\n",
    "article_reviews_prompt_template = \"\"\"\n",
    "We personally reviewed the article and compiled a list of reviews based on which you have to \n",
    "edit the article you wrote one step before.\n",
    "\n",
    "## Reviewing Logic\n",
    "\n",
    "Here is how we created the feedback reviews:\n",
    "- We compared the article against the <article_guideline> to ensure it follows user intent\n",
    "- We compared against all profile constraints\n",
    "- Manual human reviews create special \"human_feedback\" reviews (highest priority)\n",
    "- For each broken rule, we created a review\n",
    "\n",
    "## Ranking the Importance of the Reviews\n",
    "\n",
    "1. Always prioritize the human feedback reviews above everything else\n",
    "2. Next prioritize reviews based on the <article_guideline>\n",
    "3. Finally prioritize reviews based on other profiles\n",
    "\n",
    "## Reviews\n",
    "\n",
    "Here are the reviews you have to fix:\n",
    "{reviews}\n",
    "\n",
    "## Chain of Thought\n",
    "\n",
    "1. Analyze the reviews to understand what needs to be changed\n",
    "2. Prioritize the reviews based on the importance ranking\n",
    "3. Apply necessary edits while following all instructions from profiles and guidelines\n",
    "4. Ensure edited text is still anchored in <research> and <article_guideline>\n",
    "5. Ensure edited text flows naturally with surrounding content\n",
    "6. Return the fully edited article\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**Key Features:**\n",
    "\n",
    "- Explains the review creation process\n",
    "- Provides clear priority ranking\n",
    "- New chain of thought section adding the new reasoning steps and final task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Selected Text Reviews Prompt:**\n",
    "\n",
    "```python\n",
    "selected_text_reviews_prompt_template = \"\"\"\n",
    "We personally reviewed only a portion of the article and compiled reviews for editing just that \n",
    "selected text.\n",
    "\n",
    "## Selected Text to Edit\n",
    "\n",
    "{selected_text}\n",
    "\n",
    "Remember this selected text is part of the article from one step before. Anchor your editing within \n",
    "the broader context of the article.\n",
    "\n",
    "Selected text editing guidelines:\n",
    "- Keep selected text consistent with surrounding article context\n",
    "- Use first and last line numbers to locate the selection\n",
    "- Only edit the selected text, don't modify the entire article\n",
    "\n",
    "## [Rest similar to article reviews prompt - reviewing logic, priority ranking, etc.]\n",
    "\n",
    "{reviews}\n",
    "\n",
    "## Chain of Thought\n",
    "\n",
    "1. Place the selected text in context of the full article\n",
    "2. Analyze the reviews\n",
    "3. Prioritize reviews based on importance ranking\n",
    "4. Apply edits while following all instructions\n",
    "5. Ensure edited selected text is still anchored in research/guideline\n",
    "6. Ensure edited selected text flows naturally with surrounding content\n",
    "7. Return the fully edited selected text\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "The prompt is similar to the one for editing the whole article, but we added special details on clearly explaining how to manipulate the selected text. Remember that LLMs have zero clue of what is going on within your application and business logic. Thus, you have to explain all your processes super clearly for this to work well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. End-to-End Example: Review and Edit Loop\n",
    "\n",
    "Now let's run a complete example showing the full workflow: generate media, write article, review, and edit. First, let's run it without LangGraph. Next, we will glue everything together into a standalone LangGraph workflow that can further be shipped to production.\n",
    "\n",
    "This time, we will use a more comprehensive test sample containing a more detailed `article_guideline.md` and more facts within the `research.md` file to show what a professional input would look like. \n",
    "\n",
    "We recommend opening the new `article_guideline.md` from `02_sample_medium` and comparing it to the one from `01_sample_small`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAMPLE_DIR = Path(\"inputs/tests/02_sample_medium\")\n",
    "SAMPLE_DIR.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load all necessary context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "  STEP 1: Loading Context\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "âœ“ Guideline: 22,868 characters\n",
      "âœ“ Research: 211,790 characters, 15 images\n",
      "âœ“ Profiles: 6 profiles loaded\n",
      "âœ“ Examples: 2 article examples\n"
     ]
    }
   ],
   "source": [
    "from brown.loaders import (\n",
    "    MarkdownArticleExampleLoader,\n",
    "    MarkdownArticleGuidelineLoader,\n",
    "    MarkdownArticleProfilesLoader,\n",
    "    MarkdownResearchLoader,\n",
    ")\n",
    "\n",
    "pretty_print.wrapped(\"STEP 1: Loading Context\", width=100)\n",
    "\n",
    "# Load guideline\n",
    "guideline_loader = MarkdownArticleGuidelineLoader(uri=Path(\"article_guideline.md\"))\n",
    "article_guideline = guideline_loader.load(working_uri=SAMPLE_DIR)\n",
    "\n",
    "# Load research\n",
    "research_loader = MarkdownResearchLoader(uri=Path(\"research.md\"))\n",
    "research = research_loader.load(working_uri=SAMPLE_DIR)\n",
    "\n",
    "# Load profiles\n",
    "profiles_input = {\n",
    "    \"article\": PROFILES_DIR / \"article_profile.md\",\n",
    "    \"character\": PROFILES_DIR / \"character_profiles\" / \"paul_iusztin.md\",\n",
    "    \"mechanics\": PROFILES_DIR / \"mechanics_profile.md\",\n",
    "    \"structure\": PROFILES_DIR / \"structure_profile.md\",\n",
    "    \"terminology\": PROFILES_DIR / \"terminology_profile.md\",\n",
    "    \"tonality\": PROFILES_DIR / \"tonality_profile.md\",\n",
    "}\n",
    "profiles_loader = MarkdownArticleProfilesLoader(uri=profiles_input)\n",
    "article_profiles = profiles_loader.load()\n",
    "\n",
    "# Load examples\n",
    "examples_loader = MarkdownArticleExampleLoader(uri=EXAMPLES_DIR)\n",
    "article_examples = examples_loader.load()\n",
    "\n",
    "print(f\"âœ“ Guideline: {len(article_guideline.content):,} characters\")\n",
    "print(f\"âœ“ Research: {len(research.content):,} characters, {len(research.image_urls)} images\")\n",
    "print(f\"âœ“ Profiles: {len(profiles_input)} profiles loaded\")\n",
    "print(f\"âœ“ Examples: {len(article_examples.examples)} article examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Generate media items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "  Analyzing article guideline for media requirements...\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "  Found {len(media_jobs)} media items to generate\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m---------------------------------------------- Job 1 ----------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Tool\": \"mermaid_diagram_generator_tool\",\n",
      "  \"Description\": \"A flowchart illustrating the AI generation and human verification loop. The process starts with AI G...\",\n",
      "  \"Section\": \"AI Generation and Human Verification Loop\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "  Generating media items in parallel...\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "  Generated 1 media items successfully!\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "from brown.entities.media_items import MediaItem, MediaItems\n",
    "from brown.models import SupportedModels, get_model\n",
    "from brown.nodes import MediaGeneratorOrchestrator, MermaidDiagramGenerator, Toolkit\n",
    "\n",
    "# Create worker tool\n",
    "diagram_model = get_model(SupportedModels.GOOGLE_GEMINI_25_FLASH)\n",
    "mermaid_generator = MermaidDiagramGenerator(model=diagram_model)\n",
    "toolkit = Toolkit(tools=[mermaid_generator.as_tool()])\n",
    "\n",
    "# Create orchestrator\n",
    "orchestrator_model = get_model(SupportedModels.GOOGLE_GEMINI_25_FLASH)\n",
    "orchestrator = MediaGeneratorOrchestrator(\n",
    "    article_guideline=article_guideline,\n",
    "    research=research,\n",
    "    model=orchestrator_model,\n",
    "    toolkit=toolkit,\n",
    ")\n",
    "\n",
    "# Get media generation jobs\n",
    "pretty_print.wrapped(\"Analyzing article guideline for media requirements...\")\n",
    "media_jobs = await orchestrator.ainvoke()\n",
    "\n",
    "\n",
    "pretty_print.wrapped(\"Found {len(media_jobs)} media items to generate\")\n",
    "media_jobs_dict = {}\n",
    "for i, job in enumerate(media_jobs):\n",
    "    pretty_print.wrapped(\n",
    "        {\n",
    "            \"Tool\": job[\"name\"],\n",
    "            \"Description\": job[\"args\"].get(\"description_of_the_diagram\", \"N/A\")[:100] + \"...\",\n",
    "            \"Section\": job[\"args\"].get(\"section_title\", \"N/A\"),\n",
    "        },\n",
    "        title=f\"Job {i + 1}\",\n",
    "    )\n",
    "\n",
    "pretty_print.wrapped(\"Generating media items in parallel...\")\n",
    "coroutines = []\n",
    "for job in media_jobs:\n",
    "    tool = orchestrator.toolkit.get_tool_by_name(job[\"name\"])\n",
    "    if tool:\n",
    "        coroutines.append(tool.ainvoke(job[\"args\"]))\n",
    "\n",
    "media_items = await asyncio.gather(*coroutines)\n",
    "media_items = MediaItems.build(media_items=media_items)\n",
    "\n",
    "pretty_print.wrapped(f\"Generated {len(media_items.media_items)} media items successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Write the first draft of the article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "  STEP 3: Writing Article (First Draft)\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "This may take 1-2 minutes...\n",
      "âœ“ Article generated: 25,570 characters\n",
      "\u001b[93m-------------------------------------------- First Draft (First 1000 chars) --------------------------------------------\u001b[0m\n",
      "  # Workflows vs. Agents: The Critical Decision Every AI Engineer Faces\n",
      "### Navigating the architectural spectrum to ship reliable AI products\n",
      "\n",
      "Building AI applications today means facing a fundamental architectural choice right from the start. As an engineer, you decide whether to create a predictable, step-by-step workflow with explicit control over every action or an autonomous agent that thinks and decides for itself. This isn't a trivial decision; it shapes everything from development time and costs to reliability and the overall user experience.\n",
      "\n",
      "Choose the wrong path, and you might end up with an overly rigid system that struggles with new features or unexpected user inputs. Alternatively, you could find yourself with an unpredictable agent that shines in demos but fails catastrophically when it matters most. I've seen countless teams, including my own, waste months rebuilding architectures, leaving users frustrated and executives questioning the spiraling costs. The truth is, man\n",
      "\u001b[93m------------------------------------------------------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from brown.nodes import ArticleWriter\n",
    "\n",
    "pretty_print.wrapped(\"STEP 3: Writing Article (First Draft)\", width=100)\n",
    "print(\"This may take 1-2 minutes...\")\n",
    "\n",
    "writer_model = get_model(SupportedModels.GOOGLE_GEMINI_25_FLASH)\n",
    "article_writer = ArticleWriter(\n",
    "    article_guideline=article_guideline,\n",
    "    research=research,\n",
    "    article_profiles=article_profiles,\n",
    "    media_items=media_items,\n",
    "    article_examples=article_examples,\n",
    "    model=writer_model,\n",
    "    reviews=None,  # No reviews for first draft\n",
    ")\n",
    "\n",
    "article = await article_writer.ainvoke()\n",
    "\n",
    "print(f\"âœ“ Article generated: {len(article.content):,} characters\")\n",
    "pretty_print.wrapped(article.content[:1000], title=\"First Draft (First 1000 chars)\", width=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Review the article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "  STEP 4: Reviewing Article\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "âœ“ Generated 52 reviews\n",
      "\u001b[93m--------------------------------------------- Review 1 ---------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"structure_profile\",\n",
      "  \"Location\": \"Introduction - First paragraph\",\n",
      "  \"Comment\": \"The first paragraph of the introduction is 90 words long, exceeding the maximum allowed paragraph length of 80 words.\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m--------------------------------------------- Review 2 ---------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"structure_profile\",\n",
      "  \"Location\": \"Introduction - Second paragraph\",\n",
      "  \"Comment\": \"The second paragraph of the introduction is 81 words long, exceeding the maximum allowed paragraph length of 80 words.\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m--------------------------------------------- Review 3 ---------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"mechanics_profile\",\n",
      "  \"Location\": \"Introduction - Second paragraph\",\n",
      "  \"Comment\": \"The sentence 'I've seen countless teams, including my own, waste months rebuilding architectures, leaving users frustrated and executives questioning ...\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m--------------------------------------------- Review 4 ---------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"structure_profile\",\n",
      "  \"Location\": \"Introduction - Third paragraph\",\n",
      "  \"Comment\": \"The third paragraph of the introduction is 69 words long, but the section length guideline is 300 words. This paragraph is too short to meet the secti...\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m--------------------------------------------- Review 5 ---------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"article_guideline\",\n",
      "  \"Location\": \"Section 1 - Introduction: The Critical Decision Every AI Engineer Faces\",\n",
      "  \"Comment\": \"The introduction is 240 words long, which is less than the specified 300-word length in the article guideline.\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m--------------------------------------------- Review 6 ---------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"terminology_profile\",\n",
      "  \"Location\": \"Introduction - Second paragraph\",\n",
      "  \"Comment\": \"The phrase 'spiraling costs' is considered AI slop and should be rephrased to be more direct and simple.\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m--------------------------------------------- Review 7 ---------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"structure_profile\",\n",
      "  \"Location\": \"Section 2 - Understanding the Spectrum: From Workflows to Agents - Third paragraph\",\n",
      "  \"Comment\": \"The third paragraph under 'LLM Workflows' is 81 words long, exceeding the maximum allowed paragraph length of 80 words.\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m--------------------------------------------- Review 8 ---------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"structure_profile\",\n",
      "  \"Location\": \"Section 2 - Understanding the Spectrum: From Workflows to Agents - Fifth paragraph\",\n",
      "  \"Comment\": \"The fifth paragraph under 'AI Agents' is 82 words long, exceeding the maximum allowed paragraph length of 80 words.\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m--------------------------------------------- Review 9 ---------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"article_guideline\",\n",
      "  \"Location\": \"Section 2 - Understanding the Spectrum: From Workflows to Agents\",\n",
      "  \"Comment\": \"The total word count for Section 2 is approximately 250 words, which is significantly less than the 400 words required by the article guideline.\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m-------------------------------------------- Review 10 --------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"structure_profile\",\n",
      "  \"Location\": \"Section 3: Choosing Your Path - Third paragraph\",\n",
      "  \"Comment\": \"The third paragraph under 'When to Use LLM Workflows' is 82 words long, exceeding the maximum allowed paragraph length of 80 words.\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m-------------------------------------------- Review 11 --------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"structure_profile\",\n",
      "  \"Location\": \"Section 3: Choosing Your Path - Fifth paragraph\",\n",
      "  \"Comment\": \"The fifth paragraph under 'When to Use LLM Workflows' is 83 words long, exceeding the maximum allowed paragraph length of 80 words.\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m-------------------------------------------- Review 12 --------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"structure_profile\",\n",
      "  \"Location\": \"Section 3: Choosing Your Path - Sixth paragraph\",\n",
      "  \"Comment\": \"The sixth paragraph under 'When to Use LLM Workflows' is 83 words long, exceeding the maximum allowed paragraph length of 80 words.\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m-------------------------------------------- Review 13 --------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"structure_profile\",\n",
      "  \"Location\": \"Section 3: Choosing Your Path - Seventh paragraph\",\n",
      "  \"Comment\": \"The seventh paragraph under 'When to Use LLM Workflows' is 82 words long, exceeding the maximum allowed paragraph length of 80 words.\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m-------------------------------------------- Review 14 --------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"structure_profile\",\n",
      "  \"Location\": \"Section 3: Choosing Your Path - Eighth paragraph\",\n",
      "  \"Comment\": \"The eighth paragraph under 'When to Use LLM Workflows' is 84 words long, exceeding the maximum allowed paragraph length of 80 words.\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m-------------------------------------------- Review 15 --------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"structure_profile\",\n",
      "  \"Location\": \"Section 3: Choosing Your Path - Ninth paragraph\",\n",
      "  \"Comment\": \"The ninth paragraph under 'When to Use LLM Workflows' is 84 words long, exceeding the maximum allowed paragraph length of 80 words.\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m-------------------------------------------- Review 16 --------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"structure_profile\",\n",
      "  \"Location\": \"Section 3: Choosing Your Path - Eleventh paragraph\",\n",
      "  \"Comment\": \"The eleventh paragraph under 'When to Use AI Agents' is 83 words long, exceeding the maximum allowed paragraph length of 80 words.\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m-------------------------------------------- Review 17 --------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"structure_profile\",\n",
      "  \"Location\": \"Section 3: Choosing Your Path - Twelfth paragraph\",\n",
      "  \"Comment\": \"The twelfth paragraph under 'When to Use AI Agents' is 84 words long, exceeding the maximum allowed paragraph length of 80 words.\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m-------------------------------------------- Review 18 --------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"mechanics_profile\",\n",
      "  \"Location\": \"Section 3: Choosing Your Path - Fourteenth paragraph\",\n",
      "  \"Comment\": \"The phrase 'giving more control to the agent with fewer human-in-the-loop steps' uses the forbidden 'human-in-the-loop' term, which should be avoided ...\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m-------------------------------------------- Review 19 --------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"article_guideline\",\n",
      "  \"Location\": \"Section 3: Choosing Your Path - Fifteenth paragraph\",\n",
      "  \"Comment\": \"The article guideline explicitly requests a mermaid diagram to illustrate the 'AI generation and human verification loop'. The provided mermaid diagra...\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m-------------------------------------------- Review 20 --------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"article_guideline\",\n",
      "  \"Location\": \"Section 3: Choosing Your Path\",\n",
      "  \"Comment\": \"The total word count for Section 3 is approximately 700 words (excluding the mermaid diagram code), which exceeds the 500 words required by the articl...\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m-------------------------------------------- Review 21 --------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"terminology_profile\",\n",
      "  \"Location\": \"Section 3: Choosing Your Path - Fourth paragraph\",\n",
      "  \"Comment\": \"The word 'paramount' is in the AI Slop Banned Words List and should be replaced with simpler language.\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m-------------------------------------------- Review 22 --------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"terminology_profile\",\n",
      "  \"Location\": \"Section 3: Choosing Your Path - Fifth paragraph\",\n",
      "  \"Comment\": \"The word 'crucially' is in the AI Slop Banned Words List and should be replaced with simpler language.\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m-------------------------------------------- Review 23 --------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"terminology_profile\",\n",
      "  \"Location\": \"Section 3: Choosing Your Path - Fourteenth paragraph\",\n",
      "  \"Comment\": \"The phrase 'ultimate goal' should be rephrased to avoid AI slop and be more direct.\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m-------------------------------------------- Review 24 --------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"structure_profile\",\n",
      "  \"Location\": \"Section 4: Exploring Common Patterns - First paragraph\",\n",
      "  \"Comment\": \"The first paragraph is 85 words long, exceeding the maximum allowed paragraph length of 80 words.\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m-------------------------------------------- Review 25 --------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"structure_profile\",\n",
      "  \"Location\": \"Section 4: Exploring Common Patterns - Second paragraph\",\n",
      "  \"Comment\": \"The second paragraph under 'LLM Workflows' is 81 words long, exceeding the maximum allowed paragraph length of 80 words.\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m-------------------------------------------- Review 26 --------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"structure_profile\",\n",
      "  \"Location\": \"Section 4: Exploring Common Patterns - Third paragraph\",\n",
      "  \"Comment\": \"The third paragraph under 'LLM Workflows' is 88 words long, exceeding the maximum allowed paragraph length of 80 words.\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m-------------------------------------------- Review 27 --------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"structure_profile\",\n",
      "  \"Location\": \"Section 4: Exploring Common Patterns - Eighth paragraph\",\n",
      "  \"Comment\": \"The eighth paragraph under 'Core Components of a ReAct AI Agent' is 83 words long, exceeding the maximum allowed paragraph length of 80 words.\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m-------------------------------------------- Review 28 --------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"article_guideline\",\n",
      "  \"Location\": \"Section 4: Exploring Common Patterns\",\n",
      "  \"Comment\": \"The article guideline specifies drawing a mermaid diagram showing 'the dynamics between the core components of an AI agent illustrating the ReAct patt...\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m-------------------------------------------- Review 29 --------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"article_guideline\",\n",
      "  \"Location\": \"Section 4: Exploring Common Patterns\",\n",
      "  \"Comment\": \"The total word count for Section 4 is approximately 480 words, which is less than the 550 words required by the article guideline.\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m-------------------------------------------- Review 30 --------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"terminology_profile\",\n",
      "  \"Location\": \"Section 4: Exploring Common Patterns - Fourth paragraph\",\n",
      "  \"Comment\": \"The phrase 'drastically be improved' should be rephrased to avoid AI slop and be more direct.\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m-------------------------------------------- Review 31 --------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"terminology_profile\",\n",
      "  \"Location\": \"Section 4: Exploring Common Patterns - Fifth paragraph\",\n",
      "  \"Comment\": \"The sentence 'This is the core of a ReAct agent.' uses 'This is where X comes in' pattern (indirectly) which is listed in the AI Slop Banned Expressio...\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m-------------------------------------------- Review 32 --------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"structure_profile\",\n",
      "  \"Location\": \"Section 5: Zooming In on Our Favorite Examples - Second paragraph\",\n",
      "  \"Comment\": \"The first paragraph under 'Document Summarization and Analysis Workflow by Gemini in Google Workspace' is 80 words long, meeting the maximum but also ...\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m-------------------------------------------- Review 33 --------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"structure_profile\",\n",
      "  \"Location\": \"Section 5: Zooming In on Our Favorite Examples - Third paragraph\",\n",
      "  \"Comment\": \"The second paragraph under 'Document Summarization and Analysis Workflow by Gemini in Google Workspace' is 80 words long, meeting the maximum. It shou...\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m-------------------------------------------- Review 34 --------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"structure_profile\",\n",
      "  \"Location\": \"Section 5: Zooming In on Our Favorite Examples - Fifth paragraph\",\n",
      "  \"Comment\": \"The first paragraph under 'Gemini CLI Coding Assistant' is 84 words long, exceeding the maximum allowed paragraph length of 80 words.\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m-------------------------------------------- Review 35 --------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"structure_profile\",\n",
      "  \"Location\": \"Section 5: Zooming In on Our Favorite Examples - Sixth paragraph\",\n",
      "  \"Comment\": \"The second paragraph under 'Gemini CLI Coding Assistant' is 84 words long, exceeding the maximum allowed paragraph length of 80 words.\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m-------------------------------------------- Review 36 --------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"structure_profile\",\n",
      "  \"Location\": \"Section 5: Zooming In on Our Favorite Examples - Seventh paragraph\",\n",
      "  \"Comment\": \"The third paragraph under 'Gemini CLI Coding Assistant' is 83 words long, exceeding the maximum allowed paragraph length of 80 words.\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m-------------------------------------------- Review 37 --------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"structure_profile\",\n",
      "  \"Location\": \"Section 5: Zooming In on Our Favorite Examples - Eighth paragraph\",\n",
      "  \"Comment\": \"The fourth paragraph under 'Gemini CLI Coding Assistant' is 85 words long, exceeding the maximum allowed paragraph length of 80 words.\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m-------------------------------------------- Review 38 --------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"structure_profile\",\n",
      "  \"Location\": \"Section 5: Zooming In on Our Favorite Examples - Ninth paragraph\",\n",
      "  \"Comment\": \"The fifth paragraph under 'Gemini CLI Coding Assistant' is 87 words long, exceeding the maximum allowed paragraph length of 80 words.\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m-------------------------------------------- Review 39 --------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"structure_profile\",\n",
      "  \"Location\": \"Section 5: Zooming In on Our Favorite Examples - Thirteenth paragraph\",\n",
      "  \"Comment\": \"The first paragraph under 'Perplexity Deep Research' is 88 words long, exceeding the maximum allowed paragraph length of 80 words.\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m-------------------------------------------- Review 40 --------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"structure_profile\",\n",
      "  \"Location\": \"Section 5: Zooming In on Our Favorite Examples - Fourteenth paragraph\",\n",
      "  \"Comment\": \"The second paragraph under 'Perplexity Deep Research' is 83 words long, exceeding the maximum allowed paragraph length of 80 words.\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m-------------------------------------------- Review 41 --------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"structure_profile\",\n",
      "  \"Location\": \"Section 5: Zooming In on Our Favorite Examples - Fifteenth paragraph\",\n",
      "  \"Comment\": \"The third paragraph under 'Perplexity Deep Research' is 81 words long, exceeding the maximum allowed paragraph length of 80 words.\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m-------------------------------------------- Review 42 --------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"structure_profile\",\n",
      "  \"Location\": \"Section 5: Zooming In on Our Favorite Examples - Sixteenth paragraph\",\n",
      "  \"Comment\": \"The fourth paragraph under 'Perplexity Deep Research' is 88 words long, exceeding the maximum allowed paragraph length of 80 words.\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m-------------------------------------------- Review 43 --------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"structure_profile\",\n",
      "  \"Location\": \"Section 5: Zooming In on Our Favorite Examples - Seventeenth paragraph\",\n",
      "  \"Comment\": \"The fifth paragraph under 'Perplexity Deep Research' is 82 words long, exceeding the maximum allowed paragraph length of 80 words.\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m-------------------------------------------- Review 44 --------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"structure_profile\",\n",
      "  \"Location\": \"Section 5: Zooming In on Our Favorite Examples - Eighteenth paragraph\",\n",
      "  \"Comment\": \"The sixth paragraph under 'Perplexity Deep Research' is 86 words long, exceeding the maximum allowed paragraph length of 80 words.\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m-------------------------------------------- Review 45 --------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"article_guideline\",\n",
      "  \"Location\": \"Section 5: Zooming In on Our Favorite Examples\",\n",
      "  \"Comment\": \"The total word count for Section 5 is approximately 1100 words (excluding mermaid diagrams and images), which significantly exceeds the 900 words requ...\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m-------------------------------------------- Review 46 --------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"structure_profile\",\n",
      "  \"Location\": \"Section 6 - Conclusion: The Challenges of Every AI Engineer - Second paragraph\",\n",
      "  \"Comment\": \"The second paragraph is 88 words long, exceeding the maximum allowed paragraph length of 80 words.\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m-------------------------------------------- Review 47 --------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"structure_profile\",\n",
      "  \"Location\": \"Section 6 - Conclusion: The Challenges of Every AI Engineer - Third paragraph\",\n",
      "  \"Comment\": \"The third paragraph is 85 words long, exceeding the maximum allowed paragraph length of 80 words.\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m-------------------------------------------- Review 48 --------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"structure_profile\",\n",
      "  \"Location\": \"Section 6 - Conclusion: The Challenges of Every AI Engineer - Fourth paragraph\",\n",
      "  \"Comment\": \"The fourth paragraph is 82 words long, exceeding the maximum allowed paragraph length of 80 words.\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m-------------------------------------------- Review 49 --------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"structure_profile\",\n",
      "  \"Location\": \"Section 6 - Conclusion: The Challenges of Every AI Engineer - Fifth paragraph\",\n",
      "  \"Comment\": \"The fifth paragraph is 85 words long, exceeding the maximum allowed paragraph length of 80 words.\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m-------------------------------------------- Review 50 --------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"article_guideline\",\n",
      "  \"Location\": \"Section 6 - Conclusion: The Challenges of Every AI Engineer\",\n",
      "  \"Comment\": \"The conclusion is approximately 450 words long, which exceeds the 350 words required by the article guideline. Several paragraphs are also too long.\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m-------------------------------------------- Review 51 --------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"terminology_profile\",\n",
      "  \"Location\": \"Section 6 - Conclusion: The Challenges of Every AI Engineer - Fifth paragraph\",\n",
      "  \"Comment\": \"The phrase 'powerful but also robust, efficient, and safe' is verbose and uses AI slop. It should be rephrased to be more concise and direct.\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m-------------------------------------------- Review 52 --------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"Profile\": \"structure_profile\",\n",
      "  \"Location\": \"References\",\n",
      "  \"Comment\": \"The reference list does not use APA 7th edition format for some entries, for example, missing publication dates for several entries. For instance, (n....\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "pretty_print.wrapped(\"STEP 4: Reviewing Article\", width=100)\n",
    "\n",
    "reviewer_model = get_model(SupportedModels.GOOGLE_GEMINI_25_FLASH)\n",
    "article_reviewer = ArticleReviewer(\n",
    "    to_review=article,\n",
    "    article_guideline=article_guideline,\n",
    "    article_profiles=article_profiles,\n",
    "    model=reviewer_model,\n",
    ")\n",
    "\n",
    "article_reviews = await article_reviewer.ainvoke()\n",
    "\n",
    "print(f\"âœ“ Generated {len(article_reviews.reviews)} reviews\")\n",
    "for i, review in enumerate(article_reviews.reviews, 1):\n",
    "    pretty_print.wrapped({\n",
    "        \"Profile\": review.profile,\n",
    "        \"Location\": review.location,\n",
    "        \"Comment\": review.comment[:150] + \"...\" if len(review.comment) > 150 else review.comment\n",
    "    },title=f\"Review {i}\", width=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Edit the article based on reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "  STEP 5: Editing Article Based on Reviews\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "  This may take 1-2 minutes...\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\n",
      "\u001b[93m------------------------------------------ Edited Article ------------------------------------------\u001b[0m\n",
      "  âœ“ Article edited: {len(edited_article.content)\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m------------------------------------------ Edited Article (First 1000 chars) ------------------------------------------\u001b[0m\n",
      "  # Workflows vs. Agents: The Critical Decision Every AI Engineer Faces\n",
      "### Navigating the architectural spectrum to ship reliable AI products\n",
      "\n",
      "Building AI applications today means facing a fundamental architectural choice right from the start. As an engineer, you decide whether to create a predictable, step-by-step workflow with explicit control over every action or an autonomous agent that thinks and decides for itself. This isn't a trivial decision, as it shapes everything from development time and costs to reliability and the overall user experience.\n",
      "\n",
      "Choose the wrong path, and you might end up with an overly rigid system that struggles with new features or unexpected user inputs. You could also find yourself with an unpredictable agent that shines in demos but fails catastrophically when it matters most. We've seen countless teams, including our own, waste months rebuilding architectures, leaving users frustrated and executives questioning the escalating costs.\n",
      "\n",
      "The truth is, many b\n",
      "\u001b[93m------------------------------------------------------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "pretty_print.wrapped(\"STEP 5: Editing Article Based on Reviews\", width=100)\n",
    "pretty_print.wrapped(\"This may take 1-2 minutes...\", width=100)\n",
    "print()\n",
    "\n",
    "editor_model = get_model(SupportedModels.GOOGLE_GEMINI_25_FLASH)\n",
    "article_editor = ArticleWriter(\n",
    "    article_guideline=article_guideline,\n",
    "    research=research,\n",
    "    article_profiles=article_profiles,\n",
    "    media_items=media_items,\n",
    "    article_examples=article_examples,\n",
    "    model=editor_model,\n",
    "    reviews=article_reviews,  # Pass reviews to trigger editing mode\n",
    ")\n",
    "\n",
    "edited_article = await article_editor.ainvoke()\n",
    "\n",
    "pretty_print.wrapped(\"âœ“ Article edited: {len(edited_article.content)\", title=\"Edited Article\", width=100)\n",
    "pretty_print.wrapped(edited_article.content[:1000], title=\"Edited Article (First 1000 chars)\", width=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Compare original vs edited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m---------------------------------- COMPARISON: Original vs Edited ----------------------------------\u001b[0m\n",
      "  Original length: 25,570 characters\n",
      "Edited length: 27,357 characters\n",
      "Difference: +1,787 characters\n",
      "\n",
      "Number of reviews addressed: 52\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "comparison_text = f\"\"\"Original length: {len(article.content):,} characters\n",
    "Edited length: {len(edited_article.content):,} characters\n",
    "Difference: {len(edited_article.content) - len(article.content):+,} characters\n",
    "\n",
    "Number of reviews addressed: {len(article_reviews.reviews)}\"\"\"\n",
    "\n",
    "pretty_print.wrapped(comparison_text, title=\"COMPARISON: Original vs Edited\", width=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Save the articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m------------------------------------------- First Draft -------------------------------------------\u001b[0m\n",
      "  âœ“ Saved first draft to: inputs/tests/02_sample_medium/article_draft.md\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m------------------------------------------ Edited Version ------------------------------------------\u001b[0m\n",
      "  âœ“ Saved edited version to: inputs/tests/02_sample_medium/article_edited.md\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from brown.renderers import MarkdownArticleRenderer\n",
    "\n",
    "renderer = MarkdownArticleRenderer()\n",
    "\n",
    "# Save first draft\n",
    "first_draft_path = SAMPLE_DIR / \"article_draft.md\"\n",
    "renderer.render(article, output_uri=first_draft_path)\n",
    "\n",
    "# Save edited version\n",
    "edited_path = SAMPLE_DIR / \"article_edited.md\"\n",
    "renderer.render(edited_article, output_uri=edited_path)\n",
    "\n",
    "pretty_print.wrapped(f\"âœ“ Saved first draft to: {first_draft_path}\", title=\"First Draft\", width=100)\n",
    "pretty_print.wrapped(f\"âœ“ Saved edited version to: {edited_path}\", title=\"Edited Version\", width=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open up the files and check out the difference! We recommend to use a diff tool such as the one from your IDE or search one on the internet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. App Configuration: Centralized Control\n",
    "\n",
    "Before gluing everything together into a LangGraph workflow, let's explore our centralized configuration system. This allows us to configure the entire application from a single YAML file.\n",
    "\n",
    "### Why Centralized Configuration?\n",
    "\n",
    "As our system grows more complex, we need:\n",
    "\n",
    "- **Single source of truth**: One file that controls everything\n",
    "- **Easy experimentation**: Change models, parameters without touching code\n",
    "- **Environment-specific configs**: Different settings for dev/prod\n",
    "- **Version control**: Track configuration changes over time\n",
    "\n",
    "The Brown agent uses a Pydantic-based configuration system that validates all settings at load time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The AppConfig Class Structure\n",
    "\n",
    "From `brown.config_app`, here's the configuration class hierarchy:\n",
    "\n",
    "**1. Context Configuration:**\n",
    "```python\n",
    "from pathlib import Path\n",
    "from pydantic import BaseModel, DirectoryPath, Field\n",
    "from typing import Literal, Annotated\n",
    "\n",
    "\n",
    "class Context(BaseModel):\n",
    "    # Article guideline\n",
    "    article_guideline_loader: Literal[\"markdown\"]\n",
    "    article_guideline_uri: Path\n",
    "\n",
    "    # Research\n",
    "    research_loader: Literal[\"markdown\"]\n",
    "    research_uri: Path\n",
    "\n",
    "    # Article\n",
    "    article_loader: Literal[\"markdown\"]\n",
    "    article_renderer: Literal[\"markdown\"]\n",
    "    article_uri: Path\n",
    "\n",
    "    # Profiles\n",
    "    profiles_loader: Literal[\"markdown\"]\n",
    "    profiles_uri: Annotated[DirectoryPath, Field(description=\"URI to profiles directory\")]\n",
    "    character_profile: str\n",
    "\n",
    "    # Examples\n",
    "    examples_loader: Literal[\"markdown\"]\n",
    "    examples_uri: Annotated[DirectoryPath, Field(description=\"URI to examples directory\")]\n",
    "\n",
    "    def build_article_uri(self, iteration: int) -> Path:\n",
    "        return self.article_uri.with_stem(f\"{self.article_uri.stem}_{iteration:03d}\")\n",
    "```\n",
    "\n",
    "This defines all the paths and loaders for different content types.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Node and Tool Configuration:**\n",
    "```python\n",
    "from brown.models.config import ModelConfig, SupportedModels\n",
    "\n",
    "\n",
    "class ToolConfig(BaseModel):\n",
    "    name: str\n",
    "    model_id: SupportedModels\n",
    "    config: ModelConfig\n",
    "\n",
    "\n",
    "class NodeConfig(BaseModel):\n",
    "    model_id: SupportedModels\n",
    "    config: ModelConfig\n",
    "    tools: dict[str, ToolConfig]\n",
    "```\n",
    "\n",
    "Each node can have its own model and configuration. Tools (like diagram generators) have their own configs too.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Memory Configuration:**\n",
    "```python\n",
    "class Memory(BaseModel):\n",
    "    checkpointer: Literal[\"in_memory\", \"sqlite\"]\n",
    "```\n",
    "\n",
    "Controls which checkpointing strategy to use for workflow state persistence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. The Main AppConfig Class:**\n",
    "```python\n",
    "from annotated_types import Ge\n",
    "\n",
    "\n",
    "class AppConfig(BaseModel):\n",
    "    context: Context\n",
    "    memory: Memory\n",
    "    \n",
    "    num_reviews: Annotated[int, Ge(1), Field(\n",
    "        description=\"The number of reviews to perform while generating the article\"\n",
    "    )]\n",
    "    nodes: dict[str, NodeConfig]\n",
    "\n",
    "    @classmethod\n",
    "    def from_yaml(cls, file_path: Path) -> \"AppConfig\":\n",
    "        \"\"\"Load configuration from a YAML file.\"\"\"\n",
    "        if not file_path.exists():\n",
    "            raise FileNotFoundError(f\"Configuration file not found: {file_path}\")\n",
    "        \n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = yaml.safe_load(f)\n",
    "        \n",
    "        return cls(**data)\n",
    "```\n",
    "\n",
    "**Key Features:**\n",
    "\n",
    "- `num_reviews`: How many review-edit iterations to run\n",
    "- `nodes`: Configuration for each workflow node\n",
    "- `from_yaml()`: Load configuration from YAML file\n",
    "- Full Pydantic validation ensures type safety\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Configuration File\n",
    "\n",
    "Let's look at an actual configuration file from `configs/course.yaml`:\n",
    "\n",
    "```yaml\n",
    "context:\n",
    "  article_guideline_loader: \"markdown\"\n",
    "  article_guideline_uri: \"article_guideline.md\"\n",
    "  research_loader: \"markdown\"\n",
    "  research_uri: \"research.md\"\n",
    "  article_loader: \"markdown\"\n",
    "  article_renderer: \"markdown\"\n",
    "  article_uri: \"article.md\"\n",
    "  profiles_loader: \"markdown\"\n",
    "  profiles_uri: \"inputs/profiles\"\n",
    "  character_profile: \"paul_iusztin.md\"\n",
    "  examples_loader: \"markdown\"\n",
    "  examples_uri: \"inputs/examples/course_lessons\"\n",
    "\n",
    "memory:\n",
    "  checkpointer: \"in_memory\"\n",
    "\n",
    "num_reviews: 2\n",
    "\n",
    "nodes:\n",
    "  generate_media_items:\n",
    "    model_id: \"google_genai:gemini-2.5-flash\"\n",
    "    model_config:\n",
    "      temperature: 0.0\n",
    "      include_thoughts: false\n",
    "      thinking_budget: null\n",
    "    tools:\n",
    "      mermaid_diagram_generator:\n",
    "        model_id: \"google_genai:gemini-2.5-flash\"\n",
    "        config:\n",
    "          temperature: 0.0\n",
    "          include_thoughts: false\n",
    "\n",
    "  write_article:\n",
    "    model_id: \"google_genai:gemini-2.5-pro\"\n",
    "    model_config:\n",
    "      temperature: 0.7\n",
    "      include_thoughts: false\n",
    "\n",
    "  review_article:\n",
    "    model_id: \"google_genai:gemini-2.5-pro\"\n",
    "    model_config:\n",
    "      temperature: 0.0\n",
    "      include_thoughts: false\n",
    "\n",
    "  edit_article:\n",
    "    model_id: \"google_genai:gemini-2.5-pro\"\n",
    "    model_config:\n",
    "      temperature: 0.1\n",
    "      include_thoughts: false\n",
    "```\n",
    "\n",
    "**Configuration Highlights:**\n",
    "\n",
    "- **Media generation**: Uses fast Flash model with 0 temperature (deterministic)\n",
    "- **Article writing**: Uses Pro model with higher temperature (0.7) for creativity\n",
    "- **Reviewing**: Uses Pro model with 0 temperature (strict adherence to rules)\n",
    "- **Editing**: Uses Pro model with low temperature (0.1) for focused changes\n",
    "- **2 review iterations**: Runs the review-edit loop twice\n",
    "\n",
    "This fine-grained control lets you optimize for quality, speed, and cost.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Using the Configuration\n",
    "\n",
    "Let's load a configuration file and examine it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m------------------------------------------ Configuration ------------------------------------------\u001b[0m\n",
      "  \n",
      "Configuration loaded successfully!\n",
      "Num review iterations: 2\n",
      "Memory checkpointer: in_memory\n",
      "Character profile: paul_iusztin.md\n",
      "\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m----------------------------------------- Configured nodes -----------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"generate_media_items\": {\n",
      "    \"Model\": \"google_genai:gemini-2.5-flash\",\n",
      "    \"Temperature\": 0.0,\n",
      "    \"Tools\": [\n",
      "      \"mermaid_diagram_generator\"\n",
      "    ]\n",
      "  },\n",
      "  \"write_article\": {\n",
      "    \"Model\": \"google_genai:gemini-2.5-flash\",\n",
      "    \"Temperature\": 0.7\n",
      "  },\n",
      "  \"review_article\": {\n",
      "    \"Model\": \"google_genai:gemini-2.5-flash\",\n",
      "    \"Temperature\": 0.0\n",
      "  },\n",
      "  \"edit_article\": {\n",
      "    \"Model\": \"google_genai:gemini-2.5-flash\",\n",
      "    \"Temperature\": 0.1\n",
      "  },\n",
      "  \"review_selected_text\": {\n",
      "    \"Model\": \"google_genai:gemini-2.5-flash\",\n",
      "    \"Temperature\": 0.0\n",
      "  },\n",
      "  \"edit_selected_text\": {\n",
      "    \"Model\": \"google_genai:gemini-2.5-flash\",\n",
      "    \"Temperature\": 0.1\n",
      "  }\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from brown.config_app import AppConfig\n",
    "\n",
    "# Load configuration from YAML\n",
    "config_path = CONFIGS_DIR / \"course.yaml\"\n",
    "app_config = AppConfig.from_yaml(config_path)\n",
    "\n",
    "text = f\"\"\"\n",
    "Configuration loaded successfully!\n",
    "Num review iterations: {app_config.num_reviews}\n",
    "Memory checkpointer: {app_config.memory.checkpointer}\n",
    "Character profile: {app_config.context.character_profile}\n",
    "\"\"\"\n",
    "pretty_print.wrapped(text, title=\"Configuration\", width=100)\n",
    "\n",
    "node_info = {}\n",
    "for node_name, node_config in app_config.nodes.items():\n",
    "    node_dict = {\n",
    "        \"Model\": node_config.model_id,\n",
    "        \"Temperature\": node_config.config.temperature,\n",
    "    }\n",
    "    if node_config.tools:\n",
    "        node_dict[\"Tools\"] = list(node_config.tools.keys())\n",
    "    node_info[node_name] = node_dict\n",
    "pretty_print.wrapped(node_info, title=\"Configured nodes\", width=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benefits of This Approach\n",
    "\n",
    "1. **Experimentation**: Change models/temperatures without editing code\n",
    "2. **Cost optimization**: Use cheaper models for simple tasks\n",
    "3. **Quality tuning**: Adjust temperatures per task\n",
    "4. **Environment flexibility**: Different configs for dev/staging/prod\n",
    "5. **Reproducibility**: Version control your configurations\n",
    "6. **Global overview**: You can see the whole setup in a glance\n",
    "\n",
    "This configuration-driven approach makes the system highly flexible and easy to iterate on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. LangGraph Workflow Integration\n",
    "\n",
    "Now let's explore how everything is glued together into a robust LangGraph workflow. The complete workflow is in `brown.workflows.generate_article`.\n",
    "\n",
    "### Workflow Architecture\n",
    "\n",
    "The workflow uses LangGraph's Function API to orchestrate the complete article generation process. Let's break down the key components.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Workflow\n",
    "\n",
    "**1. The Build Function:**\n",
    "```python\n",
    "from langgraph.func import entrypoint\n",
    "from langgraph.checkpoint.base import BaseCheckpointSaver\n",
    "\n",
    "\n",
    "def build_generate_article_workflow(checkpointer: BaseCheckpointSaver):\n",
    "    \"\"\"Create a generate article workflow with optional checkpointer.\n",
    "    \n",
    "    Args:\n",
    "        checkpointer: Checkpointer to use for workflow state persistence\n",
    "        \n",
    "    Returns:\n",
    "        Configured workflow entrypoint\n",
    "    \"\"\"\n",
    "    return entrypoint(checkpointer=checkpointer)(_generate_article_workflow)\n",
    "```\n",
    "\n",
    "**Why This Pattern?**\n",
    "\n",
    "We could directly decorate `_generate_article_workflow` with `@entrypoint`, but this builder function allows us to:\n",
    "\n",
    "1. **Inject dependencies at runtime**: Pass the checkpointer when building the workflow\n",
    "2. **Follow clean architecture**: Separate infrastructure (checkpointer) from business logic\n",
    "3. **Enable testing**: Easily swap checkpointers for different environments\n",
    "\n",
    "This pattern makes sense when you see how it's called in the next section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. The Workflow Input:**\n",
    "\n",
    "```python\n",
    "from typing import TypedDict\n",
    "\n",
    "\n",
    "class GenerateArticleInput(TypedDict):\n",
    "    dir_path: Path\n",
    "```\n",
    "\n",
    "Simple typed input containing just the directory path where all resources are located.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. The Main Workflow Function:**\n",
    "\n",
    "\n",
    "```python\n",
    "from langgraph.config import get_stream_writer, RunnableConfig\n",
    "\n",
    "\n",
    "async def _generate_article_workflow(inputs: GenerateArticleInput, config: RunnableConfig) -> str:\n",
    "    dir_path = inputs[\"dir_path\"]\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    writer = get_stream_writer()\n",
    "    \n",
    "    # Step 1: Load context\n",
    "    writer(WorkflowProgress(progress=0, message=\"Loading context\").model_dump(mode=\"json\"))\n",
    "    context = {}\n",
    "    loaders = build_loaders(app_config)\n",
    "    for context_name in [\"article_guideline\", \"research\", \"profiles\", \"examples\"]:\n",
    "        loader = cast(Loader, loaders[context_name])\n",
    "        context[context_name] = loader.load(working_uri=dir_path)\n",
    "    writer(WorkflowProgress(progress=2, message=\"Loaded context\").model_dump(mode=\"json\"))\n",
    "    \n",
    "    # Step 2: Generate media items\n",
    "    writer(WorkflowProgress(progress=3, message=\"Generating media items\").model_dump(mode=\"json\"))\n",
    "    media_items = await generate_media_items(context[\"article_guideline\"], context[\"research\"])\n",
    "    writer(WorkflowProgress(progress=10, message=\"Generated media items\").model_dump(mode=\"json\"))\n",
    "    \n",
    "    # Step 3: Write article\n",
    "    writer(WorkflowProgress(progress=15, message=\"Writing article\").model_dump(mode=\"json\"))\n",
    "    article = await write_article(...) \n",
    "    writer(WorkflowProgress(progress=20, message=\"Written raw article\").model_dump(mode=\"json\"))\n",
    "    \n",
    "    # Save iteration 0\n",
    "    article_path = dir_path / app_config.context.build_article_uri(0)\n",
    "    article_renderer = build_article_renderer(app_config)\n",
    "    article_renderer.render(article, output_uri=article_path)\n",
    "    \n",
    "    # Steps 4-5: Review and edit loop\n",
    "    for i in range(1, app_config.num_reviews + 1):\n",
    "        # Review\n",
    "        reviews = await generate_reviews(article, context[\"article_guideline\"], context[\"profiles\"])\n",
    "        \n",
    "        # Edit\n",
    "        article = await edit_based_on_reviews(...)\n",
    "        \n",
    "        # Save iteration i\n",
    "        article_path = dir_path / app_config.context.build_article_uri(i)\n",
    "        article_renderer.render(article, output_uri=article_path)\n",
    "    \n",
    "    # Save final article\n",
    "    article_path = dir_path / app_config.context.article_uri\n",
    "    article_renderer.render(article, output_uri=article_path)\n",
    "    \n",
    "    return f\"Final article rendered to `{article_path}`.\"\n",
    "```\n",
    "\n",
    "**Key Features:**\n",
    "\n",
    "1. **Progress reporting**: Uses `get_stream_writer()` to send progress updates\n",
    "2. **Iterative refinement**: Loops through review-edit cycles\n",
    "3. **Version saving**: Saves each iteration for comparison\n",
    "4. **Configuration-driven**: Uses `app_config` for all settings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. The Task Functions with Retry Policies:**\n",
    "\n",
    "Each major step is wrapped in a `@task` decorator with retry policies:\n",
    "\n",
    "\n",
    "```python\n",
    "from langgraph.func import task\n",
    "from langgraph.types import RetryPolicy\n",
    "\n",
    "retry_policy = RetryPolicy(max_attempts=3, retry_on=Exception)\n",
    "\n",
    "\n",
    "@task(retry_policy=retry_policy)\n",
    "async def generate_media_items(article_guideline: ArticleGuideline, research: Research) -> MediaItems:\n",
    "    writer = get_stream_writer()\n",
    "    \n",
    "    model, toolkit = build_model(app_config, node=\"generate_media_items\")\n",
    "    media_generator_orchestrator = MediaGeneratorOrchestrator(...)\n",
    "    media_items_to_generate_jobs = await media_generator_orchestrator.ainvoke()\n",
    "    \n",
    "    # Generate media items in parallel\n",
    "    coroutines = [tool.ainvoke(job[\"args\"]) for job in media_items_to_generate_jobs]\n",
    "    media_items = await asyncio.gather(*coroutines)\n",
    "    \n",
    "    return MediaItems.build(media_items)\n",
    "\n",
    "\n",
    "@task(retry_policy=retry_policy)\n",
    "async def write_article(...) -> Article:\n",
    "    model, _ = build_model(app_config, node=\"write_article\")\n",
    "    article_writer = ArticleWriter(...)\n",
    "    article = await article_writer.ainvoke()\n",
    "    return cast(Article, article)\n",
    "\n",
    "\n",
    "@task(retry_policy=retry_policy)\n",
    "async def generate_reviews(...) -> ArticleReviews:\n",
    "    model, _ = build_model(app_config, node=\"review_article\")\n",
    "    article_reviewer = ArticleReviewer(...)\n",
    "    reviews = await article_reviewer.ainvoke()\n",
    "    return cast(ArticleReviews, reviews)\n",
    "\n",
    "\n",
    "@task(retry_policy=retry_policy)\n",
    "async def edit_based_on_reviews(...) -> Article:\n",
    "    model, _ = build_model(app_config, node=\"edit_article\")\n",
    "    article_writer = ArticleWriter(..., reviews=reviews)\n",
    "    article = await article_writer.ainvoke()\n",
    "    return cast(Article, article)\n",
    "```\n",
    "\n",
    "**Why Retry Policies?**\n",
    "\n",
    "- **Resilience**: API failures, rate limits, network issues happen\n",
    "- **Automatic recovery**: Retry failed steps without manual intervention\n",
    "- **Task-level granularity**: Only retry the failed step, not the entire workflow\n",
    "- **Production-ready**: Makes the system robust for real-world use\n",
    "\n",
    "Having each step as a separate task with single responsibility makes retry policies extremely effective.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Short-Term Memory: Checkpointing\n",
    "\n",
    "Before running the complete workflow, let's understand the checkpointing system that provides workflow state persistence.\n",
    "\n",
    "### Why Checkpointing?\n",
    "\n",
    "Checkpointing serves several purposes:\n",
    "\n",
    "1. **Resume from failure**: If a workflow crashes, resume from the last checkpoint. For example, resume from the last `generate_reviews` step that crashed.\n",
    "2. **State inspection**: Examine workflow state at any point\n",
    "3. **Debugging**: Step through workflow execution\n",
    "4. **Human-in-the-loop**: Pause for human input, then resume\n",
    "\n",
    "For our use case, checkpointing enables the review-edit iterations to maintain state between steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### InMemory Checkpointer\n",
    "\n",
    "From `brown.memory`, we have a simple in-memory checkpointer factory:\n",
    "\n",
    "\n",
    "```python\n",
    "from contextlib import asynccontextmanager\n",
    "from typing import AsyncIterator\n",
    "\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "\n",
    "@asynccontextmanager\n",
    "async def build_in_memory_checkpointer() -> AsyncIterator[InMemorySaver]:\n",
    "    \"\"\"Build an in-memory checkpointer.\n",
    "    \n",
    "    Returns an async context manager that yields an InMemorySaver.\n",
    "    \n",
    "    Yields:\n",
    "        InMemorySaver instance\n",
    "    \"\"\"\n",
    "    yield InMemorySaver()\n",
    "```\n",
    "\n",
    "**Characteristics:**\n",
    "\n",
    "- **Ephemeral**: State lost when process ends\n",
    "- **Development-friendly**: Perfect for testing and iteration\n",
    "- **No persistence**: Not suitable for production long-running workflows\n",
    "\n",
    "**When to use**: Development, testing, short-lived workflows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Complete LangGraph Example\n",
    "\n",
    "Now let's run the complete workflow with LangGraph integration! This brings together everything we've learned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "  COMPLETE WORKFLOW WITH LANGGRAPH\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "from brown.memory import build_in_memory_checkpointer\n",
    "from brown.workflows.generate_article import GenerateArticleInput, build_generate_article_workflow\n",
    "\n",
    "pretty_print.wrapped(\"COMPLETE WORKFLOW WITH LANGGRAPH\", width=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Building short-term memory...\n",
      "   âœ“ In-memory checkpointer created\n",
      "\n",
      "2. Building workflow...\n",
      "   âœ“ Workflow built with checkpointer\n",
      "\n",
      "3. Configuring workflow...\n",
      "   âœ“ Thread ID: b59a8bd1-6150-4dec-b225-5d1c39ae6e6d\n",
      "\n",
      "4. Running workflow...\n",
      "   This will take several minutes...\n",
      "\u001b[93m---------------------------------------------- Event ----------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"progress\": 0,\n",
      "  \"message\": \"Loading context\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m---------------------------------------------- Event ----------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"progress\": 2,\n",
      "  \"message\": \"Loaded context\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m---------------------------------------------- Event ----------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"progress\": 3,\n",
      "  \"message\": \"Genererating media items\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m---------------------------------------------- Event ----------------------------------------------\u001b[0m\n",
      "  Found 8 media items to generate using the following tool configurations:\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m---------------------------------------------- Event ----------------------------------------------\u001b[0m\n",
      "    â€¢ Tool 1: mermaid_diagram_generator_tool - A flowchart illustrating the AI generation and human verification loop. The loop starts with 'AI Generation' (e.g., content, code, response), which then flows to 'Human Verification & Review'. From there, the human can either 'Accept Output' (ending the loop for that iteration) or provide 'Feedback/Refinement Request', which flows back to 'AI Generation' for further iteration. This highlights the continuous feedback and improvement cycle with human oversight.\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m---------------------------------------------- Event ----------------------------------------------\u001b[0m\n",
      "    â€¢ Tool 2: mermaid_diagram_generator_tool - A flowchart illustrating the 'Chaining and Routing' pattern for LLM workflows. The diagram should start with 'Input', which leads to a 'Router (LLM/Logic)'. The router then conditionally directs the flow to different 'LLM Call/Operation' nodes (e.g., 'LLM Call A', 'Database Query', 'LLM Call B'). After these operations, the paths converge to a 'Synthesizer/Aggregator' before leading to 'Output'. The diagram should emphasize the predefined, conditional flow.\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m---------------------------------------------- Event ----------------------------------------------\u001b[0m\n",
      "    â€¢ Tool 3: mermaid_diagram_generator_tool - A flowchart illustrating the 'Orchestrator-Worker' pattern for LLM workflows. The diagram should show 'User Intent' as the starting point, leading to an 'Orchestrator LLM'. The Orchestrator LLM performs 'Dynamic Task Planning' and then delegates tasks to multiple 'Worker LLMs/Tools' (e.g., 'Worker A (Sub-task 1)', 'Worker B (Sub-task 2)', 'Tool C (Action)'). The results from these workers are then sent back to the Orchestrator LLM for 'Result Synthesis', leading to a 'Final Answer'. The diagram should highlight the dynamic delegation and synthesis.\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m---------------------------------------------- Event ----------------------------------------------\u001b[0m\n",
      "    â€¢ Tool 4: mermaid_diagram_generator_tool - A flowchart illustrating the 'Evaluator-Optimizer Loop' pattern for LLM workflows. The loop begins with an 'LLM Generator' producing an 'Initial Output'. This output is then sent to an 'LLM Evaluator/Reviewer'. The Evaluator performs 'Output Analysis' and generates 'Feedback/Reflection (Error Report)'. This feedback is then sent back to the 'LLM Generator' for 'Output Refinement', closing the loop. The process continues iteratively until the output meets predefined criteria.\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m---------------------------------------------- Event ----------------------------------------------\u001b[0m\n",
      "    â€¢ Tool 5: mermaid_diagram_generator_tool - A flowchart illustrating the core components and dynamics of a ReAct (Reason and Act) AI agent. The central component is the 'Agent (LLM)'. It receives 'Input/Goal'. The LLM then enters a loop: 'Reason' (decide what to do) -> 'Act' (call a 'Tool' in the 'External Environment') -> 'Observe' (receive 'Tool Output') -> 'Reason' (interpret output, decide next step). The Agent also interacts with 'Short-term Memory' (working context) and 'Long-term Memory' (knowledge base, user preferences). The loop continues until the 'Goal Achieved/Response Generated'.\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m---------------------------------------------- Event ----------------------------------------------\u001b[0m\n",
      "    â€¢ Tool 6: mermaid_diagram_generator_tool - A flowchart illustrating a simple LLM workflow for document summarization and analysis. The workflow starts with 'User Input (Document)'. This leads to 'Read Document', followed by 'Summarize Document (LLM Call 1)'. The output of this step goes to 'Extract Key Points (LLM Call 2)'. The extracted key points are then sent to 'Save Results to Database', and finally, 'Show Results to User'. The diagram should clearly show a linear, predefined sequence of operations.\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m---------------------------------------------- Event ----------------------------------------------\u001b[0m\n",
      "    â€¢ Tool 7: mermaid_diagram_generator_tool - A flowchart illustrating the operational loop of the Gemini CLI coding assistant, based on the ReAct agent architecture. The loop starts with 'User Request'. This leads to 'Context Gathering' (load directory, tools, history). Then, 'LLM Reasoning' (analyze input, plan actions). Next, 'Human in the Loop' (validate execution plan). If approved, 'Tool Execution' (e.g., file operations, web requests, code generation). After execution, 'Evaluation' (run/compile code). Finally, 'Loop Decision' (task completed or repeat from 'LLM Reasoning'). The diagram should show the iterative nature and decision points.\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m---------------------------------------------- Event ----------------------------------------------\u001b[0m\n",
      "    â€¢ Tool 8: mermaid_diagram_generator_tool - A flowchart illustrating the iterative multi-step process of Perplexity's Deep Research agent, a hybrid system. The process begins with 'User Research Question'. An 'Orchestrator (Workflow)' performs 'Research Planning & Decomposition' into 'Sub-Questions'. These sub-questions are then processed in parallel by 'Specialized Search Agents' (Agents). Each agent performs 'Information Gathering' (using 'Web Search Tool', 'Document Retrieval Tool') and 'Analysis & Synthesis' of sources. The results are sent back to the 'Orchestrator', which performs 'Iterative Refinement & Gap Analysis'. If gaps exist, the process loops back to 'Research Planning & Decomposition'. Once complete, the 'Orchestrator' performs 'Report Generation' leading to 'Final Research Report'. The diagram should highlight parallel agent execution and iterative refinement.\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m---------------------------------------------- Event ----------------------------------------------\u001b[0m\n",
      "  Executing 8 media item generation jobs in parallel.\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m---------------------------------------------- Event ----------------------------------------------\u001b[0m\n",
      "  Generated 8 media items.\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m---------------------------------------------- Event ----------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"progress\": 10,\n",
      "  \"message\": \"Generated media items\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m---------------------------------------------- Event ----------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"progress\": 15,\n",
      "  \"message\": \"Writing article\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m---------------------------------------------- Event ----------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"progress\": 20,\n",
      "  \"message\": \"Written raw article\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m---------------------------------------------- Event ----------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"progress\": 25,\n",
      "  \"message\": \"Rendered raw article to `inputs/tests/02_sample_medium/article_000.md`\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m---------------------------------------------- Event ----------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"progress\": 37,\n",
      "  \"message\": \"Rewiewing article [Iteration 1 / 2]\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m---------------------------------------------- Event ----------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"progress\": 37,\n",
      "  \"message\": \"Generated reviews\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m---------------------------------------------- Event ----------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"progress\": 50,\n",
      "  \"message\": \"Editing article\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m---------------------------------------------- Event ----------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"progress\": 50,\n",
      "  \"message\": \"Edited article\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m---------------------------------------------- Event ----------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"progress\": 62,\n",
      "  \"message\": \"Rendered article to `inputs/tests/02_sample_medium/article_001.md`\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m---------------------------------------------- Event ----------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"progress\": 75,\n",
      "  \"message\": \"Rewiewing article [Iteration 2 / 2]\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m---------------------------------------------- Event ----------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"progress\": 75,\n",
      "  \"message\": \"Generated reviews\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m---------------------------------------------- Event ----------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"progress\": 87,\n",
      "  \"message\": \"Editing article\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m---------------------------------------------- Event ----------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"progress\": 87,\n",
      "  \"message\": \"Edited article\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m---------------------------------------------- Event ----------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"progress\": 99,\n",
      "  \"message\": \"Rendered article to `inputs/tests/02_sample_medium/article_002.md`\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m---------------------------------------------- Event ----------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"progress\": 100,\n",
      "  \"message\": \"Final article rendered to `inputs/tests/02_sample_medium/article.md`\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m---------------------------------------------- Output ----------------------------------------------\u001b[0m\n",
      "  Final article rendered to`inputs/tests/02_sample_medium/article.md`.\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "  WORKFLOW COMPLETE - Check SAMPLE_DIR for generated articles\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "async with build_in_memory_checkpointer() as checkpointer:\n",
    "\n",
    "    print(\"1. Building workflow...\")\n",
    "    workflow = build_generate_article_workflow(checkpointer=checkpointer)\n",
    "\n",
    "    print(\"2. Configuring workflow...\")\n",
    "    thread_id = str(uuid.uuid4())\n",
    "    config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "    print(f\"   âœ“ Thread ID: {thread_id}\")\n",
    "\n",
    "    print(\"3. Running workflow...\")\n",
    "    print(\"   This will take several minutes...\")\n",
    "\n",
    "    inputs = GenerateArticleInput(dir_path=SAMPLE_DIR)\n",
    "\n",
    "    async for event in workflow.astream(inputs, config=config, stream_mode=[\"custom\", \"values\"]):\n",
    "        event_type, event_data = event\n",
    "        if event_type == \"custom\":\n",
    "            pretty_print.wrapped(event_data, title=\"Event\")\n",
    "        elif event_type == \"values\":\n",
    "            pretty_print.wrapped(event_data, title=\"Output\")\n",
    "\n",
    "pretty_print.wrapped(f\"WORKFLOW COMPLETE - Check `{SAMPLE_DIR}` for generated articles\", width=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [!WARNING]\n",
    "> If the workflows fail due to API limits or timeouts, that's totally normal. Just retry the workflow. If after retrying it still fails, wait for a bit and retry again. Unfortunately, these are the downsides of working with APIs rather than your own self-hosted open-source LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the generated articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated article files:\n",
      "  - article.md: 29,344 bytes\n",
      "  - article_000.md: 30,194 bytes\n",
      "  - article_001.md: 11,495 bytes\n",
      "  - article_002.md: 29,344 bytes\n",
      "  - article_draft.md: 25,578 bytes\n",
      "  - article_edited.md: 27,365 bytes\n",
      "  - article_guideline.md: 22,872 bytes\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nGenerated article files:\")\n",
    "article_files = sorted(SAMPLE_DIR.glob(\"article*.md\"))\n",
    "for article_file in article_files:\n",
    "    size = article_file.stat().st_size\n",
    "    print(f\"  - {article_file.name}: {size:,} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened?\n",
    "\n",
    "Let's break down the workflow execution:\n",
    "\n",
    "**Infrastructure Setup:**\n",
    "1. Created an in-memory checkpointer for state persistence\n",
    "2. Built the workflow by injecting the checkpointer\n",
    "3. Generated a unique thread ID for this workflow run\n",
    "\n",
    "**Workflow Execution:**\n",
    "1. **Loaded Context** (Progress: 0-2%): All guidelines, research, profiles, examples\n",
    "2. **Generated Media** (Progress: 3-10%): Orchestrator identified and delegated media generation\n",
    "3. **Wrote Article** (Progress: 15-20%): ArticleWriter created first draft\n",
    "4. **Review-Edit Loop** (Progress: 25-99%): For each iteration:\n",
    "   - ArticleReviewer analyzed the article\n",
    "   - ArticleWriter edited based on reviews\n",
    "   - Saved intermediate version\n",
    "5. **Final Save** (Progress: 100%): Saved the final refined article\n",
    "\n",
    "**Key LangGraph Features Used:**\n",
    "- **Checkpointing**: State persistence between steps\n",
    "- **Streaming**: Real-time progress updates\n",
    "- **Retry policies**: Automatic recovery from failures\n",
    "- **Tasks**: Composable, retryable workflow steps\n",
    "- **Configuration**: Thread-based workflow isolation\n",
    "\n",
    "**Output Files:**\n",
    "\n",
    "Saved to the sample input directory:\n",
    "\n",
    "- `article_000.md`: Initial draft\n",
    "- `article_001.md`: After first review-edit iteration\n",
    "- `article_002.md`: After second review-edit iteration (if num_reviews=2)\n",
    "- `article.md`: Final refined article\n",
    "\n",
    "This demonstrates a production-ready implementation of the evaluator-optimizer pattern!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the saved files at:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('inputs/tests/02_sample_medium')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAMPLE_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listing them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('inputs/tests/02_sample_medium/article_draft.md'),\n",
       " PosixPath('inputs/tests/02_sample_medium/article_edited.md'),\n",
       " PosixPath('inputs/tests/02_sample_medium/article.md'),\n",
       " PosixPath('inputs/tests/02_sample_medium/article_001.md'),\n",
       " PosixPath('inputs/tests/02_sample_medium/article_000.md'),\n",
       " PosixPath('inputs/tests/02_sample_medium/article_002.md'),\n",
       " PosixPath('inputs/tests/02_sample_medium/article_guideline.md')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(SAMPLE_DIR.glob(\"article*.md\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recommend opening up each checkpoint and seeing how the article evolved after each iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Conclusion and Future Steps\n",
    "\n",
    "Congratulations! You've learned how to implement the evaluator-optimizer pattern and integrate it into a production-ready workflow.\n",
    "\n",
    "### What We've Learned\n",
    "\n",
    "**1. The Evaluator-Optimizer Pattern:**\n",
    "- Evaluator (ArticleReviewer) identifies issues against requirements\n",
    "- Optimizer (ArticleWriter) fixes issues based on feedback\n",
    "- Iterative refinement gradually improves quality\n",
    "- Transparent, debuggable, and improvable process\n",
    "\n",
    "**2. Centralized Configuration:**\n",
    "- Single YAML file controls everything\n",
    "- Per-node model and parameter configuration\n",
    "- Easy experimentation and optimization\n",
    "- Type-safe with Pydantic validation\n",
    "\n",
    "**3. LangGraph Integration:**\n",
    "- Workflow orchestration with Function API\n",
    "- Task-based architecture with retry policies\n",
    "- Checkpointing for state persistence\n",
    "- Streaming for progress reporting\n",
    "- Production-ready error handling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideas for Extension\n",
    "\n",
    "Now that you understand the system, here are ways to extend it:\n",
    "\n",
    "**1. Different AI Frameworks:**\n",
    "\n",
    "Replace LangGraph Function API with LangGraph Graph API. Try PydanticAI or other frameworks. The clean architecture makes swapping easy.\n",
    "\n",
    "**2. Improve the Reviewer:**\n",
    "\n",
    "Play around with the reviewer, get a feeling of how it extract the reviews from the profiles and tweak either the profiles or the reviewer for better results.\n",
    "\n",
    "**3. Modify the configuration:**\n",
    "\n",
    "Change models, temperatures, num_reviews from the `configs/course.yaml` file and see how the output of the article changes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's Next in the Course\n",
    "\n",
    "In **Lesson 24: Human-in-the-Loop**, we'll explore:\n",
    "\n",
    "- Adding two new workflows for iteratively editing the whole article or just a selected piece of text.\n",
    "- Properly add humans in the loop between the generated article and future edit iterations.\n",
    "- Expose the workflows as MCP tools.\n",
    "\n",
    "You'll see why having a fixed number of review iterations (rather than scoring until \"good enough\") makes perfect sense when humans are in the loop.\n",
    "\n",
    "### Resources\n",
    "\n",
    "- **Brown Package**: Explore `lessons/writing_workflow`\n",
    "- **Configuration Examples**: Check `configs/` for different configurations\n",
    "- **Test Data**: Use `inputs/tests/` for additional testing scenarios\n",
    "- **Writing Profiles**: Check `inputs/profiles/` for more profile templates"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
