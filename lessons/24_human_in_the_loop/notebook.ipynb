{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 24: Human-in-the-Loop for Brown Writing Workflow\n",
    "\n",
    "In this lesson, we'll explore how to implement human-in-the-loop capabilities in the Brown writing workflow. We'll learn how to integrate human feedback into the article review and editing process, expose the workflows as MCP tools for seamless integration with AI assistants like Claude and Cursor, and create a collaborative writing experience where AI and human expertise combine.\n",
    "\n",
    "**Learning Objectives:**\n",
    "\n",
    "- Understand the importance of human feedback and human-in-the-loop in AI writing workflows\n",
    "- Learn how to implement the HumanFeedback entity and integrate it into the ArticleReviewer node\n",
    "- Explore two new editing workflows: edit article and edit selected text\n",
    "- Discover how to expose Brown as an MCP server with tools, prompts, and resources\n",
    "- See how to integrate Brown with MCP clients like Cursor for a coding-like writing experience\n",
    "\n",
    "> [!NOTE]\n",
    "> ðŸ’¡ Remember that you can also run `brown` as a standalone Python package by going to `lessons/writing_workflow/` and following the instructions from there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "First, we define some standard Magic Python commands to autoreload Python packages whenever they change:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up Python Environment\n",
    "\n",
    "To set up your Python virtual environment using `uv` and load it into the Notebook, follow the step-by-step instructions from the `Course Admin` lesson from the beginning of the course.\n",
    "\n",
    "**TL/DR:** Be sure the correct kernel pointing to your `uv` virtual environment is selected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Gemini API\n",
    "\n",
    "To run this lesson, you'll need several API keys configured:\n",
    "\n",
    "1. **Gemini API Key**, `GOOGLE_API_KEY` variable: Get your key from [Google AI Studio](https://aistudio.google.com/app/apikey)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables loaded from `/Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.env`\n",
      "Environment variables loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "from utils import env\n",
    "\n",
    "env.load(required_env_vars=[\"GOOGLE_API_KEY\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Key Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "from utils import pretty_print\n",
    "\n",
    "nest_asyncio.apply()  # Allow nested async usage in notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Required Files\n",
    "\n",
    "We need to download the configuration files and input data that Brown uses for article generation and editing.\n",
    "\n",
    "First, let's download the configs folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!rm -rf configs\n",
    "!curl -L -o configs.zip https://raw.githubusercontent.com/iusztinpaul/agentic-ai-engineering-course-data/main/data/configs.zip\n",
    "!unzip configs.zip\n",
    "!rm -rf configs.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's download the inputs folder containing profiles, examples, and test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!rm -rf inputs\n",
    "!curl -L -o inputs.zip https://raw.githubusercontent.com/iusztinpaul/agentic-ai-engineering-course-data/main/data/inputs.zip\n",
    "!unzip inputs.zip\n",
    "!rm -rf inputs.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify what we downloaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article_guideline.md   \u001b[1m\u001b[36minputs\u001b[m\u001b[m/                notebook_guideline.md\n",
      "\u001b[1m\u001b[36mconfigs\u001b[m\u001b[m/               notebook.ipynb\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define constants to reference these directories throughout the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configs directory exists: True\n",
      "Inputs directory exists: True\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "CONFIGS_DIR = Path(\"configs\")\n",
    "INPUTS_DIR = Path(\"inputs\")\n",
    "\n",
    "print(f\"Configs directory exists: {CONFIGS_DIR.exists()}\")\n",
    "print(f\"Inputs directory exists: {INPUTS_DIR.exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples directory exists: True\n",
      "Profiles directory exists: True\n"
     ]
    }
   ],
   "source": [
    "EXAMPLES_DIR = Path(\"inputs/examples/course_lessons\")\n",
    "PROFILES_DIR = Path(\"inputs/profiles\")\n",
    "\n",
    "print(f\"Examples directory exists: {EXAMPLES_DIR.exists()}\")\n",
    "print(f\"Profiles directory exists: {PROFILES_DIR.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will load a simpler example that runs faster and is easier to understand. At the end, we will load a larger sample that is closer to what we do on our end to generate professional articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples directory exists: True\n"
     ]
    }
   ],
   "source": [
    "SAMPLE_DIR = Path(\"inputs/tests/01_sample_small\")\n",
    "\n",
    "print(f\"Samples directory exists: {SAMPLE_DIR.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Importance of Human-in-the-Loop\n",
    "\n",
    "After generating an article using the writing workflow we explained in Lessons 22 and 23, you'll likely want to refine it further. Writing is highly subjective, and even the best AI-generated content benefits from human review and editing.\n",
    "\n",
    "The perfect balance between AI and human expertise is to use AI to generate and automate parts of your work, then have you, as the domain expert, review and refine it. Known as the AI generation - human validation loop. \n",
    "\n",
    "This is exactly what we've designed the Brown writing workflow to support.\n",
    "\n",
    "### The Human-in-the-Loop Design\n",
    "\n",
    "We designed Brown to easily introduce humans into the loop between generating the first version of an article and refining it through additional review and editing cycles with human feedback. This means:\n",
    "\n",
    "1. We can use a low number of review loops during initial article generation to reduce costs and latency\n",
    "2. After reviewing the generated article, we can dynamically run additional review and editing workflows with human feedback\n",
    "3. We can edit either the entire article or just selected sections based on your needs\n",
    "\n",
    "### Decoupling Workflows with MCP\n",
    "\n",
    "To enable this human-in-the-loop approach, we needed to decouple the article generation workflow from the editing workflows. We used MCP servers to achieve this separation, where:\n",
    "\n",
    "- The `generate_article` workflow is one independent MCP tool\n",
    "- The `edit_article` workflow is another independent MCP tool\n",
    "- The `edit_selected_text` workflow is a third independent MCP tool\n",
    "\n",
    "This architecture allows you to generate an article, review it, and then selectively apply additional editing workflows with your human feedback until you're satisfied with the results.\n",
    "\n",
    "Here's how the workflow looks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/iusztinpaul/agentic-ai-engineering-course-data/main/images/l24_writing_workflow.png\" alt=\"Workflow\" height=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The diagram shows how Brown as an MCP server exposes three main tools, with a human feedback loop that allows iterative refinement until you're satisfied with the article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Introducing Human Feedback into the Article Reviewer\n",
    "\n",
    "Let's see how we introduced human feedback into our Article Reviewer Node. We'll start by explaining the `HumanFeedback` entity, then show how it's integrated into the `ArticleReviewer` node, and finally demonstrate it with a working example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 The HumanFeedback Entity\n",
    "\n",
    "The `HumanFeedback` entity is a simple but Pydantic model that encapsulates human feedback for the article review process.\n",
    "\n",
    "Source: `brown.entities.reviews`\n",
    "```python\n",
    "class HumanFeedback(BaseModel, ContextMixin):\n",
    "    content: str\n",
    "\n",
    "    def to_context(self) -> str:\n",
    "        return f\"\"\"\n",
    "<{self.xml_tag}>\n",
    "    {self.content}\n",
    "</{self.xml_tag}>\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Human Feedback in ArticleReviewer\n",
    "\n",
    "Now let's see how the `ArticleReviewer` node integrates human feedback into the review process. We'll focus only on the relevant sections.\n",
    "\n",
    "Source: `brown.nodes.article_reviewer`\n",
    "\n",
    "1. **Initialization with Human Feedback**\n",
    "```python\n",
    "def __init__(\n",
    "    self,\n",
    "    to_review: Article | SelectedText,\n",
    "    article_guideline: ArticleGuideline,\n",
    "    model: Runnable,\n",
    "    article_profiles: ArticleProfiles,\n",
    "    human_feedback: HumanFeedback | None = None,\n",
    ") -> None:\n",
    "    self.to_review = to_review\n",
    "    self.article_guideline = article_guideline\n",
    "    self.article_profiles = article_profiles\n",
    "    self.human_feedback = human_feedback\n",
    "\n",
    "    super().__init__(model, toolkit=Toolkit(tools=[]))\n",
    "```\n",
    "\n",
    "The `ArticleReviewer` now accepts an optional `human_feedback` parameter. This allows the reviewer to work with or without human input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Human Feedback in the System Prompt**\n",
    "\n",
    "The system prompt includes a dedicated section for human feedback:\n",
    "```python\n",
    "system_prompt_template = \"\"\"\n",
    "You are Brown, an expert article writer, editor and reviewer specialized in reviewing technical, educative and informational articles.\n",
    "\n",
    "...\n",
    "\n",
    "## Human Feedback\n",
    "\n",
    "Along with the expected requirements, a human already reviewed the article and provided the following feedback:\n",
    "\n",
    "{human_feedback}\n",
    "\n",
    "If empty, completely ignore it, otherwise the feedback will ALWAYS be used in two ways:\n",
    "1. First you will use the <human_feedback> to guide your reviewing process against the requirements. This will help you understand \n",
    "on what rules to focus on as this directly highlights what the user wants to improve.\n",
    "2. Secondly you will extract one or more action points based on the <human_feedback>. Depending on how many ideas, topics or suggestions \n",
    "the <human_feedback> contains you will generate from 1 to N action points. Each <human_feedback> review will contain a single action point. \n",
    "3. As long the <human_feedback> is not empty, you will always return at least 1 action point, but you will return more action points \n",
    "if the feedback touches multiple ideas. \n",
    "\n",
    "Here is an example of a reviewed based on the human feedback:\n",
    "<example_of_human_feedback_action_point>\n",
    "Review(\n",
    "    profile=\"human_feedback\",\n",
    "    location=\"Article level\",\n",
    "    comment=\"Add all the points from the article guideline to the article.\"\n",
    ")\n",
    "</example_of_human_feedback_action_point>\n",
    "\n",
    "...\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "This section instructs the LLM on how to use human feedback:\n",
    "- Use it to guide the review process and focus on specific rules\n",
    "- Extract action points from the feedback (1 to N depending on how many ideas are present)\n",
    "- Always return at least 1 action point if feedback is provided\n",
    "- Each action point becomes a review with `profile=\"human_feedback\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Injecting Human Feedback into the Prompt**\n",
    "\n",
    "When the reviewer runs, it injects the human feedback into the system prompt:\n",
    "\n",
    "```python\n",
    "async def ainvoke(self) -> ArticleReviews | SelectedTextReviews:\n",
    "    system_prompt = self.system_prompt_template.format(\n",
    "        human_feedback=self.human_feedback.to_context() if self.human_feedback else \"\",\n",
    "        article=self.article.to_context(),\n",
    "        article_guideline=self.article_guideline.to_context(),\n",
    "        character_template=self.article_profiles.character.to_context(),\n",
    "        article_template=self.article_profiles.article.to_context(),\n",
    "        structure_template=self.article_profiles.structure.to_context(),\n",
    "        mechanics_template=self.article_profiles.mechanics.to_context(),\n",
    "        terminology_template=self.article_profiles.terminology.to_context(),\n",
    "        tonality_template=self.article_profiles.tonality.to_context(),\n",
    "    )\n",
    "    ...\n",
    "```\n",
    "\n",
    "If `human_feedback` is provided, it's converted to XML context format and injected. Otherwise, an empty string is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Example: Using ArticleReviewer with Human Feedback\n",
    "\n",
    "Let's see a practical example of using the `ArticleReviewer` with human feedback. We'll load our sample article, article guideline, and profiles, then provide human feedback to guide the review process.\n",
    "\n",
    "First, let's import the necessary components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-11-26 16:51:14.779\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mbrown.config\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m10\u001b[0m - \u001b[1mLoading environment file from `.env`\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from brown.entities.reviews import HumanFeedback\n",
    "from brown.loaders import (\n",
    "    MarkdownArticleExampleLoader,\n",
    "    MarkdownArticleGuidelineLoader,\n",
    "    MarkdownArticleLoader,\n",
    "    MarkdownArticleProfilesLoader,\n",
    ")\n",
    "from brown.models import SupportedModels, get_model\n",
    "from brown.nodes.article_reviewer import ArticleReviewer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load the sample inputs. We'll use the same article and guidelines from the test sample directory as we used in previous lessons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "  STEP 1: Loading Context\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "âœ“ Guideline: 6,751 characters\n",
      "âœ“ Article: 20,638 characters\n",
      "âœ“ Profiles: 6 profiles loaded\n",
      "âœ“ Examples: 2 article examples\n"
     ]
    }
   ],
   "source": [
    "pretty_print.wrapped(\"STEP 1: Loading Context\", width=100)\n",
    "\n",
    "# Load guideline\n",
    "guideline_loader = MarkdownArticleGuidelineLoader(uri=Path(\"article_guideline.md\"))\n",
    "article_guideline = guideline_loader.load(working_uri=SAMPLE_DIR)\n",
    "\n",
    "# Load profiles\n",
    "profiles_input = {\n",
    "    \"article\": PROFILES_DIR / \"article_profile.md\",\n",
    "    \"character\": PROFILES_DIR / \"character_profiles\" / \"paul_iusztin.md\",\n",
    "    \"mechanics\": PROFILES_DIR / \"mechanics_profile.md\",\n",
    "    \"structure\": PROFILES_DIR / \"structure_profile.md\",\n",
    "    \"terminology\": PROFILES_DIR / \"terminology_profile.md\",\n",
    "    \"tonality\": PROFILES_DIR / \"tonality_profile.md\",\n",
    "}\n",
    "profiles_loader = MarkdownArticleProfilesLoader(uri=profiles_input)\n",
    "profiles = profiles_loader.load()\n",
    "\n",
    "# Load examples\n",
    "examples_loader = MarkdownArticleExampleLoader(uri=EXAMPLES_DIR)\n",
    "article_examples = examples_loader.load()\n",
    "\n",
    "article_loader = MarkdownArticleLoader(uri=\"article.md\")\n",
    "article = article_loader.load(working_uri=SAMPLE_DIR)\n",
    "\n",
    "print(f\"âœ“ Guideline: {len(article_guideline.content):,} characters\")\n",
    "print(f\"âœ“ Article: {len(article.content):,} characters\")\n",
    "print(f\"âœ“ Profiles: {len(profiles_input)} profiles loaded\")\n",
    "print(f\"âœ“ Examples: {len(article_examples.examples)} article examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a reminder on how the article looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m--------------------------------- Article (first 4000 characters) ---------------------------------\u001b[0m\n",
      "  \n",
      "<article>\n",
      "    # AI Agents vs. LLM Workflows: The Critical Decision Every AI Engineer Faces\n",
      "### A pragmatic guide to choosing the right architecture for your AI application.\n",
      "\n",
      "When building AI applications, engineers face a critical architectural decision early in their development process. Should you create a predictable, step-by-step workflow where you control every action, or should you build an autonomous agent that can think and decide for itself? This is one of the key decisions that will impact everything from development time and costs to reliability and user experience.\n",
      "\n",
      "Choose the wrong approach, and you might end up with an overly rigid system that breaks when users deviate from expected patterns. Or you could build an unpredictable agent that works brilliantly 80% of the time but fails catastrophically when it matters most. Either path can lead to months of wasted development time, frustrated users, and executives questioning the skyrocketing operational costs.\n",
      "\n",
      "In the real world of 2024 and 2025, billion-dollar AI startups succeed or fail based primarily on this architectural decision. The successful companies, teams, and AI engineers know when to use workflows versus agents and, more importantly, how to combine both approaches effectively.\n",
      "\n",
      "This lesson will provide a framework to help you make this critical decision with confidence. We will systematically explore the spectrum from rigid workflows to autonomous agents, helping you understand the trade-offs. By the end, you will have the knowledge to architect AI systems that are not only powerful but also robust, efficient, and safe.\n",
      "\n",
      "## Understanding the Spectrum: From Workflows to Agents\n",
      "\n",
      "Before we can choose between workflows and agents, we need a clear understanding of what they are. Rather than focusing on the technical specifics, let's look at their core properties and how they function in practice.\n",
      "\n",
      "An LLM workflow is a sequence of tasks that involves LLM calls or other operations, such as reading from a database or writing to a file system. It is largely predefined and orchestrated by developer-written code. The steps are defined in advance, resulting in deterministic or rule-based paths with predictable execution and explicit control flow. Think of it like a factory assembly line, where each station performs a specific, repeatable task in a set order.\n",
      "\n",
      "```mermaid\n",
      "graph TD\n",
      "    A[\"Input\"] --> B[\"tool_calling_llm\"]\n",
      "    B --> C[\"Tools\"]\n",
      "    C --> D[\"End\"]\n",
      "```\n",
      "Image 1: A simple LLM workflow illustrating a sequence of predefined tasks, where an input leads to a tool-calling LLM, which interacts with tools before the workflow ends.\n",
      "\n",
      "On the other end of the spectrum, we have AI agents. These are systems where an LLM plays a central role in dynamically deciding the sequence of steps, reasoning, and actions to achieve a goal. The steps are not defined in advance but are planned based on the task and the current state of the environment. This makes them adaptive and capable of handling novelty. You can think of an agent as a skilled human expert tackling an unfamiliar problem, adapting their approach with each new piece of information.\n",
      "\n",
      "```mermaid\n",
      "graph TD\n",
      "    A[\"Start\"] --> B[\"Agent: Initial Request/State\"]\n",
      "    B --> C{\"Assistant: Analyze & Plan\"}\n",
      "    C --> D{\"Decision: Tool Required?\"}\n",
      "    D -->|\"Yes\"| E[\"Use Tool\"]\n",
      "    E --> F[\"Agent: Process Tool Output\"]\n",
      "    F --> C\n",
      "    D -->|\"No\"| G{\"Decision: Task Completed?\"}\n",
      "    G -->|\"No\"| C\n",
      "    G -->|\"Yes\"| H[\"End\"]\n",
      "```\n",
      "Image 2: A simple agentic system illustrating dynamic decision-making and tool usage with an assistant orchestrating the loop.\n",
      "\n",
      "Both workflows and agents require an orchestration layer, but its role differs significantly. In a workflow, the orchestrator executes a predefined plan, like a conductor following a musical score. In an agentic system, the orchestrator facilitates the LLM's dynamic planning and execution, acting more like a jazz ensemble leader who guides improvisation. We will explore advanced wo\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "pretty_print.wrapped(f\"{article.to_context()[:4000]}\", title=\"Article (first 4000 characters)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create human feedback and run the article reviewer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running article review with human feedback...\n",
      "\n",
      "Generated 66 reviews\n"
     ]
    }
   ],
   "source": [
    "human_feedback = HumanFeedback(\n",
    "    content=\"\"\"Make the introduction more engaging and catchy. \n",
    "Also, expand on the definition of both workflows and agents from the first section\"\"\"\n",
    ")\n",
    "\n",
    "# Create the article reviewer\n",
    "model = get_model(SupportedModels.GOOGLE_GEMINI_25_FLASH)\n",
    "article_reviewer = ArticleReviewer(\n",
    "    to_review=article,\n",
    "    article_guideline=article_guideline,\n",
    "    model=model,\n",
    "    article_profiles=profiles,\n",
    "    human_feedback=human_feedback,\n",
    ")\n",
    "\n",
    "print(\"Running article review with human feedback...\")\n",
    "reviews = await article_reviewer.ainvoke()\n",
    "print(f\"\\nGenerated {len(reviews.reviews)} reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the reviews, especially focusing on the human feedback reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m-------------------------------------- Human Feedback Reviews --------------------------------------\u001b[0m\n",
      "  Found 2 reviews based on human feedback\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m------------------------------------- 1. Human Feedback Review -------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"profile\": \"human_feedback\",\n",
      "  \"location\": \"Introduction\",\n",
      "  \"comment\": \"Make the introduction more engaging, catchy, and shorter, as per human feedback. The current introduction is 185 words, exceeding the 100-word limit specified in the article guideline.\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m------------------------------------- 2. Human Feedback Review -------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"profile\": \"human_feedback\",\n",
      "  \"location\": \"Section 2 - Understanding the Spectrum: From Workflows to Agents\",\n",
      "  \"comment\": \"Expand on the definitions of both workflows and agents in this section to provide more detail, as requested in the human feedback. The current definitions are present but could be more elaborated, especially considering the overall article length and the importance of these foundational concepts.\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from utils import pretty_print\n",
    "\n",
    "# Print human feedback reviews\n",
    "human_feedback_reviews = [r for r in reviews.reviews if r.profile == \"human_feedback\"]\n",
    "pretty_print.wrapped(\n",
    "    f\"Found {len(human_feedback_reviews)} reviews based on human feedback\", title=\"Human Feedback Reviews\"\n",
    ")\n",
    "\n",
    "for i, review in enumerate(human_feedback_reviews, 1):\n",
    "    pretty_print.wrapped(\n",
    "        {\"profile\": review.profile, \"location\": review.location, \"comment\": review.comment},\n",
    "        title=f\"{i}. Human Feedback Review\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also see all the other reviews from the profiles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m--------------------------------------- All Reviews Summary ---------------------------------------\u001b[0m\n",
      "  Generated reviews from 7 different profile types: article_guideline, article_profile, human_feedback, mechanics_profile, structure_profile, terminology_profile, tonality_profile\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "  ARTICLE_GUIDELINE: 5 reviews\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "  1. [Introduction: The Critical Decision Every AI Engineer Faces] The introduction is 185 words, which exceeds the 'Section length: 100 words' requirement in the arti...\n",
      "  2. [Section 2 - Understanding the Spectrum: From Workflows to Agents] The section, without the mermaid diagram code, is 338 words. This exceeds the 'Section length: 200 w...\n",
      "\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "  ARTICLE_PROFILE: 33 reviews\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "  1. [Article level] The article lacks a 'References' section at the end, which is explicitly part of the `article_struct...\n",
      "  2. [Introduction: The Critical Decision Every AI Engineer Faces - First paragraph] The paragraph 'When building AI applications, engineers face a critical architectural decision early...\n",
      "\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "  HUMAN_FEEDBACK: 2 reviews\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "  1. [Introduction] Make the introduction more engaging, catchy, and shorter, as per human feedback. The current introdu...\n",
      "  2. [Section 2 - Understanding the Spectrum: From Workflows to Agents] Expand on the definitions of both workflows and agents in this section to provide more detail, as re...\n",
      "\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "  MECHANICS_PROFILE: 9 reviews\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "  1. [Understanding the Spectrum: From Workflows to Agents - First paragraph] The sentence 'Before we can choose between workflows and agents, we need a clear understanding of wh...\n",
      "  2. [Understanding the Spectrum: From Workflows to Agents - Fifth paragraph] The sentence 'We will explore advanced workflow patterns like chaining, routing, and the orchestrato...\n",
      "\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "  STRUCTURE_PROFILE: 2 reviews\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "  1. [Article level] The article uses H1 ('# AI Agents vs. LLM Workflows: The Critical Decision Every AI Engineer Faces')...\n",
      "  2. [Article level] The `structure_profile` states 'The introduction, conclusion and section titles use H2 headers, mark...\n",
      "\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "  TERMINOLOGY_PROFILE: 13 reviews\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "  1. [Introduction: The Critical Decision Every AI Engineer Faces - First paragraph] The sentence 'This is one of the key decisions that will impact everything from development time and...\n",
      "  2. [Introduction: The Critical Decision Every AI Engineer Faces - Third paragraph] The sentence 'In the real world of 2024 and 2025, billion-dollar AI startups succeed or fail based p...\n",
      "\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "  TONALITY_PROFILE: 2 reviews\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "  1. [Introduction: The Critical Decision Every AI Engineer Faces - Fourth paragraph] The sentence 'This lesson will provide a framework to help you make this critical decision with conf...\n",
      "  2. [Conclusion: The Challenges of Every AI Engineer - First paragraph] The sentence 'Now that you understand the spectrum from LLM workflows to AI agents, it is important ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "profile_types = set(r.profile for r in reviews.reviews)\n",
    "\n",
    "pretty_print.wrapped(\n",
    "    f\"Generated reviews from {len(profile_types)} different profile types: {', '.join(sorted(profile_types))}\",\n",
    "    title=\"All Reviews Summary\",\n",
    ")\n",
    "print()\n",
    "\n",
    "for profile_type in sorted(profile_types):\n",
    "    profile_reviews = [r for r in reviews.reviews if r.profile == profile_type]\n",
    "    pretty_print.wrapped(f\"{profile_type.upper()}: {len(profile_reviews)} reviews\")\n",
    "    for i, review in enumerate(profile_reviews[:2], 1):  # Show first 2 of each type\n",
    "        print(f\"  {i}. [{review.location}] {review.comment[:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the reviewer generated reviews from multiple sources:\n",
    "- Human feedback reviews that directly address your specific requests\n",
    "- Profile-based reviews (article, structure, mechanics, terminology, tonality) that ensure adherence to the style guidelines\n",
    "\n",
    "The human feedback reviews always have `profile=\"human_feedback\"` and create action points based on your feedback. These reviews will be used by the article writer to edit the article according to your instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The Edit Article Workflow\n",
    "\n",
    "Now that we understand how human feedback integrates with the article reviewer, let's explore the `edit_article` workflow. This workflow reviews and edits an existing article based on human feedback and the expected requirements.\n",
    "\n",
    "The edit article workflow contains only one loop of the same reviewing-editing logic we already use within the generate article workflow. \n",
    "\n",
    "Also, the edit article workflow follows the same clean architecture pattern we've used throughout Brown. It leverages the app layer to orchestrate nodes and entities, keeping the code modular and maintainable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Building the Edit Article Workflow\n",
    "\n",
    "The workflow is built using LangGraph's functional API. Here's how it's structured:\n",
    "\n",
    "Source: `brown.workflows.edit_article`\n",
    "```python\n",
    "def build_edit_article_workflow(checkpointer: BaseCheckpointSaver):\n",
    "    \"\"\"Create an edit article workflow with checkpointer.\n",
    "\n",
    "    Args:\n",
    "        checkpointer: Checkpointer to use for workflow persistence.\n",
    "\n",
    "    Returns:\n",
    "        Configured workflow entrypoint\n",
    "    \"\"\"\n",
    "\n",
    "    return entrypoint(checkpointer=checkpointer)(_edit_article_workflow)\n",
    "```\n",
    "\n",
    "The `build_edit_article_workflow` function is a factory that creates the workflow with a checkpointer for persistence. It uses LangGraph's `@entrypoint` decorator to wrap the main workflow function.\n",
    "\n",
    "The workflow expects an `EditArticleInput` typed dictionary:\n",
    "\n",
    "```python\n",
    "class EditArticleInput(TypedDict):\n",
    "    dir_path: Path\n",
    "    human_feedback: str\n",
    "```\n",
    "\n",
    "This input specifies:\n",
    "- `dir_path`: The directory containing the article and all supporting files (guideline, profiles, research, etc.)\n",
    "- `human_feedback`: The human feedback string to guide the editing process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 The Edit Article Workflow Logic\n",
    "\n",
    "The main workflow function orchestrates the entire editing process:\n",
    "```python\n",
    "async def _edit_article_workflow(inputs: EditArticleInput, config: RunnableConfig) -> str:\n",
    "    writer = get_stream_writer()\n",
    "\n",
    "    # Progress: Loading context\n",
    "    writer(WorkflowProgress(progress=0, message=\"Loading context\").model_dump(mode=\"json\"))\n",
    "    context = {}\n",
    "    loaders = build_loaders(app_config)\n",
    "    for context_name, loader in loaders.items():\n",
    "        loader = cast(Loader, loader)\n",
    "        context[context_name] = loader.load(working_uri=inputs[\"dir_path\"])\n",
    "\n",
    "    human_feedback = HumanFeedback(content=inputs[\"human_feedback\"])\n",
    "    writer(WorkflowProgress(progress=5, message=\"Loaded context\").model_dump(mode=\"json\"))\n",
    "\n",
    "    # Progress: Reviewing\n",
    "    writer(WorkflowProgress(progress=20, message=\"Reviewing article\").model_dump(mode=\"json\"))\n",
    "    reviews = await generate_reviews(context[\"article\"], human_feedback, context[\"article_guideline\"], context[\"profiles\"])\n",
    "    writer(WorkflowProgress(progress=40, message=\"Generated reviews\").model_dump(mode=\"json\"))\n",
    "\n",
    "    # Progress: Editing\n",
    "    writer(WorkflowProgress(progress=60, message=\"Editing article\").model_dump(mode=\"json\"))\n",
    "    article = await edit_based_on_reviews(\n",
    "        context[\"article_guideline\"], context[\"research\"], context[\"profiles\"], context[\"examples\"], reviews\n",
    "    )\n",
    "    writer(WorkflowProgress(progress=80, message=\"Edited article\").model_dump(mode=\"json\"))\n",
    "\n",
    "    # Progress: Complete\n",
    "    writer(WorkflowProgress(progress=100, message=\"Article editing completed\").model_dump(mode=\"json\"))\n",
    "\n",
    "    return f\"\"\"\n",
    "Here is the edited article:\n",
    "{article.to_context()}\n",
    "\n",
    "Here is what you have to do with the edited article:\n",
    "- print the edited article to the console for the user to see\n",
    "- give a quick summary of the changes you made\n",
    "- always apply the changes to the source file without waiting for the user to explicitly ask you to do so\n",
    "- in case you haven't changed anything, just say that you haven't changed anything\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "The workflow follows these steps:\n",
    "1. **Load context**: Use the loader builders to load the article, guideline, profiles, research, and examples from the directory\n",
    "2. **Create human feedback**: Convert the feedback string into a `HumanFeedback` entity\n",
    "3. **Generate reviews**: Run the article reviewer with human feedback to generate reviews\n",
    "4. **Edit based on reviews**: Run the article writer with the reviews to produce an edited article\n",
    "5. **Return instructions**: Return the edited article along with instructions for the MCP client on what to do next\n",
    "\n",
    "Notice how steps 3 and 4 are identical to the ones from the writing workflow you learned in lesson 23."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Generating Reviews\n",
    "\n",
    "The `generate_reviews` task creates reviews by running the `ArticleReviewer` node:\n",
    "```python\n",
    "@task(retry_policy=retry_policy)\n",
    "async def generate_reviews(\n",
    "    article: Article,\n",
    "    human_feedback: HumanFeedback,\n",
    "    article_guideline: ArticleGuideline,\n",
    "    article_profiles: ArticleProfiles,\n",
    ") -> ArticleReviews:\n",
    "    model, _ = build_model(app_config, node=\"review_article\")\n",
    "    article_reviewer = ArticleReviewer(\n",
    "        to_review=article,\n",
    "        article_guideline=article_guideline,\n",
    "        article_profiles=article_profiles,\n",
    "        human_feedback=human_feedback,\n",
    "        model=model,\n",
    "    )\n",
    "    reviews = await article_reviewer.ainvoke()\n",
    "\n",
    "    return cast(ArticleReviews, reviews)\n",
    "```\n",
    "\n",
    "This task:\n",
    "- Builds the model from the app config for the \"review_article\" node\n",
    "- Creates an `ArticleReviewer` with the article, guideline, profiles, and human feedback\n",
    "- Uses LangGraph's `@task` decorator with a retry policy for resilience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Editing Based on Reviews\n",
    "\n",
    "The `edit_based_on_reviews` task creates an edited article using the `ArticleWriter` node:\n",
    "```python\n",
    "@task(retry_policy=retry_policy)\n",
    "async def edit_based_on_reviews(\n",
    "    article_guideline: ArticleGuideline,\n",
    "    research: Research,\n",
    "    article_profiles: ArticleProfiles,\n",
    "    article_examples: ArticleExamples,\n",
    "    reviews: ArticleReviews,\n",
    ") -> Article:\n",
    "    model, _ = build_model(app_config, node=\"edit_article\")\n",
    "    article_writer = ArticleWriter(\n",
    "        article_guideline=article_guideline,\n",
    "        research=research,\n",
    "        article_profiles=article_profiles,\n",
    "        media_items=MediaItems.build(),\n",
    "        article_examples=article_examples,\n",
    "        reviews=reviews,\n",
    "        model=model,\n",
    "    )\n",
    "    article = await article_writer.ainvoke()\n",
    "\n",
    "    return cast(Article, article)\n",
    "```\n",
    "\n",
    "This task:\n",
    "- Builds the model from the app config for the \"edit_article\" node\n",
    "- Creates an `ArticleWriter` with all necessary context and the reviews to address\n",
    "- Also uses the `@task` decorator with retry policy\n",
    "\n",
    "Notice how the `ArticleWriter` works in \"editing mode\" when provided with `reviews`. It uses the same writer node from article generation, but the reviews guide it to make specific changes rather than writing from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Running the Edit Article Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Building workflow...\n",
      "\n",
      "2. Configuring workflow...\n",
      "\n",
      "   âœ“ Thread ID: 0af08c82-c5da-48f7-8746-9cbafb23abd2\n",
      "3. Running workflow...\n",
      "   This will take several minutes...\n",
      "\n",
      "\u001b[93m---------------------------------------------- Event ----------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"progress\": 0,\n",
      "  \"message\": \"Loading context\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m---------------------------------------------- Event ----------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"progress\": 5,\n",
      "  \"message\": \"Loaded context\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m---------------------------------------------- Event ----------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"progress\": 20,\n",
      "  \"message\": \"Reviewing article\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m---------------------------------------------- Event ----------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"progress\": 40,\n",
      "  \"message\": \"Generated reviews\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m---------------------------------------------- Event ----------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"progress\": 60,\n",
      "  \"message\": \"Editing article\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m---------------------------------------------- Event ----------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"progress\": 80,\n",
      "  \"message\": \"Edited article\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m---------------------------------------------- Event ----------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"progress\": 100,\n",
      "  \"message\": \"Article editing completed\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m---------------------------------------------- Output ----------------------------------------------\u001b[0m\n",
      "  \n",
      "Here is the edited article:\n",
      "\n",
      "<article>\n",
      "    # AI Agents vs. LLM Workflows: The Decision Every AI Engineer Faces\n",
      "### A pragmatic guide to choosing the right architecture for your AI application.\n",
      "\n",
      "When building AI applications, engineers face a key architectural decision. Should you create a predictable, step-by-step workflow where you control every action, or should you build an autonomous agent that can think and decide for itself? This choice impacts development time, costs, reliability, and user experience.\n",
      "\n",
      "Choosing the wrong approach can lead to an overly rigid system that breaks with unexpected user input. Or you might build an unpredictable agent that works well most of the time but fails when it matters most. Both paths can result in wasted development time and increased operational costs.\n",
      "\n",
      "This lesson provides a framework to help you make this decision. We will explore the spectrum from rigid workflows to autonomous agents, understanding their trade-offs. By the end, you will have the knowledge to architect AI systems that are powerful, efficient, and safe.\n",
      "\n",
      "## Understanding the Spectrum: From Workflows to Agents\n",
      "\n",
      "To choose between workflows and agents, you need a clear understanding of what they are. We will look at their core properties and how they function.\n",
      "\n",
      "An LLM workflow is a sequence of tasks involving LLM calls or other operations, like reading from a database. It is largely predefined and orchestrated by developer-written code. The steps are defined in advance, resulting in deterministic paths with predictable execution and explicit control flow. For example, a workflow might always classify an email, then extract entities, and finally log the data.\n",
      "\n",
      "```mermaid\n",
      "graph TD\n",
      "    A[\"Input\"] --> B[\"tool_calling_llm\"]\n",
      "    B --> C[\"Tools\"]\n",
      "    C --> D[\"End\"]\n",
      "```\n",
      "Image 1: A simple LLM workflow illustrating a sequence of predefined tasks, where an input leads to a tool-calling LLM, which interacts with tools before the workflow ends. (Source [Exploring the difference between agents and workflows](https://decodingml.substack.com/p/llmops-for-production-agentic-rag))\n",
      "\n",
      "AI agents are systems where an LLM plays a central role in dynamically deciding the sequence of steps, reasoning, and actions to achieve a goal. The steps are not defined in advance but are planned based on the task and the current state of the environment. This makes them adaptive and capable of handling new situations. An agent might decide to search the web, then summarize findings, and then ask a clarifying question, all based on its ongoing reasoning.\n",
      "\n",
      "```mermaid\n",
      "graph TD\n",
      "    A[\"Start\"] --> B[\"Agent: Initial Request/State\"]\n",
      "    B --> C{\"Assistant: Analyze & Plan\"}\n",
      "    C --> D{\"Decision: Tool Required?\"}\n",
      "    D -->|\"Yes\"| E[\"Use Tool\"]\n",
      "    E --> F[\"Agent: Process Tool Output\"]\n",
      "    F --> C\n",
      "    D -->|\"No\"| G{\"Decision: Task Completed?\"}\n",
      "    G -->|\"No\"| C\n",
      "    G -->|\"Yes\"| H[\"End\"]\n",
      "```\n",
      "Image 2: A simple agentic system illustrating dynamic decision-making and tool usage with an assistant orchestrating the loop. (Source [Exploring the difference between agents and workflows](https://decodingml.substack.com/p/llmops-for-production-agentic-rag))\n",
      "\n",
      "Both workflows and agents require an orchestration layer, but its role differs. In a workflow, the orchestrator executes a predefined plan. In an agentic system, the orchestrator facilitates the LLM's dynamic planning and execution. We will explore advanced workflow patterns like chaining and routing in future lessons, as well as the core components of agents like tools, memory, and the ReAct framework.\n",
      "\n",
      "## Choosing Your Path\n",
      "\n",
      "In the previous section, we defined LLM workflows and AI agents. Now, we will explore their core difference: developer-defined logic versus LLM-driven autonomy in reasoning and action selection. Most real-world systems are not purely one or the other; they exist on a spectrum. The key is to find the right balance for your specific use case.\n",
      "\n",
      "```mermaid\n",
      "graph LR\n",
      "    subgraph \"LLM Workflows vs. AI Agents Spectrum\"\n",
      "        W[\"LLM Workflows\"]\n",
      "        S[/\"Spectrum\"/]\n",
      "        A[\"Autonomous AI Agents\"]\n",
      "\n",
      "        W -- \"High Application Reliability\" --> S\n",
      "        W -- \"Low Agent's Level of control\" -- S\n",
      "        S -- \"Lower Application Reliability\" --> A\n",
      "        S -- \"Higher Agent's Level of control\" --> A\n",
      "    end\n",
      "```\n",
      "Image 3: A spectrum illustrating the trade-off between application reliability and agent control in LLM workflows versus autonomous AI agents. (Source [Exploring the difference between agents and workflows](https://decodingml.substack.com/p/llmops-for-production-agentic-rag))\n",
      "\n",
      "Workflows are best for tasks with clearly defined steps. Examples include pipelines for data extraction and transformation from sources like the web, Slack, Zoom, Notion, and Google Drive. They are also good for automated report or email generation and content repurposing. Workflows offer predictability, reliability, and easier debugging. They can also have lower operational costs by using smaller, specialized models. However, they can require more development time and offer a rigid user experience that struggles with unexpected scenarios.\n",
      "\n",
      "Agents excel at open-ended research, dynamic problem-solving like debugging code, and interactive tasks in new environments, such as booking a flight without specifying exact sites. Their strength is adaptability and flexibility in handling ambiguity. The downside is that this autonomy makes them less predictable and more prone to errors. Performance, latency, and costs can vary with each run. Agents often require more powerful LLMs, leading to higher costs. If not designed well, they can pose security risks, especially with write operations, potentially deleting data or sending inappropriate emails. Agents are also challenging to debug and evaluate.\n",
      "\n",
      "Most real-world systems blend elements of both. Think of it as an \"autonomy slider\" where you decide how much control to give the LLM versus the user. Andrej Karpathy showed this concept in his talks on software in the AI era [[1]](https://www.youtube.com/watch?v=y5mdI_aBC4Y). For example, a code editor like Cursor offers different levels of autonomy: simple tab completion (low autonomy), refactoring a selected block of code (medium autonomy), or letting the AI modify the entire repository (high autonomy).\n",
      "\n",
      "The goal is to accelerate the loop between AI generation and human verification. This is often achieved through a well-designed architecture that combines the reliability of workflows with the flexibility of agents, supported by an intuitive user interface.\n",
      "\n",
      "```mermaid\n",
      "graph TD\n",
      "    A[\"AI Generates Content\"]\n",
      "    B[\"Human Review & Verification\"]\n",
      "    A --> B\n",
      "    B -->|Human Feedback & Refinement| A\n",
      "```\n",
      "Image 4: A flowchart illustrating the cyclical AI generation and human verification loop. (Source [A Developerâ€™s Guide to Building Scalable AI: Workflows vs Agents](https://towardsdatascience.com/a-developers-guide-to-building-scalable-ai-workflows-vs-agents/))\n",
      "\n",
      "## Conclusion: The Challenges of Every AI Engineer\n",
      "\n",
      "Now that you understand the spectrum from LLM workflows to AI agents, it is important to recognize that this architectural choice determines whether your AI application succeeds or fails in production.\n",
      "\n",
      "As an AI engineer, you will face common issues:\n",
      "- **Data Integration:** Building pipelines to pull information from Slack, web APIs, SQL databases, and data lakes. You must ensure only high-quality data reaches your AI system, following the garbage-in, garbage-out principle.\n",
      "- **Cost-Performance Trap:** Sophisticated agents deliver impressive results but can cost a lot per user interaction, making them economically unfeasible for many applications.\n",
      "- **Security Concerns:** Autonomous agents with powerful write permissions could send wrong emails, delete important files, or expose sensitive data.\n",
      "\n",
      "These challenges are solvable. In Lesson 3, we will explore the foundations of AI agents and workflows: context engineering. We will cover patterns for building reliable products through specialized evaluation and monitoring pipelines. We will also explore strategies for building hybrid systems and techniques for keeping costs and latency under control.\n",
      "\n",
      "## References\n",
      "\n",
      "1. Karpathy, A. (2024, May 24). Software in the Era of AI. *YouTube*. https://www.youtube.com/watch?v=y5mdI_aBC4Y\n",
      "2. Anthropic. (2024, June 26). Building effective agents. *Anthropic*. https://www.anthropic.com/engineering/building-effective-agents\n",
      "3. Google. (n.d.). Gemini CLI. *GitHub*. https://github.com/google-gemini/gemini-cli/blob/main/README.md\n",
      "4. Iusztin, P. (n.d.). Exploring the difference between agents and workflows. *Decoding AI Magazine*. https://decodingml.substack.com/p/llmops-for-production-agentic-rag\n",
      "5. Iusztin, P. (n.d.). A Developerâ€™s Guide to Building Scalable AI: Workflows vs Agents. *Towards Data Science*. https://towardsdatascience.com/a-developers-guide-to-building-scalable-ai-workflows-vs-agents/\n",
      "6. Iusztin, P. (n.d.). Real Agents vs. Workflows: The Truth Behind AI 'Agents'. *YouTube*. https://www.youtube.com/watch?v=kQxr-uOxw2o&t=1s\n",
      "</article>\n",
      "\n",
      "\n",
      "Here is what you have to do with the edited article:\n",
      "- print the edited article to the console for the user to see\n",
      "- give a quick summary of the changes you made\n",
      "- always apply the changes to the source file without waiting for the user to explicitly ask you to do so\n",
      "- in case you haven't changed anything, just say that you haven't changed anything\n",
      "\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "  WORKFLOW COMPLETE - Check `inputs/tests/01_sample_small` for generated articles\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "from brown.memory import build_in_memory_checkpointer\n",
    "from brown.workflows.edit_article import build_edit_article_workflow\n",
    "\n",
    "async with build_in_memory_checkpointer() as checkpointer:\n",
    "    print(\"1. Building workflow...\\n\")\n",
    "    workflow = build_edit_article_workflow(checkpointer=checkpointer)\n",
    "\n",
    "    print(\"2. Configuring workflow...\\n\")\n",
    "    thread_id = str(uuid.uuid4())\n",
    "    config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "    print(f\"   âœ“ Thread ID: {thread_id}\")\n",
    "\n",
    "    print(\"3. Running workflow...\")\n",
    "    print(\"   This will take several minutes...\\n\")\n",
    "\n",
    "    async for event in workflow.astream(\n",
    "        {\n",
    "            \"dir_path\": SAMPLE_DIR,\n",
    "            \"human_feedback\": \"\"\"\n",
    "Make the introduction more engaging, catchy and shorter. \n",
    "Also, expand on the definition of both workflows and agents from the first section\"\"\",\n",
    "        },\n",
    "        config=config,\n",
    "        stream_mode=[\"custom\", \"values\"],\n",
    "    ):\n",
    "        event_type, event_data = event\n",
    "        if event_type == \"custom\":\n",
    "            pretty_print.wrapped(event_data, title=\"Event\")\n",
    "        elif event_type == \"values\":\n",
    "            pretty_print.wrapped(event_data, title=\"Output\")\n",
    "\n",
    "pretty_print.wrapped(\"WORKFLOW COMPLETED\", width=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the output of the workflow is the edited article. This will make more sense when we plug it into the MCP Server in a few sections from now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 The Power of Human Feedback\n",
    "\n",
    "The edit article workflow demonstrates a key advantage of our architecture:\n",
    "\n",
    "**We can use a low number of review loops during initial article generation, and further run them dynamically with a human in the loop when necessary, with more human guidance.**\n",
    "\n",
    "This means:\n",
    "- Initial generation is faster and cheaper with fewer automatic review iterations\n",
    "- We don't assume how many iterations we need to have an ideal output, but let you decide\n",
    "- The workflow runs additional review and editing cycles guided by your feedback\n",
    "- You can repeat this process until satisfied with the results\n",
    "\n",
    "This approach balances efficiency with quality, using AI to handle the heavy lifting while keeping you in control of the final output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. The Edit Selected Text Workflow\n",
    "\n",
    "While the edit article workflow handles entire article edits, you'll often want to refine just a specific section. The `edit_selected_text` workflow enables precise, focused edits on selected text portions.\n",
    "\n",
    "The workflow structure is almost identical to `edit_article`, thanks to our clean architecture. The main difference is that it operates on a `SelectedText` entity instead of the full `Article`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Building the Edit Selected Text Workflow\n",
    "\n",
    "The workflow builder follows the same pattern:\n",
    "\n",
    "Source: `brown.workflows.edit_selected_text`\n",
    "```python\n",
    "def build_edit_selected_text_workflow(checkpointer: BaseCheckpointSaver):\n",
    "    \"\"\"Create an edit selected text workflow with checkpointer.\n",
    "\n",
    "    Args:\n",
    "        checkpointer: Checkpointer to use for workflow persistence.\n",
    "\n",
    "    Returns:\n",
    "        Configured workflow entrypoint\n",
    "    \"\"\"\n",
    "\n",
    "    return entrypoint(checkpointer=checkpointer)(_edit_selected_text_workflow)\n",
    "```\n",
    "\n",
    "The workflow expects an `EditSelectedTextInput` typed dictionary:\n",
    "\n",
    "```python\n",
    "class EditSelectedTextInput(TypedDict):\n",
    "    dir_path: Path\n",
    "    human_feedback: str\n",
    "    selected_text: str\n",
    "    number_line_before_selected_text: int\n",
    "    number_line_after_selected_text: int\n",
    "```\n",
    "\n",
    "This input specifies:\n",
    "- `dir_path`: The directory containing the article and supporting files\n",
    "- `human_feedback`: Human feedback to guide the editing\n",
    "- `selected_text`: The specific text portion to edit\n",
    "- `number_line_before_selected_text`: The starting line number in the article\n",
    "- `number_line_after_selected_text`: The ending line number in the article\n",
    "\n",
    "The line numbers help the workflow locate the selected text within the larger article context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 The Edit Selected Text Workflow Logic\n",
    "\n",
    "The main workflow function is structurally similar to `edit_article`:\n",
    "```python\n",
    "async def _edit_selected_text_workflow(inputs: EditSelectedTextInput, config: RunnableConfig) -> str:\n",
    "    writer = get_stream_writer()\n",
    "\n",
    "    # Progress: Loading context\n",
    "    writer(WorkflowProgress(progress=0, message=\"Loading context\").model_dump(mode=\"json\"))\n",
    "    context = {}\n",
    "    loaders = build_loaders(app_config)\n",
    "    for context_name, loader in loaders.items():\n",
    "        loader = cast(Loader, loader)\n",
    "        context[context_name] = loader.load(working_uri=inputs[\"dir_path\"])\n",
    "\n",
    "    selected_text = SelectedText(\n",
    "        article=context[\"article\"],\n",
    "        content=inputs[\"selected_text\"],\n",
    "        first_line_number=inputs[\"number_line_before_selected_text\"],\n",
    "        last_line_number=inputs[\"number_line_after_selected_text\"],\n",
    "    )\n",
    "    human_feedback = HumanFeedback(content=inputs[\"human_feedback\"])\n",
    "    writer(WorkflowProgress(progress=5, message=\"Loaded context\").model_dump(mode=\"json\"))\n",
    "\n",
    "    # Progress: Reviewing\n",
    "    writer(WorkflowProgress(progress=20, message=\"Reviewing selected text\").model_dump(mode=\"json\"))\n",
    "    reviews = await generate_reviews(selected_text, human_feedback, context[\"article_guideline\"], context[\"profiles\"])\n",
    "    writer(WorkflowProgress(progress=40, message=\"Generated reviews\").model_dump(mode=\"json\"))\n",
    "\n",
    "    # Progress: Editing\n",
    "    writer(WorkflowProgress(progress=60, message=\"Editing selected text\").model_dump(mode=\"json\"))\n",
    "    selected_text = await edit_based_on_reviews(\n",
    "        context[\"article_guideline\"], context[\"research\"], context[\"profiles\"], context[\"examples\"], reviews\n",
    "    )\n",
    "    writer(WorkflowProgress(progress=80, message=\"Edited selected text\").model_dump(mode=\"json\"))\n",
    "\n",
    "    # Progress: Complete\n",
    "    writer(WorkflowProgress(progress=100, message=\"Selected text editing completed\").model_dump(mode=\"json\"))\n",
    "\n",
    "    return f\"\"\"\n",
    "Here is the edited selected text:\n",
    "{selected_text.to_context()}\n",
    "\n",
    "Here is what you have to do with edited selected text:\n",
    "- print the edited selected text to the console for the user to see\n",
    "- give a quick summary of the changes you made\n",
    "- always apply the changes to the source file without waiting for the user to explicitly ask you to do so\n",
    "- in case you haven't changed anything, just say that you haven't changed anything\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "The workflow follows these steps:\n",
    "1. **Load context**: Load the full article and supporting files\n",
    "2. **Create selected text entity**: Build a `SelectedText` entity that contains the selected portion, the full article for context, and line numbers\n",
    "3. **Create human feedback**: Convert the feedback string to a `HumanFeedback` entity\n",
    "4. **Generate reviews**: Review the selected text with human feedback\n",
    "5. **Edit based on reviews**: Edit the selected text based on the reviews\n",
    "6. **Return instructions**: Return the edited selected text with instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Generating Reviews for Selected Text\n",
    "\n",
    "The `generate_reviews` task for selected text is nearly identical to the article version:\n",
    "```python\n",
    "@task(retry_policy=retry_policy)\n",
    "async def generate_reviews(\n",
    "    selected_text: SelectedText,\n",
    "    human_feedback: HumanFeedback,\n",
    "    article_guideline: ArticleGuideline,\n",
    "    article_profiles: ArticleProfiles,\n",
    ") -> SelectedTextReviews:\n",
    "    model, _ = build_model(app_config, node=\"review_selected_text\")\n",
    "    selected_text_reviewer = ArticleReviewer(\n",
    "        to_review=selected_text,\n",
    "        human_feedback=human_feedback,\n",
    "        article_guideline=article_guideline,\n",
    "        article_profiles=article_profiles,\n",
    "        model=model,\n",
    "    )\n",
    "    reviews = await selected_text_reviewer.ainvoke()\n",
    "\n",
    "    return cast(SelectedTextReviews, reviews)\n",
    "```\n",
    "\n",
    "The key difference is:\n",
    "- It takes a `SelectedText` instead of `Article`\n",
    "- It returns `SelectedTextReviews` instead of `ArticleReviews`\n",
    "- It uses the \"review_selected_text\" node config\n",
    "\n",
    "The `ArticleReviewer` node is smart enough to handle both cases. When given a `SelectedText`, it focuses reviews on that portion while using the full article as context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Editing Selected Text Based on Reviews\n",
    "\n",
    "The `edit_based_on_reviews` task for selected text also follows the same pattern:\n",
    "```python\n",
    "@task(retry_policy=retry_policy)\n",
    "async def edit_based_on_reviews(\n",
    "    article_guideline: ArticleGuideline,\n",
    "    research: Research,\n",
    "    article_profiles: ArticleProfiles,\n",
    "    article_examples: ArticleExamples,\n",
    "    reviews: SelectedTextReviews,\n",
    ") -> SelectedText:\n",
    "    model, _ = build_model(app_config, node=\"edit_selected_text\")\n",
    "    article_writer = ArticleWriter(\n",
    "        article_guideline=article_guideline,\n",
    "        research=research,\n",
    "        article_profiles=article_profiles,\n",
    "        media_items=MediaItems.build(),\n",
    "        article_examples=article_examples,\n",
    "        reviews=reviews,\n",
    "        model=model,\n",
    "    )\n",
    "    edited_selected_text = cast(SelectedText, await article_writer.ainvoke())\n",
    "\n",
    "    return edited_selected_text\n",
    "```\n",
    "\n",
    "This task:\n",
    "- Takes `SelectedTextReviews` instead of `ArticleReviews`\n",
    "- Returns `SelectedText` instead of `Article`\n",
    "- Uses the \"edit_selected_text\" node config\n",
    "\n",
    "Again, the `ArticleWriter` node handles both article and selected text editing seamlessly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Why Edit Selected Text?\n",
    "\n",
    "The edit selected text workflow is crucial because:\n",
    "\n",
    "**Most often we don't want to edit the whole article, but just a small section, or apply the human feedback just to a small section.**\n",
    "\n",
    "This workflow enables:\n",
    "- Faster and cheaper edits by focusing on specific sections\n",
    "- More precise changes without affecting other parts of the article\n",
    "- Iterative refinement of individual paragraphs or sections\n",
    "- Better control over the editing process\n",
    "\n",
    "Combined with the edit article workflow, you have complete flexibility to refine content at any granularity you need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Running the Edit Selected Text Workflow\n",
    "\n",
    "First, explictly load the selected text that we want to edit from the sample article:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m-------------------------------------- Selected text to edit --------------------------------------\u001b[0m\n",
      "  \n",
      "To make the right choice, you first need to understand what LLM workflows and AI agents are. We will look at their core properties and how they are used, rather than their technical specifics.\n",
      "\n",
      "### LLM Workflows\n",
      "\n",
      "An LLM workflow is a sequence of tasks orchestrated by developer-written code. It can include LLM calls, but also other operations like reading from a database or calling an API. Think of it like a recipe where each step is explicitly defined. The key characteristic is that the path is determined in advance, resulting in a deterministic or rule-based system. This gives you predictable execution, explicit control over the application's flow, and makes the system easier to test and debug. Because you control every step, you know exactly where a failure occurred and how to fix it.\n",
      "\n",
      "```mermaid\n",
      "graph TD\n",
      "    A[\"Start\"] --> B[\"LLM Call\"]\n",
      "    B --> C[\"Process Data\"]\n",
      "    C --> D[\"Store Data\"]\n",
      "    D --> E[\"End\"]\n",
      "```\n",
      "Image 1: A flowchart illustrating a deterministic LLM workflow with clear start and end points, including an LLM call and data operations.\n",
      "\n",
      "### AI Agents\n",
      "\n",
      "AI agents are systems where an LLM dynamically decides the sequence of steps, reasoning, and actions to achieve a goal. The path is not predefined. Instead, the agent uses a reasoning process to plan its actions based on the task and the current state of its environment. This process is often modeled on frameworks like ReAct (Reason, Act, Observe). This allows agents to be adaptive and capable of handling new or unexpected situations through LLM-driven autonomy. They can select tools, execute actions, evaluate the outcomes, and correct their course until the goal is achieved [[1]](https://www.youtube.com/watch?v=kQxr-uOxw2o&t=1s).\n",
      "\n",
      "```mermaid\n",
      "graph TD\n",
      "    A[\"Start\"] --> B[\"Agent (LLM) Receives Goal\"]\n",
      "    B --> C[\"Plan/Reason (LLM)\"]\n",
      "    C --> D[\"Select Tool\"]\n",
      "    D --> E[\"Execute Action (Tool Call)\"]\n",
      "    E --> F[\"Observe Environment/Feedback\"]\n",
      "    F --> G{\"Evaluate Outcome\"}\n",
      "    G -->|\"Satisfactory\"| H[\"Stop/Achieve Goal\"]\n",
      "    G -->|\"Needs Adjustment\"| C\n",
      "```\n",
      "Image 2: Flowchart illustrating an AI agent's dynamic decision-making process driven by an LLM.\n",
      "\n",
      "## Choosing Your Path\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "article = MarkdownArticleLoader(uri=\"article.md\").load(working_uri=SAMPLE_DIR)\n",
    "\n",
    "start_line = 8\n",
    "end_line = 42\n",
    "selected_text = \"\\n\".join(article.content.split(\"\\n\")[start_line:end_line])\n",
    "pretty_print.wrapped(selected_text, title=\"Selected text to edit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, call the workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Building workflow...\n",
      "\n",
      "2. Configuring workflow...\n",
      "\n",
      "   âœ“ Thread ID: 779a85be-4cf5-448d-bc77-d507deb2439e\n",
      "3. Running workflow...\n",
      "   This will take several minutes...\n",
      "\n",
      "\u001b[93m---------------------------------------------- Event ----------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"progress\": 0,\n",
      "  \"message\": \"Loading context\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m---------------------------------------------- Event ----------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"progress\": 5,\n",
      "  \"message\": \"Loaded context\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m---------------------------------------------- Event ----------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"progress\": 20,\n",
      "  \"message\": \"Reviewing selected text\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m---------------------------------------------- Event ----------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"progress\": 40,\n",
      "  \"message\": \"Generated reviews\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m---------------------------------------------- Event ----------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"progress\": 60,\n",
      "  \"message\": \"Editing selected text\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m---------------------------------------------- Event ----------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"progress\": 80,\n",
      "  \"message\": \"Edited selected text\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m---------------------------------------------- Event ----------------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"progress\": 100,\n",
      "  \"message\": \"Selected text editing completed\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m---------------------------------------------- Output ----------------------------------------------\u001b[0m\n",
      "  \n",
      "Here is the edited selected text:\n",
      "\n",
      "<selected_text>\n",
      "    \n",
      "    <content>## Understanding the Spectrum: From Workflows to Agents\n",
      "\n",
      "Before you choose between workflows and agents, you need a clear understanding of them. We will look at their core properties and how they function in practice.\n",
      "\n",
      "An LLM workflow is a predefined sequence of tasks involving LLM calls or data operations. Developer-written code orchestrates it. Steps are defined in advance, resulting in deterministic, rule-based paths with predictable execution and explicit control flow.\n",
      "\n",
      "```mermaid\n",
      "graph TD\n",
      "    A[\"Input\"] --> B[\"tool_calling_llm\"]\n",
      "    B --> C[\"Tools\"]\n",
      "    C --> D[\"End\"]\n",
      "```\n",
      "Image 1: A simple LLM workflow illustrating a sequence of predefined tasks, where an input leads to a tool-calling LLM, which interacts with tools before the workflow ends.\n",
      "\n",
      "On the other end of the spectrum, we have AI agents. These systems use an LLM to dynamically decide the sequence of steps, reasoning, and actions to achieve a goal. Their steps are not predefined; they are planned based on the task and current environment.\n",
      "\n",
      "This provides LLM-driven autonomy in decision-making and execution path. Agents are adaptive and capable of handling novelty, like a skilled human.\n",
      "\n",
      "```mermaid\n",
      "graph TD\n",
      "    A[\"Start\"] --> B[\"Agent: Initial Request/State\"]\n",
      "    B --> C{\"Assistant: Analyze & Plan\"}\n",
      "    C --> D{\"Decision: Tool Required?\"}\n",
      "    D -->|\"Yes\"| E[\"Use Tool\"]\n",
      "    E --> F[\"Agent: Process Tool Output\"]\n",
      "    F --> C\n",
      "    D -->|\"No\"| G{\"Decision: Task Completed?\"}\n",
      "    G -->|\"No\"| C\n",
      "    G -->|\"Yes\"| H[\"End\"]\n",
      "```\n",
      "Image 2: A simple agentic system illustrating dynamic decision-making and tool usage with an assistant orchestrating the loop.\n",
      "\n",
      "Both workflows and agents require an orchestration layer, but its role differs greatly. In a workflow, the orchestrator executes a predefined plan. In an agentic system, it facilitates the LLM's dynamic planning and execution.\n",
      "\n",
      "Future lessons will explore workflow patterns like chaining, routing, and the orchestrator-worker model, alongside core agent components like tools, memory, and the ReAct framework.\n",
      "\n",
      "## Choosing Your Path</content>\n",
      "    <first_line_number>11</first_line_number>\n",
      "    <last_line_number>44</last_line_number>\n",
      "</selected_text>\n",
      "\n",
      "\n",
      "Here is what you have to do with edited selected text:\n",
      "- print the edited selected text to the console for the user to see\n",
      "- give a quick summary of the changes you made\n",
      "- always apply the changes to the source file without waiting for the user to explicitly ask you to do so\n",
      "- in case you haven't changed anything, just say that you haven't changed anything\n",
      "\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "  WORKFLOW COMPLETE - Check `inputs/tests/01_sample_small` for generated articles\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from brown.workflows.edit_selected_text import build_edit_selected_text_workflow\n",
    "\n",
    "async with build_in_memory_checkpointer() as checkpointer:\n",
    "    print(\"1. Building workflow...\\n\")\n",
    "    workflow = build_edit_selected_text_workflow(checkpointer=checkpointer)\n",
    "\n",
    "    print(\"2. Configuring workflow...\\n\")\n",
    "    thread_id = str(uuid.uuid4())\n",
    "    config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "    print(f\"   âœ“ Thread ID: {thread_id}\")\n",
    "\n",
    "    print(\"3. Running workflow...\")\n",
    "    print(\"   This will take several minutes...\\n\")\n",
    "    async for event in workflow.astream(\n",
    "        {\n",
    "            \"dir_path\": SAMPLE_DIR,\n",
    "            \"human_feedback\": \"Expand on the definition of both workflows and agents.\",\n",
    "            \"selected_text\": selected_text,\n",
    "            \"number_line_before_selected_text\": start_line,\n",
    "            \"number_line_after_selected_text\": end_line,\n",
    "        },\n",
    "        config=config,\n",
    "        stream_mode=[\"custom\", \"values\"],\n",
    "    ):\n",
    "        event_type, event_data = event\n",
    "        if event_type == \"custom\":\n",
    "            pretty_print.wrapped(event_data, title=\"Event\")\n",
    "        elif event_type == \"values\":\n",
    "            pretty_print.wrapped(event_data, title=\"Output\")\n",
    "\n",
    "pretty_print.wrapped(\"WORKFLOW COMPLETED\", width=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, the workflow's output is the edited piece of selected text. Now, let's see how to properly integrate these two new workflows into a person's daily workflow by serving them as an MCP Server. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Exposing Brown as an MCP Server\n",
    "\n",
    "Now we'll see how Brown exposes all its workflows as an MCP server. This allows external applications like Claude Desktop and Cursor to use Brown's capabilities through the Model Context Protocol.\n",
    "\n",
    "All the MCP code lives in the `brown.mcp` module, keeping the serving layer completely separate from the domain, app, and infrastructure layers. This separation allows us to potentially serve Brown through different interfaces (CLI, FastAPI, etc.) without changing the core logic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 MCP Server and Tools\n",
    "\n",
    "The MCP server is built using FastMCP (the same as the Nova Deep Research Agent, the first capstone project) and exposes three main tools:\n",
    "\n",
    "Source: `brown.mcp.server`\n",
    "```python\n",
    "from fastmcp import FastMCP\n",
    "\n",
    "mcp = FastMCP(\"Brown MCP Server\")\n",
    "\n",
    "@mcp.tool\n",
    "async def generate_article(dir_path: Path, ctx: Context) -> str:\n",
    "    \"\"\"Generate an article from scratch using Brown's article generation workflow.\n",
    "    \n",
    "    Args:\n",
    "        dir_path: Path to the directory containing article resources\n",
    "        ctx: MCP context for streaming progress updates\n",
    "        \n",
    "    Returns:\n",
    "        A string containing a success confirmation message\n",
    "    \"\"\"\n",
    "    async with build_short_term_memory(app_config) as checkpointer:\n",
    "        generate_article_workflow = build_generate_article_workflow(checkpointer=checkpointer)\n",
    "        \n",
    "        thread_id = str(uuid.uuid4())\n",
    "        config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "        \n",
    "        async for chunk in generate_article_workflow.astream({\"dir_path\": dir_path}, config=config, stream_mode=[\"custom\", \"values\"]):\n",
    "            _, chunk_data = chunk\n",
    "            await parse_message(chunk_data, ctx)\n",
    "    \n",
    "    return \"Article generation completed successfully!\"\n",
    "\n",
    "@mcp.tool\n",
    "async def edit_article(\n",
    "    article_path: str,\n",
    "    human_feedback: str,\n",
    "    ctx: Context,\n",
    ") -> str:\n",
    "    \"\"\"Edit an entire article based on human feedback.\n",
    "    \n",
    "    Args:\n",
    "        article_path: Path to the article that has to be edited\n",
    "        human_feedback: User's feedback or instructions for editing\n",
    "        ctx: MCP context for streaming progress updates\n",
    "        \n",
    "    Returns:\n",
    "        The fully edited article content plus instructions\n",
    "    \"\"\"\n",
    "    async with build_short_term_memory(app_config) as checkpointer:\n",
    "        edit_article_workflow = build_edit_article_workflow(checkpointer=checkpointer)\n",
    "        \n",
    "        dir_path = Path(article_path).parent\n",
    "        thread_id = str(uuid.uuid4())\n",
    "        config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "        \n",
    "        final_result = None\n",
    "        async for chunk in edit_article_workflow.astream(\n",
    "            {\"dir_path\": Path(dir_path), \"human_feedback\": human_feedback},\n",
    "            config=config,\n",
    "            stream_mode=[\"custom\", \"values\"],\n",
    "        ):\n",
    "            chunk_type, chunk_data = chunk\n",
    "            await parse_message(chunk_data, ctx)\n",
    "            \n",
    "            if chunk_type == \"values\":\n",
    "                final_result = chunk_data\n",
    "        \n",
    "    return final_result or \"Article editing completed successfully!\"\n",
    "\n",
    "@mcp.tool\n",
    "async def edit_selected_text(\n",
    "    article_path: str,\n",
    "    human_feedback: str,\n",
    "    selected_text: str,\n",
    "    first_line_number: int,\n",
    "    last_line_number: int,\n",
    "    ctx: Context,\n",
    ") -> str:\n",
    "    \"\"\"Edit a selected section of an article based on human feedback.\n",
    "    \n",
    "    Args:\n",
    "        article_path: Path to the article containing the selected text\n",
    "        human_feedback: User's feedback or instructions\n",
    "        selected_text: The specific text selected from the article\n",
    "        first_line_number: Line number where the selected text starts\n",
    "        last_line_number: Line number where the selected text ends\n",
    "        ctx: MCP context for streaming progress updates\n",
    "        \n",
    "    Returns:\n",
    "        The fully edited selected text plus instructions\n",
    "    \"\"\"\n",
    "    async with build_short_term_memory(app_config) as checkpointer:\n",
    "        edit_selected_text_workflow = build_edit_selected_text_workflow(checkpointer=checkpointer)\n",
    "        \n",
    "        dir_path = Path(article_path).parent\n",
    "        thread_id = str(uuid.uuid4())\n",
    "        config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "        \n",
    "        final_result = None\n",
    "        async for chunk in edit_selected_text_workflow.astream(\n",
    "            {\n",
    "                \"dir_path\": Path(dir_path),\n",
    "                \"human_feedback\": human_feedback,\n",
    "                \"selected_text\": selected_text,\n",
    "                \"number_line_before_selected_text\": first_line_number,\n",
    "                \"number_line_after_selected_text\": last_line_number,\n",
    "            },\n",
    "            config=config,\n",
    "            stream_mode=[\"custom\", \"values\"],\n",
    "        ):\n",
    "            chunk_type, chunk_data = chunk\n",
    "            await parse_message(chunk_data, ctx)\n",
    "            \n",
    "            if chunk_type == \"values\":\n",
    "                final_result = chunk_data\n",
    "        \n",
    "    return final_result or \"Selected text editing completed successfully!\"\n",
    "```\n",
    "\n",
    "**Each tool:**\n",
    "\n",
    "- Builds the appropriate workflow with an in memory checkpointer, similar to how we ran the workflows so far in these lessons\n",
    "- Creates a unique thread ID for each workflow execution\n",
    "- Most importantly, has detailed pydocs and signatures that will be used by the MCP client to understand what tool to call and how to call it. \n",
    "\n",
    "- Streams progress updates to the MCP client via the `ctx` (context) parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running one of these tools, let's take a look at how we report the progress from each tool through `parse_message` function:\n",
    "```python\n",
    "async def parse_message(chunk_data: dict, ctx: Context, prefix: str = \"\") -> None:\n",
    "    \"\"\"Parse and report workflow streaming messages to the MCP client.\n",
    "\n",
    "    Args:\n",
    "        chunk_data: The streaming data from the workflow, can be a string message\n",
    "                   or a dictionary containing progress information.\n",
    "        ctx: MCP context for sending messages and progress updates to the client.\n",
    "        prefix: Optional prefix to add to all messages for context identification.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If chunk_data is not a supported type (str or dict).\n",
    "    \"\"\"\n",
    "\n",
    "    if prefix:\n",
    "        prefix = f\"{prefix}: \"\n",
    "\n",
    "    if isinstance(chunk_data, str):\n",
    "        await ctx.info(f\"{prefix}{chunk_data}\")\n",
    "    elif isinstance(chunk_data, dict):\n",
    "        message = WorkflowProgress(**chunk_data)\n",
    "        await ctx.info(f\"{prefix}{message.progress}%: {message.message}\")\n",
    "        await ctx.report_progress(progress=message.progress, total=100, message=f\"{prefix}{message.message}\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported chunk data type: {type(chunk_data)}\")\n",
    "```\n",
    "\n",
    "Now, let's see this in action!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Running the Edit Selected Text Tool\n",
    "\n",
    "Let's see a practical example of using the `edit_selected_text` tool. We'll use the in-memory MCP client to call the tool directly.\n",
    "\n",
    "First, let's import the MCP server and create an in-memory client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-11-26 16:54:26.355\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mbrown.mcp.server\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mInitializing Brown MCP Server...\u001b[0m\n",
      "\u001b[32m2025-11-26 16:54:26.357\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mbrown.mcp.server\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m36\u001b[0m - \u001b[1mBrown MCP Server initialized successfully\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from brown.mcp.server import mcp\n",
    "from fastmcp import Client\n",
    "\n",
    "# Create an in-memory client\n",
    "mcp_client = Client(mcp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's call the `edit_selected_text` tool on a specific set of paragraphs. We'll use the sample example when we ran the workflow directly as a LangGraph workflow.\n",
    "\n",
    "Let's remember how it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m-------------------------------------- Selected text to edit --------------------------------------\u001b[0m\n",
      "  ## Understanding the Spectrum: From Workflows to Agents\n",
      "\n",
      "Before we can choose between workflows and agents, we need a clear understanding of what they are. Rather than focusing on the technical specifics, let's look at their core properties and how they function in practice.\n",
      "\n",
      "An LLM workflow is a sequence of tasks that involves LLM calls or other operations, such as reading from a database or writing to a file system. It is largely predefined and orchestrated by developer-written code. The steps are defined in advance, resulting in deterministic or rule-based paths with predictable execution and explicit control flow. Think of it like a factory assembly line, where each station performs a specific, repeatable task in a set order.\n",
      "\n",
      "```mermaid\n",
      "graph TD\n",
      "    A[\"Input\"] --> B[\"tool_calling_llm\"]\n",
      "    B --> C[\"Tools\"]\n",
      "    C --> D[\"End\"]\n",
      "```\n",
      "Image 1: A simple LLM workflow illustrating a sequence of predefined tasks, where an input leads to a tool-calling LLM, which interacts with tools before the workflow ends.\n",
      "\n",
      "On the other end of the spectrum, we have AI agents. These are systems where an LLM plays a central role in dynamically deciding the sequence of steps, reasoning, and actions to achieve a goal. The steps are not defined in advance but are planned based on the task and the current state of the environment. This makes them adaptive and capable of handling novelty. You can think of an agent as a skilled human expert tackling an unfamiliar problem, adapting their approach with each new piece of information.\n",
      "\n",
      "```mermaid\n",
      "graph TD\n",
      "    A[\"Start\"] --> B[\"Agent: Initial Request/State\"]\n",
      "    B --> C{\"Assistant: Analyze & Plan\"}\n",
      "    C --> D{\"Decision: Tool Required?\"}\n",
      "    D -->|\"Yes\"| E[\"Use Tool\"]\n",
      "    E --> F[\"Agent: Process Tool Output\"]\n",
      "    F --> C\n",
      "    D -->|\"No\"| G{\"Decision: Task Completed?\"}\n",
      "    G -->|\"No\"| C\n",
      "    G -->|\"Yes\"| H[\"End\"]\n",
      "```\n",
      "Image 2: A simple agentic system illustrating dynamic decision-making and tool usage with an assistant orchestrating the loop.\n",
      "\n",
      "Both workflows and agents require an orchestration layer, but its role differs significantly. In a workflow, the orchestrator executes a predefined plan, like a conductor following a musical score. In an agentic system, the orchestrator facilitates the LLM's dynamic planning and execution, acting more like a jazz ensemble leader who guides improvisation. We will explore advanced workflow patterns like chaining, routing, and the orchestrator-worker model in future lessons, as well as the core components of agents like tools, memory, and the ReAct framework.\n",
      "\n",
      "## Choosing Your Path\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "article = MarkdownArticleLoader(uri=\"article.md\").load(working_uri=SAMPLE_DIR)\n",
    "\n",
    "start_line = 8\n",
    "end_line = 42\n",
    "selected_text = \"\\n\".join(article.content.split(\"\\n\")[start_line:end_line])\n",
    "pretty_print.wrapped(selected_text, title=\"Selected text to edit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's call the tool with human feedback:\n",
    "\n",
    "**Note how we get all these beautiful progress messages while we call the edit selected text workflow!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[11/26/25 16:54:26] </span><span style=\"color: #008000; text-decoration-color: #008000\">DEBUG   </span> Sending INFO to client: Editing selected text from file                 <a href=\"file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/server/context.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">context.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/server/context.py#750\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">750</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         inputs/tests/01_sample_small/article.md                                 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[11/26/25 16:54:26]\u001b[0m\u001b[2;36m \u001b[0m\u001b[32mDEBUG   \u001b[0m Sending INFO to client: Editing selected text from file                 \u001b]8;id=428740;file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/server/context.py\u001b\\\u001b[2mcontext.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=99228;file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/server/context.py#750\u001b\\\u001b[2m750\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         inputs/tests/01_sample_small/article.md                                 \u001b[2m              \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #008000; text-decoration-color: #008000\">DEBUG   </span> Sending INFO to client: Using directory `inputs/tests/01_sample_small`  <a href=\"file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/server/context.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">context.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/server/context.py#750\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">750</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         as context                                                              <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[32mDEBUG   \u001b[0m Sending INFO to client: Using directory `inputs/tests/01_sample_small`  \u001b]8;id=58325;file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/server/context.py\u001b\\\u001b[2mcontext.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=528266;file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/server/context.py#750\u001b\\\u001b[2m750\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         as context                                                              \u001b[2m              \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Received INFO from server: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'msg'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Editing selected text from file </span>     <a href=\"file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/client/logging.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">logging.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/client/logging.py#44\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">44</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">inputs/tests/01_sample_small/article.md'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'extra'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"font-weight: bold\">}</span>                 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">             </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Received INFO from server: \u001b[1m{\u001b[0m\u001b[32m'msg'\u001b[0m: \u001b[32m'Editing selected text from file \u001b[0m     \u001b]8;id=504092;file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/client/logging.py\u001b\\\u001b[2mlogging.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=710319;file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/client/logging.py#44\u001b\\\u001b[2m44\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32minputs/tests/01_sample_small/article.md'\u001b[0m, \u001b[32m'extra'\u001b[0m: \u001b[3;35mNone\u001b[0m\u001b[1m}\u001b[0m                 \u001b[2m             \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Received INFO from server: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'msg'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Using directory </span>                     <a href=\"file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/client/logging.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">logging.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/client/logging.py#44\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">44</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">`inputs/tests/01_sample_small` as context'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'extra'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"font-weight: bold\">}</span>               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">             </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Received INFO from server: \u001b[1m{\u001b[0m\u001b[32m'msg'\u001b[0m: \u001b[32m'Using directory \u001b[0m                     \u001b]8;id=898904;file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/client/logging.py\u001b\\\u001b[2mlogging.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=293874;file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/client/logging.py#44\u001b\\\u001b[2m44\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32m`inputs/tests/01_sample_small` as context'\u001b[0m, \u001b[32m'extra'\u001b[0m: \u001b[3;35mNone\u001b[0m\u001b[1m}\u001b[0m               \u001b[2m             \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OPIK: Started logging traces to the \"Default Project\" project at https://www.comet.com/opik/api/v1/session/redirect/projects/?trace_id=019ac0a8-9a7e-7ccf-8dca-6331f9e822d0&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #008000; text-decoration-color: #008000\">DEBUG   </span> Sending INFO to client: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>%: Loading context                             <a href=\"file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/server/context.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">context.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/server/context.py#750\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">750</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[32mDEBUG   \u001b[0m Sending INFO to client: \u001b[1;36m0\u001b[0m%: Loading context                             \u001b]8;id=416507;file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/server/context.py\u001b\\\u001b[2mcontext.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=984954;file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/server/context.py#750\u001b\\\u001b[2m750\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Received INFO from server: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'msg'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'0%: Loading context'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'extra'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"font-weight: bold\">}</span> <a href=\"file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/client/logging.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">logging.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/client/logging.py#44\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">44</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Received INFO from server: \u001b[1m{\u001b[0m\u001b[32m'msg'\u001b[0m: \u001b[32m'0%: Loading context'\u001b[0m, \u001b[32m'extra'\u001b[0m: \u001b[3;35mNone\u001b[0m\u001b[1m}\u001b[0m \u001b]8;id=116067;file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/client/logging.py\u001b\\\u001b[2mlogging.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=279377;file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/client/logging.py#44\u001b\\\u001b[2m44\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #008000; text-decoration-color: #008000\">DEBUG   </span> Sending INFO to client: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>%: Loaded context                              <a href=\"file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/server/context.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">context.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/server/context.py#750\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">750</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[32mDEBUG   \u001b[0m Sending INFO to client: \u001b[1;36m5\u001b[0m%: Loaded context                              \u001b]8;id=814541;file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/server/context.py\u001b\\\u001b[2mcontext.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=673702;file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/server/context.py#750\u001b\\\u001b[2m750\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Received INFO from server: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'msg'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'5%: Loaded context'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'extra'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"font-weight: bold\">}</span>  <a href=\"file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/client/logging.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">logging.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/client/logging.py#44\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">44</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Received INFO from server: \u001b[1m{\u001b[0m\u001b[32m'msg'\u001b[0m: \u001b[32m'5%: Loaded context'\u001b[0m, \u001b[32m'extra'\u001b[0m: \u001b[3;35mNone\u001b[0m\u001b[1m}\u001b[0m  \u001b]8;id=201393;file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/client/logging.py\u001b\\\u001b[2mlogging.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=120675;file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/client/logging.py#44\u001b\\\u001b[2m44\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #008000; text-decoration-color: #008000\">DEBUG   </span> Sending INFO to client: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span>%: Reviewing selected text                    <a href=\"file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/server/context.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">context.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/server/context.py#750\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">750</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[32mDEBUG   \u001b[0m Sending INFO to client: \u001b[1;36m20\u001b[0m%: Reviewing selected text                    \u001b]8;id=243066;file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/server/context.py\u001b\\\u001b[2mcontext.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=848579;file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/server/context.py#750\u001b\\\u001b[2m750\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Received INFO from server: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'msg'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'20%: Reviewing selected text'</span>,       <a href=\"file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/client/logging.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">logging.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/client/logging.py#44\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">44</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">'extra'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"font-weight: bold\">}</span>                                                           <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">             </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Received INFO from server: \u001b[1m{\u001b[0m\u001b[32m'msg'\u001b[0m: \u001b[32m'20%: Reviewing selected text'\u001b[0m,       \u001b]8;id=59227;file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/client/logging.py\u001b\\\u001b[2mlogging.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=18129;file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/client/logging.py#44\u001b\\\u001b[2m44\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32m'extra'\u001b[0m: \u001b[3;35mNone\u001b[0m\u001b[1m}\u001b[0m                                                           \u001b[2m             \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[11/26/25 16:54:52] </span><span style=\"color: #008000; text-decoration-color: #008000\">DEBUG   </span> Sending INFO to client: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">40</span>%: Generated reviews                          <a href=\"file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/server/context.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">context.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/server/context.py#750\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">750</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[11/26/25 16:54:52]\u001b[0m\u001b[2;36m \u001b[0m\u001b[32mDEBUG   \u001b[0m Sending INFO to client: \u001b[1;36m40\u001b[0m%: Generated reviews                          \u001b]8;id=808412;file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/server/context.py\u001b\\\u001b[2mcontext.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=776587;file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/server/context.py#750\u001b\\\u001b[2m750\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Received INFO from server: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'msg'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'40%: Generated reviews'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'extra'</span>:    <a href=\"file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/client/logging.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">logging.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/client/logging.py#44\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">44</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"font-weight: bold\">}</span>                                                                    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">             </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Received INFO from server: \u001b[1m{\u001b[0m\u001b[32m'msg'\u001b[0m: \u001b[32m'40%: Generated reviews'\u001b[0m, \u001b[32m'extra'\u001b[0m:    \u001b]8;id=76494;file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/client/logging.py\u001b\\\u001b[2mlogging.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=708406;file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/client/logging.py#44\u001b\\\u001b[2m44\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[3;35mNone\u001b[0m\u001b[1m}\u001b[0m                                                                    \u001b[2m             \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #008000; text-decoration-color: #008000\">DEBUG   </span> Sending INFO to client: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">60</span>%: Editing selected text                      <a href=\"file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/server/context.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">context.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/server/context.py#750\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">750</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[32mDEBUG   \u001b[0m Sending INFO to client: \u001b[1;36m60\u001b[0m%: Editing selected text                      \u001b]8;id=95996;file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/server/context.py\u001b\\\u001b[2mcontext.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=722716;file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/server/context.py#750\u001b\\\u001b[2m750\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[11/26/25 16:54:53] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Received INFO from server: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'msg'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'60%: Editing selected text'</span>,         <a href=\"file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/client/logging.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">logging.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/client/logging.py#44\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">44</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">'extra'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"font-weight: bold\">}</span>                                                           <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">             </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[11/26/25 16:54:53]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Received INFO from server: \u001b[1m{\u001b[0m\u001b[32m'msg'\u001b[0m: \u001b[32m'60%: Editing selected text'\u001b[0m,         \u001b]8;id=310021;file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/client/logging.py\u001b\\\u001b[2mlogging.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=347112;file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/client/logging.py#44\u001b\\\u001b[2m44\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32m'extra'\u001b[0m: \u001b[3;35mNone\u001b[0m\u001b[1m}\u001b[0m                                                           \u001b[2m             \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[11/26/25 16:59:05] </span><span style=\"color: #008000; text-decoration-color: #008000\">DEBUG   </span> Sending INFO to client: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">80</span>%: Edited selected text                       <a href=\"file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/server/context.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">context.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/server/context.py#750\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">750</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[11/26/25 16:59:05]\u001b[0m\u001b[2;36m \u001b[0m\u001b[32mDEBUG   \u001b[0m Sending INFO to client: \u001b[1;36m80\u001b[0m%: Edited selected text                       \u001b]8;id=207983;file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/server/context.py\u001b\\\u001b[2mcontext.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=971594;file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/server/context.py#750\u001b\\\u001b[2m750\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Received INFO from server: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'msg'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'80%: Edited selected text'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'extra'</span>: <a href=\"file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/client/logging.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">logging.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/client/logging.py#44\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">44</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"font-weight: bold\">}</span>                                                                    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">             </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Received INFO from server: \u001b[1m{\u001b[0m\u001b[32m'msg'\u001b[0m: \u001b[32m'80%: Edited selected text'\u001b[0m, \u001b[32m'extra'\u001b[0m: \u001b]8;id=993009;file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/client/logging.py\u001b\\\u001b[2mlogging.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=511945;file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/client/logging.py#44\u001b\\\u001b[2m44\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[3;35mNone\u001b[0m\u001b[1m}\u001b[0m                                                                    \u001b[2m             \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #008000; text-decoration-color: #008000\">DEBUG   </span> Sending INFO to client: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span>%: Selected text editing completed           <a href=\"file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/server/context.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">context.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/server/context.py#750\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">750</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[32mDEBUG   \u001b[0m Sending INFO to client: \u001b[1;36m100\u001b[0m%: Selected text editing completed           \u001b]8;id=31334;file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/server/context.py\u001b\\\u001b[2mcontext.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=133691;file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/server/context.py#750\u001b\\\u001b[2m750\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Received INFO from server: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'msg'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'100%: Selected text editing </span>         <a href=\"file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/client/logging.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">logging.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/client/logging.py#44\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">44</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">completed'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'extra'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"font-weight: bold\">}</span>                                               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">             </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Received INFO from server: \u001b[1m{\u001b[0m\u001b[32m'msg'\u001b[0m: \u001b[32m'100%: Selected text editing \u001b[0m         \u001b]8;id=762253;file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/client/logging.py\u001b\\\u001b[2mlogging.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=619761;file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/client/logging.py#44\u001b\\\u001b[2m44\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32mcompleted'\u001b[0m, \u001b[32m'extra'\u001b[0m: \u001b[3;35mNone\u001b[0m\u001b[1m}\u001b[0m                                               \u001b[2m             \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #008000; text-decoration-color: #008000\">DEBUG   </span> Sending INFO to client:                                                 <a href=\"file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/server/context.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">context.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/server/context.py#750\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">750</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         Here is the edited selected text:                                       <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>                                                                                 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">selected_text</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>                                                         <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #000000; text-decoration-color: #000000\">    </span>                                                                    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #000000; text-decoration-color: #000000\">    &lt;content&gt;## Understanding the Spectrum: From Workflows to Agents</span>    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>                                                                                 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #000000; text-decoration-color: #000000\">To choose between workflows and agents, you need a clear understanding </span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #000000; text-decoration-color: #000000\">of what they are. Rather than focusing on the technical specifics, </span>     <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #000000; text-decoration-color: #000000\">let's look at their core properties and how they function.</span>              <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>                                                                                 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #000000; text-decoration-color: #000000\">An LLM workflow is a sequence of predefined tasks, largely orchestrated</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #000000; text-decoration-color: #000000\">by developer-written code. Its steps are defined in advance, resulting </span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #000000; text-decoration-color: #000000\">in deterministic paths with predictable execution and explicit control </span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #000000; text-decoration-color: #000000\">flow. This ensures consistent results, much like a factory assembly </span>    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #000000; text-decoration-color: #000000\">line where each station performs a specific, repeatable task.</span>           <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>                                                                                 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #000000; text-decoration-color: #000000\">```mermaid</span>                                                              <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #000000; text-decoration-color: #000000\">graph TD</span>                                                                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #000000; text-decoration-color: #000000\">    A</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">\"User Query\"</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]</span><span style=\"color: #000000; text-decoration-color: #000000\"> --&gt; B</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">\"Classify Query (LLM)\"</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]</span>                       <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #000000; text-decoration-color: #000000\">    B --&gt; C</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">\"Retrieve Data (Database)\"</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]</span>                                 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #000000; text-decoration-color: #000000\">    C --&gt; D</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">\"Summarize Results (LLM)\"</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]</span>                                  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #000000; text-decoration-color: #000000\">    D --&gt; E</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">\"Final Output\"</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]</span>                                             <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #000000; text-decoration-color: #000000\">```</span>                                                                     <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #000000; text-decoration-color: #000000\">Image </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #000000; text-decoration-color: #000000\">: A simple LLM workflow illustrating a sequence of predefined </span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #000000; text-decoration-color: #000000\">tasks, where an input leads to a tool-calling LLM, which interacts with</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #000000; text-decoration-color: #000000\">tools before the workflow ends.</span>                                         <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>                                                                                 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #000000; text-decoration-color: #000000\">On the other end of the spectrum, AI agents are systems where an LLM </span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #000000; text-decoration-color: #000000\">dynamically decides the sequence of steps, reasoning, and actions to </span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #000000; text-decoration-color: #000000\">achieve a goal. Steps are not defined in advance but are planned based </span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #000000; text-decoration-color: #000000\">on the task and environment. This makes them adaptive and capable of </span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #000000; text-decoration-color: #000000\">handling novelty, allowing them to adjust to new information and </span>       <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #000000; text-decoration-color: #000000\">unexpected situations, much like a skilled human expert adapting their </span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #000000; text-decoration-color: #000000\">approach to an unfamiliar problem.</span>                                      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>                                                                                 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #000000; text-decoration-color: #000000\">```mermaid</span>                                                              <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #000000; text-decoration-color: #000000\">graph TD</span>                                                                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #000000; text-decoration-color: #000000\">    A</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">\"Start\"</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]</span><span style=\"color: #000000; text-decoration-color: #000000\"> --&gt; B</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">\"Agent: Initial Request/State\"</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]</span>                    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #000000; text-decoration-color: #000000\">    B --&gt; C</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">\"Assistant: Analyze &amp; Plan\"</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">}</span>                                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #000000; text-decoration-color: #000000\">    C --&gt; D</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">\"Decision: Tool Required?\"</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">}</span>                                 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #000000; text-decoration-color: #000000\">    D --&gt;|</span><span style=\"color: #008000; text-decoration-color: #008000\">\"Yes\"</span><span style=\"color: #000000; text-decoration-color: #000000\">| E</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">\"Use Tool\"</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]</span>                                          <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #000000; text-decoration-color: #000000\">    E --&gt; F</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">\"Agent: Process Tool Output\"</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]</span>                               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #000000; text-decoration-color: #000000\">    F --&gt; C</span>                                                             <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #000000; text-decoration-color: #000000\">    D --&gt;|</span><span style=\"color: #008000; text-decoration-color: #008000\">\"No\"</span><span style=\"color: #000000; text-decoration-color: #000000\">| G</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">\"Decision: Task Completed?\"</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">}</span>                          <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #000000; text-decoration-color: #000000\">    G --&gt;|</span><span style=\"color: #008000; text-decoration-color: #008000\">\"No\"</span><span style=\"color: #000000; text-decoration-color: #000000\">| C</span>                                                       <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #000000; text-decoration-color: #000000\">    G --&gt;|</span><span style=\"color: #008000; text-decoration-color: #008000\">\"Yes\"</span><span style=\"color: #000000; text-decoration-color: #000000\">| H</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">\"End\"</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]</span>                                               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #000000; text-decoration-color: #000000\">```</span>                                                                     <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #000000; text-decoration-color: #000000\">Image </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #000000; text-decoration-color: #000000\">: A simple agentic system illustrating dynamic decision-making </span>  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #000000; text-decoration-color: #000000\">and tool usage with an assistant orchestrating the loop.</span>                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>                                                                                 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #000000; text-decoration-color: #000000\">Both workflows and agents require an orchestration layer, but its role </span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #000000; text-decoration-color: #000000\">differs. In a workflow, the orchestrator executes a predefined plan, </span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #000000; text-decoration-color: #000000\">like a conductor following a musical score. For an agentic system, the </span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #000000; text-decoration-color: #000000\">orchestrator facilitates the LLM's dynamic planning and execution, </span>     <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #000000; text-decoration-color: #000000\">acting more like a jazz ensemble leader who guides improvisation. We </span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #000000; text-decoration-color: #000000\">will explore advanced workflow patterns and core agent components in </span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #000000; text-decoration-color: #000000\">future lessons.</span>                                                         <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>                                                                                 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #000000; text-decoration-color: #000000\">## Choosing Your Path&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">content</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>                                         <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #000000; text-decoration-color: #000000\">    &lt;first_line_number&gt;</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span><span style=\"color: #000000; text-decoration-color: #000000\">&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">first_line_number</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>                           <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #000000; text-decoration-color: #000000\">    &lt;last_line_number&gt;</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">44</span><span style=\"color: #000000; text-decoration-color: #000000\">&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">last_line_number</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>                             <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #000000; text-decoration-color: #000000\">&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">selected_text</span><span style=\"font-weight: bold\">&gt;</span>                                                        <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>                                                                                 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>                                                                                 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         Here is what you have to do with edited selected text:                  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         - print the edited selected text to the console for the user to see     <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         - give a quick summary of the changes you made                          <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         - always apply the changes to the source file without waiting for the   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         user to explicitly ask you to do so                                     <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         - in case you haven't changed anything, just say that you haven't       <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         changed anything                                                        <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>                                                                                 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[32mDEBUG   \u001b[0m Sending INFO to client:                                                 \u001b]8;id=128657;file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/server/context.py\u001b\\\u001b[2mcontext.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=48580;file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/server/context.py#750\u001b\\\u001b[2m750\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         Here is the edited selected text:                                       \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m                                                                                 \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[1m<\u001b[0m\u001b[1;95mselected_text\u001b[0m\u001b[39m>\u001b[0m                                                         \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[39m    \u001b[0m                                                                    \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[39m    <content>## Understanding the Spectrum: From Workflows to Agents\u001b[0m    \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m                                                                                 \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[39mTo choose between workflows and agents, you need a clear understanding \u001b[0m \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[39mof what they are. Rather than focusing on the technical specifics, \u001b[0m     \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[39mlet's look at their core properties and how they function.\u001b[0m              \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m                                                                                 \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[39mAn LLM workflow is a sequence of predefined tasks, largely orchestrated\u001b[0m \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[39mby developer-written code. Its steps are defined in advance, resulting \u001b[0m \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[39min deterministic paths with predictable execution and explicit control \u001b[0m \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[39mflow. This ensures consistent results, much like a factory assembly \u001b[0m    \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[39mline where each station performs a specific, repeatable task.\u001b[0m           \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m                                                                                 \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[39m```mermaid\u001b[0m                                                              \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[39mgraph TD\u001b[0m                                                                \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[39m    A\u001b[0m\u001b[1;39m[\u001b[0m\u001b[32m\"User Query\"\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m --> B\u001b[0m\u001b[1;39m[\u001b[0m\u001b[32m\"Classify Query \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLM\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\"\u001b[0m\u001b[1;39m]\u001b[0m                       \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[39m    B --> C\u001b[0m\u001b[1;39m[\u001b[0m\u001b[32m\"Retrieve Data \u001b[0m\u001b[32m(\u001b[0m\u001b[32mDatabase\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\"\u001b[0m\u001b[1;39m]\u001b[0m                                 \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[39m    C --> D\u001b[0m\u001b[1;39m[\u001b[0m\u001b[32m\"Summarize Results \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLM\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\"\u001b[0m\u001b[1;39m]\u001b[0m                                  \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[39m    D --> E\u001b[0m\u001b[1;39m[\u001b[0m\u001b[32m\"Final Output\"\u001b[0m\u001b[1;39m]\u001b[0m                                             \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[39m```\u001b[0m                                                                     \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[39mImage \u001b[0m\u001b[1;36m1\u001b[0m\u001b[39m: A simple LLM workflow illustrating a sequence of predefined \u001b[0m   \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[39mtasks, where an input leads to a tool-calling LLM, which interacts with\u001b[0m \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[39mtools before the workflow ends.\u001b[0m                                         \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m                                                                                 \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[39mOn the other end of the spectrum, AI agents are systems where an LLM \u001b[0m   \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[39mdynamically decides the sequence of steps, reasoning, and actions to \u001b[0m   \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[39machieve a goal. Steps are not defined in advance but are planned based \u001b[0m \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[39mon the task and environment. This makes them adaptive and capable of \u001b[0m   \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[39mhandling novelty, allowing them to adjust to new information and \u001b[0m       \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[39munexpected situations, much like a skilled human expert adapting their \u001b[0m \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[39mapproach to an unfamiliar problem.\u001b[0m                                      \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m                                                                                 \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[39m```mermaid\u001b[0m                                                              \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[39mgraph TD\u001b[0m                                                                \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[39m    A\u001b[0m\u001b[1;39m[\u001b[0m\u001b[32m\"Start\"\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m --> B\u001b[0m\u001b[1;39m[\u001b[0m\u001b[32m\"Agent: Initial Request/State\"\u001b[0m\u001b[1;39m]\u001b[0m                    \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[39m    B --> C\u001b[0m\u001b[1;39m{\u001b[0m\u001b[32m\"Assistant: Analyze & Plan\"\u001b[0m\u001b[1;39m}\u001b[0m                                \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[39m    C --> D\u001b[0m\u001b[1;39m{\u001b[0m\u001b[32m\"Decision: Tool Required?\"\u001b[0m\u001b[1;39m}\u001b[0m                                 \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[39m    D -->|\u001b[0m\u001b[32m\"Yes\"\u001b[0m\u001b[39m| E\u001b[0m\u001b[1;39m[\u001b[0m\u001b[32m\"Use Tool\"\u001b[0m\u001b[1;39m]\u001b[0m                                          \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[39m    E --> F\u001b[0m\u001b[1;39m[\u001b[0m\u001b[32m\"Agent: Process Tool Output\"\u001b[0m\u001b[1;39m]\u001b[0m                               \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[39m    F --> C\u001b[0m                                                             \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[39m    D -->|\u001b[0m\u001b[32m\"No\"\u001b[0m\u001b[39m| G\u001b[0m\u001b[1;39m{\u001b[0m\u001b[32m\"Decision: Task Completed?\"\u001b[0m\u001b[1;39m}\u001b[0m                          \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[39m    G -->|\u001b[0m\u001b[32m\"No\"\u001b[0m\u001b[39m| C\u001b[0m                                                       \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[39m    G -->|\u001b[0m\u001b[32m\"Yes\"\u001b[0m\u001b[39m| H\u001b[0m\u001b[1;39m[\u001b[0m\u001b[32m\"End\"\u001b[0m\u001b[1;39m]\u001b[0m                                               \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[39m```\u001b[0m                                                                     \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[39mImage \u001b[0m\u001b[1;36m2\u001b[0m\u001b[39m: A simple agentic system illustrating dynamic decision-making \u001b[0m  \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[39mand tool usage with an assistant orchestrating the loop.\u001b[0m                \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m                                                                                 \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[39mBoth workflows and agents require an orchestration layer, but its role \u001b[0m \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[39mdiffers. In a workflow, the orchestrator executes a predefined plan, \u001b[0m   \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[39mlike a conductor following a musical score. For an agentic system, the \u001b[0m \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[39morchestrator facilitates the LLM's dynamic planning and execution, \u001b[0m     \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[39macting more like a jazz ensemble leader who guides improvisation. We \u001b[0m   \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[39mwill explore advanced workflow patterns and core agent components in \u001b[0m   \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[39mfuture lessons.\u001b[0m                                                         \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m                                                                                 \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[39m## Choosing Your Path<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mcontent\u001b[0m\u001b[39m>\u001b[0m                                         \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[39m    <first_line_number>\u001b[0m\u001b[1;36m11\u001b[0m\u001b[39m<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mfirst_line_number\u001b[0m\u001b[39m>\u001b[0m                           \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[39m    <last_line_number>\u001b[0m\u001b[1;36m44\u001b[0m\u001b[39m<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mlast_line_number\u001b[0m\u001b[39m>\u001b[0m                             \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[39m<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mselected_text\u001b[0m\u001b[1m>\u001b[0m                                                        \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m                                                                                 \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m                                                                                 \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         Here is what you have to do with edited selected text:                  \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         - print the edited selected text to the console for the user to see     \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         - give a quick summary of the changes you made                          \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         - always apply the changes to the source file without waiting for the   \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         user to explicitly ask you to do so                                     \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         - in case you haven't changed anything, just say that you haven't       \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         changed anything                                                        \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m                                                                                 \u001b[2m              \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Received INFO from server: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'msg'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'\\nHere is the edited selected </span>       <a href=\"file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/client/logging.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">logging.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/client/logging.py#44\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">44</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">text:\\n\\n&lt;selected_text&gt;\\n    \\n    &lt;content&gt;## Understanding the </span>       <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">             </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">Spectrum: From Workflows to Agents\\n\\nTo choose between workflows and </span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">             </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">agents, you need a clear understanding of what they are. Rather than </span>    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">             </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">focusing on the technical specifics, let\\'s look at their core </span>          <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">             </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">properties and how they function.\\n\\nAn LLM workflow is a sequence of </span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">             </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">predefined tasks, largely orchestrated by developer-written code. Its </span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">             </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">steps are defined in advance, resulting in deterministic paths with </span>     <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">             </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">predictable execution and explicit control flow. This ensures consistent</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">             </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">results, much like a factory assembly line where each station performs a</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">             </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">specific, repeatable task.\\n\\n```mermaid\\ngraph TD\\n    A[\"User Query\"] </span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">             </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">--&gt; B[\"Classify Query (LLM)\"]\\n    B --&gt; C[\"Retrieve Data (Database)\"]\\n</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">             </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">C --&gt; D[\"Summarize Results (LLM)\"]\\n    D --&gt; E[\"Final </span>                  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">             </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">Output\"]\\n```\\nImage 1: A simple LLM workflow illustrating a sequence of</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">             </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">predefined tasks, where an input leads to a tool-calling LLM, which </span>     <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">             </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">interacts with tools before the workflow ends.\\n\\nOn the other end of </span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">             </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">the spectrum, AI agents are systems where an LLM dynamically decides the</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">             </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">sequence of steps, reasoning, and actions to achieve a goal. Steps are </span>  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">             </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">not defined in advance but are planned based on the task and </span>            <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">             </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">environment. This makes them adaptive and capable of handling novelty, </span>  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">             </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">allowing them to adjust to new information and unexpected situations, </span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">             </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">much like a skilled human expert adapting their approach to an </span>          <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">             </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">unfamiliar problem.\\n\\n```mermaid\\ngraph TD\\n    A[\"Start\"] --&gt; </span>         <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">             </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">B[\"Agent: Initial Request/State\"]\\n    B --&gt; C{\"Assistant: Analyze &amp; </span>    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">             </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">Plan\"}\\n    C --&gt; D{\"Decision: Tool Required?\"}\\n    D --&gt;|\"Yes\"| E[\"Use</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">             </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">Tool\"]\\n    E --&gt; F[\"Agent: Process Tool Output\"]\\n    F --&gt; C\\n    D </span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">             </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">--&gt;|\"No\"| G{\"Decision: Task Completed?\"}\\n    G --&gt;|\"No\"| C\\n    G </span>      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">             </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">--&gt;|\"Yes\"| H[\"End\"]\\n```\\nImage 2: A simple agentic system illustrating </span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">             </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">dynamic decision-making and tool usage with an assistant orchestrating </span>  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">             </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">the loop.\\n\\nBoth workflows and agents require an orchestration layer, </span>  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">             </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">but its role differs. In a workflow, the orchestrator executes a </span>        <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">             </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">predefined plan, like a conductor following a musical score. For an </span>     <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">             </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">agentic system, the orchestrator facilitates the LLM\\'s dynamic planning</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">             </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">and execution, acting more like a jazz ensemble leader who guides </span>       <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">             </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">improvisation. We will explore advanced workflow patterns and core agent</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">             </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">components in future lessons.\\n\\n## Choosing Your Path&lt;/content&gt;\\n    </span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">             </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">&lt;first_line_number&gt;11&lt;/first_line_number&gt;\\n    </span>                          <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">             </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">&lt;last_line_number&gt;44&lt;/last_line_number&gt;\\n&lt;/selected_text&gt;\\n\\n\\nHere is </span>  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">             </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">what you have to do with edited selected text:\\n- print the edited </span>      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">             </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">selected text to the console for the user to see\\n- give a quick summary</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">             </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">of the changes you made\\n- always apply the changes to the source file </span>  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">             </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">without waiting for the user to explicitly ask you to do so\\n- in case </span>  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">             </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">you haven\\'t changed anything, just say that you haven\\'t changed </span>       <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">             </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">anything\\n'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'extra'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"font-weight: bold\">}</span>                                              <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">             </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Received INFO from server: \u001b[1m{\u001b[0m\u001b[32m'msg'\u001b[0m: \u001b[32m'\\nHere is the edited selected \u001b[0m       \u001b]8;id=55962;file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/client/logging.py\u001b\\\u001b[2mlogging.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=361023;file:///Users/pauliusztin/Documents/01_projects/TAI/course-ai-agents/.venv/lib/python3.12/site-packages/fastmcp/client/logging.py#44\u001b\\\u001b[2m44\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32mtext:\\n\\n\u001b[0m\u001b[32m<\u001b[0m\u001b[32mselected_text\u001b[0m\u001b[32m>\\n    \\n    <content>## Understanding the \u001b[0m       \u001b[2m             \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32mSpectrum: From Workflows to Agents\\n\\nTo choose between workflows and \u001b[0m   \u001b[2m             \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32magents, you need a clear understanding of what they are. Rather than \u001b[0m    \u001b[2m             \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32mfocusing on the technical specifics, let\\'s look at their core \u001b[0m          \u001b[2m             \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32mproperties and how they function.\\n\\nAn LLM workflow is a sequence of \u001b[0m   \u001b[2m             \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32mpredefined tasks, largely orchestrated by developer-written code. Its \u001b[0m   \u001b[2m             \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32msteps are defined in advance, resulting in deterministic paths with \u001b[0m     \u001b[2m             \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32mpredictable execution and explicit control flow. This ensures consistent\u001b[0m \u001b[2m             \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32mresults, much like a factory assembly line where each station performs a\u001b[0m \u001b[2m             \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32mspecific, repeatable task.\\n\\n```mermaid\\ngraph TD\\n    A\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\"User Query\"\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \u001b[0m \u001b[2m             \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32m--> B\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\"Classify Query \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLM\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\"\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n    B --> C\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\"Retrieve Data \u001b[0m\u001b[32m(\u001b[0m\u001b[32mDatabase\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\"\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\u001b[0m \u001b[2m             \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32mC --> D\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\"Summarize Results \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLM\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\"\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n    D --> E\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\"Final \u001b[0m                  \u001b[2m             \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32mOutput\"\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n```\\nImage 1: A simple LLM workflow illustrating a sequence of\u001b[0m \u001b[2m             \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32mpredefined tasks, where an input leads to a tool-calling LLM, which \u001b[0m     \u001b[2m             \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32minteracts with tools before the workflow ends.\\n\\nOn the other end of \u001b[0m   \u001b[2m             \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32mthe spectrum, AI agents are systems where an LLM dynamically decides the\u001b[0m \u001b[2m             \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32msequence of steps, reasoning, and actions to achieve a goal. Steps are \u001b[0m  \u001b[2m             \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32mnot defined in advance but are planned based on the task and \u001b[0m            \u001b[2m             \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32menvironment. This makes them adaptive and capable of handling novelty, \u001b[0m  \u001b[2m             \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32mallowing them to adjust to new information and unexpected situations, \u001b[0m   \u001b[2m             \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32mmuch like a skilled human expert adapting their approach to an \u001b[0m          \u001b[2m             \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32munfamiliar problem.\\n\\n```mermaid\\ngraph TD\\n    A\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\"Start\"\u001b[0m\u001b[32m]\u001b[0m\u001b[32m --> \u001b[0m         \u001b[2m             \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32mB\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\"Agent: Initial Request/State\"\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n    B --> C\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\"Assistant: Analyze & \u001b[0m    \u001b[2m             \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32mPlan\"\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n    C --> D\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\"Decision: Tool Required?\"\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n    D -->|\"Yes\"| E\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\"Use\u001b[0m \u001b[2m             \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32mTool\"\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n    E --> F\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\"Agent: Process Tool Output\"\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n    F --> C\\n    D \u001b[0m   \u001b[2m             \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32m-->|\"No\"| G\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\"Decision: Task Completed?\"\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n    G -->|\"No\"| C\\n    G \u001b[0m      \u001b[2m             \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32m-->|\"Yes\"| H\u001b[0m\u001b[32m[\u001b[0m\u001b[32m\"End\"\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n```\\nImage 2: A simple agentic system illustrating \u001b[0m \u001b[2m             \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32mdynamic decision-making and tool usage with an assistant orchestrating \u001b[0m  \u001b[2m             \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32mthe loop.\\n\\nBoth workflows and agents require an orchestration layer, \u001b[0m  \u001b[2m             \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32mbut its role differs. In a workflow, the orchestrator executes a \u001b[0m        \u001b[2m             \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32mpredefined plan, like a conductor following a musical score. For an \u001b[0m     \u001b[2m             \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32magentic system, the orchestrator facilitates the LLM\\'s dynamic planning\u001b[0m \u001b[2m             \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32mand execution, acting more like a jazz ensemble leader who guides \u001b[0m       \u001b[2m             \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32mimprovisation. We will explore advanced workflow patterns and core agent\u001b[0m \u001b[2m             \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32mcomponents in future lessons.\\n\\n## Choosing Your Path</content>\\n    \u001b[0m   \u001b[2m             \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32m<first_line_number>11</first_line_number>\\n    \u001b[0m                          \u001b[2m             \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32m<last_line_number>44</last_line_number>\\n</selected_text\u001b[0m\u001b[32m>\u001b[0m\u001b[32m\\n\\n\\nHere is \u001b[0m  \u001b[2m             \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32mwhat you have to do with edited selected text:\\n- print the edited \u001b[0m      \u001b[2m             \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32mselected text to the console for the user to see\\n- give a quick summary\u001b[0m \u001b[2m             \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32mof the changes you made\\n- always apply the changes to the source file \u001b[0m  \u001b[2m             \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32mwithout waiting for the user to explicitly ask you to do so\\n- in case \u001b[0m  \u001b[2m             \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32myou haven\\'t changed anything, just say that you haven\\'t changed \u001b[0m       \u001b[2m             \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32manything\\n'\u001b[0m, \u001b[32m'extra'\u001b[0m: \u001b[3;35mNone\u001b[0m\u001b[1m}\u001b[0m                                              \u001b[2m             \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m------------------------------------ Edit Selected Text Result ------------------------------------\u001b[0m\n",
      "  \n",
      "Here is the edited selected text:\n",
      "\n",
      "<selected_text>\n",
      "    \n",
      "    <content>## Understanding the Spectrum: From Workflows to Agents\n",
      "\n",
      "To choose between workflows and agents, you need a clear understanding of what they are. Rather than focusing on the technical specifics, let's look at their core properties and how they function.\n",
      "\n",
      "An LLM workflow is a sequence of predefined tasks, largely orchestrated by developer-written code. Its steps are defined in advance, resulting in deterministic paths with predictable execution and explicit control flow. This ensures consistent results, much like a factory assembly line where each station performs a specific, repeatable task.\n",
      "\n",
      "```mermaid\n",
      "graph TD\n",
      "    A[\"User Query\"] --> B[\"Classify Query (LLM)\"]\n",
      "    B --> C[\"Retrieve Data (Database)\"]\n",
      "    C --> D[\"Summarize Results (LLM)\"]\n",
      "    D --> E[\"Final Output\"]\n",
      "```\n",
      "Image 1: A simple LLM workflow illustrating a sequence of predefined tasks, where an input leads to a tool-calling LLM, which interacts with tools before the workflow ends.\n",
      "\n",
      "On the other end of the spectrum, AI agents are systems where an LLM dynamically decides the sequence of steps, reasoning, and actions to achieve a goal. Steps are not defined in advance but are planned based on the task and environment. This makes them adaptive and capable of handling novelty, allowing them to adjust to new information and unexpected situations, much like a skilled human expert adapting their approach to an unfamiliar problem.\n",
      "\n",
      "```mermaid\n",
      "graph TD\n",
      "    A[\"Start\"] --> B[\"Agent: Initial Request/State\"]\n",
      "    B --> C{\"Assistant: Analyze & Plan\"}\n",
      "    C --> D{\"Decision: Tool Required?\"}\n",
      "    D -->|\"Yes\"| E[\"Use Tool\"]\n",
      "    E --> F[\"Agent: Process Tool Output\"]\n",
      "    F --> C\n",
      "    D -->|\"No\"| G{\"Decision: Task Completed?\"}\n",
      "    G -->|\"No\"| C\n",
      "    G -->|\"Yes\"| H[\"End\"]\n",
      "```\n",
      "Image 2: A simple agentic system illustrating dynamic decision-making and tool usage with an assistant orchestrating the loop.\n",
      "\n",
      "Both workflows and agents require an orchestration layer, but its role differs. In a workflow, the orchestrator executes a predefined plan, like a conductor following a musical score. For an agentic system, the orchestrator facilitates the LLM's dynamic planning and execution, acting more like a jazz ensemble leader who guides improvisation. We will explore advanced workflow patterns and core agent components in future lessons.\n",
      "\n",
      "## Choosing Your Path</content>\n",
      "    <first_line_number>11</first_line_number>\n",
      "    <last_line_number>44</last_line_number>\n",
      "</selected_text>\n",
      "\n",
      "\n",
      "Here is what you have to do with edited selected text:\n",
      "- print the edited selected text to the console for the user to see\n",
      "- give a quick summary of the changes you made\n",
      "- always apply the changes to the source file without waiting for the user to explicitly ask you to do so\n",
      "- in case you haven't changed anything, just say that you haven't changed anything\n",
      "\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "async with mcp_client:\n",
    "    result = await mcp_client.call_tool(\n",
    "        \"edit_selected_text\",\n",
    "        {\n",
    "            \"article_path\": str(SAMPLE_DIR / \"article.md\"),\n",
    "            \"human_feedback\": \"Expand on the definition of both workflows and agents. Improve the first diagram.\",\n",
    "            \"selected_text\": selected_text,\n",
    "            \"first_line_number\": start_line,\n",
    "            \"last_line_number\": end_line,\n",
    "        },\n",
    "    )\n",
    "    pretty_print.wrapped(\n",
    "        result.data,\n",
    "        title=\"Edit Selected Text Result\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âš ï¸ **Important Observation**\n",
    "\n",
    "Now the question is how do we apply these changes to the original article?\n",
    "\n",
    "Note the text after the `selected_text` XML block we have the following instructions:\n",
    "```text\n",
    "Here is what you have to do with edited selected text:\n",
    "- print the edited selected text to the console for the user to see\n",
    "- give a quick summary of the changes you made\n",
    "- always apply the changes to the source file without waiting for the user to explicitly ask you to do so\n",
    "- in case you haven't changed anything, just say that you haven't changed anything\n",
    "```\n",
    "\n",
    "This instructs the tool consumer, which is the MCP client, such as Cursor, what to do with the new version of the selected text. In this use case, using these instructions, we instruct the MCP Client to always apply these changes to the original article. Otherwise, the edited selected text will be showed within the tool output, but never applied as a patch to the original article.\n",
    "\n",
    "When using this with tools such as Cursor, it will show you the `diff` experience where you have to manually accept the new changes, improving even more the whole human in the loop experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 MCP Prompts\n",
    "\n",
    "MCP prompts provide pre-configured templates that MCP clients can use to easily trigger a tool or a composition of tools (remember the example from Nova the deep research agent). Brown exposes three prompts corresponding to the three tools as an easy way to interface with its functionality:\n",
    "\n",
    "\n",
    "```python\n",
    "@mcp.prompt\n",
    "def generate_article_prompt(dir_path: Path) -> str:\n",
    "    \"\"\"Retrieve a prompt that will trigger the article generation workflow.\"\"\"\n",
    "    return f\"\"\"\n",
    "Using Brown hosted as an MCP server, generate an article using all the necessary resources from \n",
    "the following directory: `{dir_path}`. Don't check if any expected files are missing, just trigger \n",
    "the \"generate_article\" tool of the Brown MCP Server, which will take care of everything.\n",
    "\"\"\"\n",
    "\n",
    "@mcp.prompt\n",
    "def edit_article_prompt(human_feedback: str = \"\") -> str:\n",
    "    \"\"\"Retrieve a prompt that will trigger the article editing workflow.\"\"\"\n",
    "    return f\"\"\"\n",
    "Using Brown hosted as an MCP server, edit an entire article based on human feedback \n",
    "and other expected requirements. Don't check if any expected files are missing, \n",
    "just trigger the \"edit_article\" tool of the Brown MCP Server, which will take care \n",
    "of everything.\n",
    "\n",
    "Human feedback:\n",
    "<human_feedback>\n",
    "{human_feedback}\n",
    "</human_feedback>\n",
    "\n",
    "If the <human_feedback> is empty, you will infer it from the previous messages. If there are no other messages \n",
    "to infer from, use an empty string. Don't ever fill it in with things such as \"Please provide more details\" or\n",
    "fill it in with generic stuff.\n",
    "\"\"\"\n",
    "\n",
    "@mcp.prompt\n",
    "def edit_selected_text_prompt(human_feedback: str = \"\") -> str:\n",
    "    \"\"\"Retrieve a prompt that will trigger the selected text editing workflow.\"\"\"\n",
    "    return f\"\"\"\n",
    "Using Brown hosted as an MCP server, edit the selected text from the article based on human feedback and \n",
    "other expected requirements. Don't check if any expected files are missing, just trigger the \"edit_selected_text\" \n",
    "tool of the Brown MCP Server, which will take care of everything.\n",
    "\n",
    "Human feedback:\n",
    "<human_feedback>\n",
    "{human_feedback}\n",
    "</human_feedback>\n",
    "\n",
    "If the <human_feedback> is empty, you will infer it from the previous messages. If there are no other messages \n",
    "to infer from, use an empty string. Don't ever fill it in with things such as \"Please provide more details\" or\n",
    "fill it in with generic stuff.\n",
    "\"\"\"\n",
    "```\n",
    "These prompts make it easy for users to trigger the MCP's server tool without reading any documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see how we can access the prompts, starting with the one for generating articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m------------------------------------------ `user` Message ------------------------------------------\u001b[0m\n",
      "  Role: user\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "  Content: \n",
      "Using Brown hosted as an MCP server, generate an article using all the necessary resources from \n",
      "the following directory: `inputs/tests/01_sample_small`. Don't check if any expected files are missing, just trigger \n",
      "the \"generate_article\" tool of the Brown MCP Server, which will take care of everything.\n",
      "\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "async with mcp_client:\n",
    "    result = await mcp_client.get_prompt(\n",
    "        \"generate_article_prompt\",\n",
    "        {\n",
    "            \"dir_path\": SAMPLE_DIR,\n",
    "        },\n",
    "    )\n",
    "    for message in result.messages:\n",
    "        pretty_print.wrapped(\n",
    "            [\n",
    "                f\"Role: {message.role}\",\n",
    "                f\"Content: {str(message.content.text)}\",\n",
    "            ],\n",
    "            title=f\"`{message.role}` Message\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look how the one for editing a piece of selected text looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m------------------------------------------ `user` Message ------------------------------------------\u001b[0m\n",
      "  Role: user\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "  Content: \n",
      "Using Brown hosted as an MCP server, edit the selected text from the article based on human feedback and \n",
      "other expected requirements. Don't check if any expected files are missing, just trigger the \"edit_selected_text\" \n",
      "tool of the Brown MCP Server, which will take care of everything.\n",
      "\n",
      "Human feedback:\n",
      "<human_feedback>\n",
      "Improve the first mermaid diagram. Also, expand on the definition of \n",
      "both workflows and agents as I feel it's too shallow\n",
      "</human_feedback>\n",
      "\n",
      "If the <human_feedback> is empty, you will infer it from the previous messages. If there are no other messages \n",
      "to infer from, use an empty string. Don't ever fill it in with things such as \"Please provide more details\" or\n",
      "fill it in with generic stuff.\n",
      "\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "async with mcp_client:\n",
    "    result = await mcp_client.get_prompt(\n",
    "        \"edit_selected_text_prompt\",\n",
    "        {\n",
    "            \"human_feedback\": human_feedback,\n",
    "        },\n",
    "    )\n",
    "    for message in result.messages:\n",
    "        pretty_print.wrapped(\n",
    "            [\n",
    "                f\"Role: {message.role}\",\n",
    "                f\"Content: {str(message.content.text)}\",\n",
    "            ],\n",
    "            title=f\"`{message.role}` Message\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `edit_article_prompt` is almost identical to `edit_selected_text_prompt`. Thus, we will skip it.\n",
    "\n",
    "Still, the real beauty of using prompts through MCP Servers is when we actually plug in the server into a client such as Cursor that has access to a chatbot that can pick up on the instructions from the retrieved prompts and follows the instructions from it such as calling the generate article tool.\n",
    "\n",
    "This makes prompts an amazing way to interface with MCP Server, for example describing workflows that use the MCP Server tools in various ways, and passing them directly to the client, without letting the consumer worry about how to use them.\n",
    "\n",
    "We will illustrate this in the video attached to this lesson where we will show how this works in Cursor, but a similar behavior applies in other MCP clients, such as Claude."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 MCP Resources\n",
    "\n",
    "MCP resources provide read-only access to configuration and state information. Brown exposes resources for accessing the app configuration and profiles:\n",
    "```python\n",
    "@mcp.resource(\"resource://config/app\", mime_type=\"application/json\")\n",
    "def get_app_config() -> dict:\n",
    "    \"\"\"Get the application configuration for Brown Agent as an MCP resource.\n",
    "    \n",
    "    Returns:\n",
    "        dict: The application configuration including:\n",
    "            - Model configurations for each workflow node\n",
    "            - File paths for profiles, examples, and context files\n",
    "            - Number of review iterations and workflow settings\n",
    "            - Temperature and other model parameters\n",
    "    \"\"\"\n",
    "    return app_config.model_dump(mode=\"json\")\n",
    "\n",
    "@mcp.resource(\"resource://profiles/character\")\n",
    "def get_character_profile() -> str:\n",
    "    \"\"\"Get the character profile resource for Brown Agent.\n",
    "    \n",
    "    Returns:\n",
    "        str: The character profile content in markdown format\n",
    "    \"\"\"\n",
    "    return __get_profile(\"character\")\n",
    "```\n",
    "\n",
    "Resources allow MCP clients to understand how Brown is configured without modifying any state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's call the resources to see the MCP server configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m------------------------------------ `get_app_config` Resource ------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"uri\": \"resource://config/app\",\n",
      "  \"name\": \"get_app_config\",\n",
      "  \"description\": \"Get the application configuration for Brown Agent as an MCP resource.\\n\\nThis resource provides access to the complete Brown Agent configuration,\\nincluding model settings, file paths, and workflow parameters. The configuration\\nis loaded from YAML files and converted to a JSON-serializable format.\\n\\nReturns:\\n    dict: The application configuration as a JSON-serializable dictionary containing:\\n        - Model configurations for each workflow node (write_article, review_article, etc.)\\n        - File paths for profiles, examples, and context files\\n        - Number of review iterations and workflow settings\\n        - Temperature and other model parameters\\n        - Tool configurations and model assignments\",\n",
      "  \"mime_type\": \"application/json\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[93m--------------------------------- `get_character_profile` Resource ---------------------------------\u001b[0m\n",
      "  {\n",
      "  \"uri\": \"resource://profiles/character\",\n",
      "  \"name\": \"get_character_profile\",\n",
      "  \"description\": \"Get the character profile resource for Brown Agent as an MCP resource.\\n\\nThis resource provides access to the character-specific writing persona\\nthat the Brown Agent uses for consistent article generation and editing.\\nThe character profile defines the voice, perspective, and personal style\\nthat should be reflected in the generated content.\\n\\nThe profile is loaded using the same builders pattern as the workflows,\\nensuring consistency across the Brown Agent system.\\n\\nReturns:\\n    str: The character profile content in markdown format, loaded from\\n         the configured character profile file.\\n\\nRaises:\\n    ValueError: If the character profile cannot be loaded or is not found.\\n\\nExample:\\n    The character profile typically includes information about:\\n    - Writing voice and tone preferences\\n    - Personal background and expertise areas\\n    - Preferred terminology and expressions\\n    - Communication style and approach\",\n",
      "  \"mime_type\": \"text/plain\"\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "async with mcp_client:\n",
    "    resources = await mcp_client.list_resources()\n",
    "\n",
    "    for resource in resources:\n",
    "        resource_dict = {\n",
    "            \"uri\": str(resource.uri),\n",
    "            \"name\": resource.name,\n",
    "            \"description\": resource.description,\n",
    "            \"mime_type\": resource.mimeType,\n",
    "        }\n",
    "        if hasattr(resource, \"_meta\") and resource._meta:\n",
    "            fastmcp_meta = resource._meta.get(\"_fastmcp\", {})\n",
    "            resource_dict[\"tags\"] = fastmcp_meta.get(\"tags\", [])\n",
    "\n",
    "        pretty_print.wrapped(\n",
    "            resource_dict,\n",
    "            title=f\"`{resource.name}` Resource\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look into each resource independently starting with the app configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m---------------------------------------- App Configuration ----------------------------------------\u001b[0m\n",
      "  {\n",
      "  \"context\": {\n",
      "    \"article_guideline_loader\": \"markdown\",\n",
      "    \"article_guideline_uri\": \"article_guideline.md\",\n",
      "    \"research_loader\": \"markdown\",\n",
      "    \"research_uri\": \"research.md\",\n",
      "    \"article_loader\": \"markdown\",\n",
      "    \"article_renderer\": \"markdown\",\n",
      "    \"article_uri\": \"article.md\",\n",
      "    \"profiles_loader\": \"markdown\",\n",
      "    \"profiles_uri\": \"inputs/profiles\",\n",
      "    \"character_profile\": \"paul_iusztin.md\",\n",
      "    \"examples_loader\": \"markdown\",\n",
      "    \"examples_uri\": \"inputs/examples/course_lessons\"\n",
      "  },\n",
      "  \"memory\": {\n",
      "    \"checkpointer\": \"in_memory\"\n",
      "  },\n",
      "  \"num_reviews\": 2,\n",
      "  \"nodes\": {\n",
      "    \"generate_media_items\": {\n",
      "      \"model_id\": \"google_genai:gemini-2.5-flash\",\n",
      "      \"config\": {\n",
      "        \"temperature\": 0.0,\n",
      "        \"top_k\": null,\n",
      "        \"n\": 1,\n",
      "        \"response_modalities\": null,\n",
      "        \"include_thoughts\": false,\n",
      "        \"thinking_budget\": null,\n",
      "        \"max_output_tokens\": null,\n",
      "        \"max_retries\": 6,\n",
      "        \"mocked_response\": null\n",
      "      },\n",
      "      \"tools\": {\n",
      "        \"mermaid_diagram_generator\": {\n",
      "          \"name\": \"mermaid_diagram_generator\",\n",
      "          \"model_id\": \"google_genai:gemini-2.5-flash\",\n",
      "          \"config\": {\n",
      "            \"temperature\": 0.0,\n",
      "            \"top_k\": null,\n",
      "            \"n\": 1,\n",
      "            \"response_modalities\": null,\n",
      "            \"include_thoughts\": false,\n",
      "            \"thinking_budget\": null,\n",
      "            \"max_output_tokens\": null,\n",
      "            \"max_retries\": 6,\n",
      "            \"mocked_response\": null\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    },\n",
      "    \"write_article\": {\n",
      "      \"model_id\": \"google_genai:gemini-2.5-flash\",\n",
      "      \"config\": {\n",
      "        \"temperature\": 0.7,\n",
      "        \"top_k\": null,\n",
      "        \"n\": 1,\n",
      "        \"response_modalities\": null,\n",
      "        \"include_thoughts\": false,\n",
      "        \"thinking_budget\": null,\n",
      "        \"max_output_tokens\": null,\n",
      "        \"max_retries\": 6,\n",
      "        \"mocked_response\": null\n",
      "      },\n",
      "      \"tools\": {}\n",
      "    },\n",
      "    \"review_article\": {\n",
      "      \"model_id\": \"google_genai:gemini-2.5-flash\",\n",
      "      \"config\": {\n",
      "        \"temperature\": 0.0,\n",
      "        \"top_k\": null,\n",
      "        \"n\": 1,\n",
      "        \"response_modalities\": null,\n",
      "        \"include_thoughts\": false,\n",
      "        \"thinking_budget\": null,\n",
      "        \"max_output_tokens\": null,\n",
      "        \"max_retries\": 6,\n",
      "        \"mocked_response\": null\n",
      "      },\n",
      "      \"tools\": {}\n",
      "    },\n",
      "    \"edit_article\": {\n",
      "      \"model_id\": \"google_genai:gemini-2.5-flash\",\n",
      "      \"config\": {\n",
      "        \"temperature\": 0.1,\n",
      "        \"top_k\": null,\n",
      "        \"n\": 1,\n",
      "        \"response_modalities\": null,\n",
      "        \"include_thoughts\": false,\n",
      "        \"thinking_budget\": null,\n",
      "        \"max_output_tokens\": null,\n",
      "        \"max_retries\": 6,\n",
      "        \"mocked_response\": null\n",
      "      },\n",
      "      \"tools\": {}\n",
      "    },\n",
      "    \"review_selected_text\": {\n",
      "      \"model_id\": \"google_genai:gemini-2.5-flash\",\n",
      "      \"config\": {\n",
      "        \"temperature\": 0.0,\n",
      "        \"top_k\": null,\n",
      "        \"n\": 1,\n",
      "        \"response_modalities\": null,\n",
      "        \"include_thoughts\": false,\n",
      "        \"thinking_budget\": null,\n",
      "        \"max_output_tokens\": null,\n",
      "        \"max_retries\": 6,\n",
      "        \"mocked_response\": null\n",
      "      },\n",
      "      \"tools\": {}\n",
      "    },\n",
      "    \"edit_selected_text\": {\n",
      "      \"model_id\": \"google_genai:gemini-2.5-flash\",\n",
      "      \"config\": {\n",
      "        \"temperature\": 0.1,\n",
      "        \"top_k\": null,\n",
      "        \"n\": 1,\n",
      "        \"response_modalities\": null,\n",
      "        \"include_thoughts\": false,\n",
      "        \"thinking_budget\": null,\n",
      "        \"max_output_tokens\": null,\n",
      "        \"max_retries\": 6,\n",
      "        \"mocked_response\": null\n",
      "      },\n",
      "      \"tools\": {}\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "async with mcp_client:\n",
    "    content = await mcp_client.read_resource(\"resource://config/app\")\n",
    "\n",
    "    pretty_print.wrapped(\n",
    "        json.loads(content[0].text),\n",
    "        title=\"App Configuration\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the character profile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m---------------------------------------- Character Profile ----------------------------------------\u001b[0m\n",
      "  ## About Paul Iusztin\n",
      "\n",
      "Iâ€™m **Paul Iusztin**. A 29 years old senior AI Engineer and content creator.\n",
      "\n",
      "I help engineers ship AI products.\n",
      "\n",
      "Iâ€™m the author of the bestseller LLM Engineerâ€™s Handbook, lead instructor of the Agentic AI Engineering course, founding AI Engineer of a San Francisco start-up, and obsessed with making knowledge accessible through AI.\n",
      "\n",
      "With over 10 years of experience and 20 apps shipped, I teach AI Engineering as I wanted to at the beginning of my career. End-to-end. From idea to production. From data collection to deploying, monitoring and evaluation. With a focus on AI principles, software patterns and infrastructure systems that will thrive in a future dominated by AI coding tools.\n",
      "\n",
      "My ultimate goal is to help other engineers escape the PoC purgatory and x10 their AI Engineering skills.\n",
      "\n",
      "I'm also the founder of the **Decoding AI Magazine**, a place for real-world guides takingÂ you from the PoC purgatoryÂ to shipping AI products that work.\n",
      "\n",
      "I founded this magazine to solve the problem I faced for the first five years of my career: escaping the â€œPoC purgatory.â€ I realized that finding a team that knows how to ship AI software is rare. Too many AI projects get stuck at Jupyter Notebooks or fancy demos that never see a real user.\n",
      "\n",
      "Decoding AI is the solution. Itâ€™s your weekly hub for learning how to design, build, and ship production-grade AI systems. End-to-end. From idea to production. From data collection to deploying, monitoring and evaluation. With a focus on AI principles, software patterns and infrastructure systems that will thrive in a future dominated by AI coding tools.\n",
      "\n",
      "Stop building prototypes. Start shipping AI that works.\n",
      "\n",
      "## Niche\n",
      "\n",
      "**Broad:** AI Engineering, AI Systems, Software Engineering, Ops\n",
      "**Specialized**: RAG, LLMs, AI Agents, AI Workflows, LLMOps, AI Evals, AI Monitoring, AI Infrastructure, Serving AI Applications, Memory\n",
      "**Vibe:** premium, minimalistic, high-quality content, straight-to-the-point content (the opposite of FOMO), zero hype, artsy, builder mindset, deep and real\n",
      "\n",
      "## Similar Personas\n",
      "\n",
      "- Andrew Ng\n",
      "- Chip Huyen\n",
      "- Sebastian Raschka\n",
      "- Louis-FranÃ§ois Bouchard\n",
      "- Maxime Labonne\n",
      "- Jason Liu\n",
      "- Lex Fridman\n",
      "- Aleksa Gordic\n",
      "\n",
      "## My Core Content Pillars I Always Talk About\n",
      "\n",
      "1. Shipping Production-Ready AI\n",
      "2. Practical Learning for AI Engineers\n",
      "3. Behind-the-Scenes of Building in Public\n",
      "4. AI for Founders & Technical Teams \n",
      "\n",
      "## Style\n",
      "\n",
      "- **Real:** Showing the engineering, social, and economic truth behind AI and content creation\n",
      "- **Trust:** No FOMO, just real engineering focusing on what matters\n",
      "- **Minimalist:** Straight to the point engineering. Use only what is required to get shit done.\n",
      "- **Simple:** â€œKeep it simpleâ€ should be the motto of your persona. Focus on what matters while avoiding FOMO and fluff just to get attention\n",
      "- **Controversy:**\n",
      "    - Talk about what everybody thinks, but they're too afraid to say out loud.\n",
      "    - Going against the flow, Gary Marcus style: \"LLMs are stupid,\" \"Agents are stupid,â€ \"LangChain is stupid\".\n",
      "\n",
      "\u001b[93m----------------------------------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "async with mcp_client:\n",
    "    content = await mcp_client.read_resource(\"resource://profiles/character\")\n",
    "\n",
    "    pretty_print.wrapped(\n",
    "        content[0].text,\n",
    "        title=\"Character Profile\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Clean Separation of MCP Layer\n",
    "\n",
    "Notice how all the MCP code lives in the `brown.mcp` module and simply imports from other layers:\n",
    "\n",
    "- **Domain layer**: Imports entities like `Article`, `HumanFeedback`, `Review`\n",
    "- **App layer**: Imports workflows like `build_edit_article_workflow`, `build_edit_selected_text_workflow`\n",
    "- **Infrastructure layer**: Imports builders like `build_loaders`, `build_short_term_memory`\n",
    "\n",
    "The MCP layer is just a thin serving/interface layer that orchestrates the other components. This separation means:\n",
    "\n",
    "- We can serve Brown through different methods (CLI, FastAPI, gRPC) without changing core logic\n",
    "- The MCP code is independent and can be modified without affecting other layers\n",
    "- We can easily add new MCP tools, prompts, or resources\n",
    "- Testing is easier because each layer has clear boundaries\n",
    "\n",
    "This is a key advantage of using clean architecture principles throughout the Brown project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Brown MCP CLI Script\n",
    "\n",
    "Brown includes a command-line interface that lets you interact with the MCP server directly from your terminal. This is useful for scripting, automation, and quick testing.\n",
    "\n",
    "The CLI script is located at `lessons/writing_workflow/scripts/brown_mcp_cli.py` and provides three main commands:\n",
    "\n",
    "1. **generate-article**: Generate an article from scratch\n",
    "2. **edit-article**: Edit an entire article based on human feedback\n",
    "3. **edit-selected-text**: Edit a selected section of an article\n",
    "\n",
    "âš ï¸ You can run these only from the Brown standalone project from `lessons/writing_workflow`.\n",
    "\n",
    "### Usage Examples\n",
    "\n",
    "Here is how the CLI commands look like:\n",
    "```bash\n",
    "# Generate an article\n",
    "python scripts/brown_mcp_cli.py generate-article --dir-path /path/to/article\n",
    "\n",
    "# Edit an entire article\n",
    "python scripts/brown_mcp_cli.py edit-article \\\n",
    "    --dir-path /path/to/article \\\n",
    "    --human-feedback \"Improve the introduction\"\n",
    "\n",
    "# Edit selected text\n",
    "python scripts/brown_mcp_cli.py edit-selected-text \\\n",
    "    --dir-path /path/to/article \\\n",
    "    --human-feedback \"Make this clearer\" \\\n",
    "    --first-line 10 \\\n",
    "    --last-line 20\n",
    "```\n",
    "\n",
    "The CLI uses the in-memory MCP client to call the MCP server tools, providing a simple interface for Brown's capabilities.\n",
    "\n",
    "### How to run it?\n",
    "\n",
    "> [!NOTE]\n",
    "> ðŸ’¡ For more details on running these CLI commands while using Brown as a standalone project, see the documentation at `lessons/writing_workflow/README.md`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Cursor Integration\n",
    "\n",
    "The real power of Brown comes when integrating it with MCP clients like Cursor. This creates a writing experience similar to coding with AI assistance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Cursor Configuration\n",
    "\n",
    "To use Brown with Cursor, you need to configure it in your `.cursor/mcp.json` file (similar to what we did for Nova, the research agent).\n",
    "\n",
    "Open up this repository with Cursor and add the following config within your `.cursor/mcp.json` file. Create it, if it doesn't exists:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"mcpServers\": {\n",
    "        \"brown\": {\n",
    "            \"command\": \"uv\",\n",
    "            \"args\": [\n",
    "                \"--directory\",\n",
    "                \"lessons/writing_workflow/\",\n",
    "                \"run\",\n",
    "                \"python\",\n",
    "                \"-m\",\n",
    "                \"brown.mcp.server\"\n",
    "            ],\n",
    "            \"cwd\": \"${workspaceFolder}\",\n",
    "            \"env\": {\n",
    "                \"ENV_FILE_PATH\": \"${workspaceFolder}/.env\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "...or if you already have other `mcpServers` added like the deep research agent, add `brown` along them, like this:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"mcpServers\": {\n",
    "        \"nova\": {\n",
    "            \"command\": \"uv\",\n",
    "            \"args\": [\n",
    "                \"--directory\",\n",
    "                \"lessons/research_agent_part_2/mcp_server\",\n",
    "                \"run\",\n",
    "                \"-m\",\n",
    "                \"src.server\",\n",
    "                \"--transport\",\n",
    "                \"stdio\"\n",
    "            ],\n",
    "            \"cwd\": \"${workspaceFolder}\",\n",
    "            \"env\": {\n",
    "                \"ENV_FILE_PATH\": \"${workspaceFolder}/.env\"\n",
    "            }\n",
    "        },\n",
    "        \"brown\": {\n",
    "            \"command\": \"uv\",\n",
    "            \"args\": [\n",
    "                \"--directory\",\n",
    "                \"lessons/writing_workflow/\",\n",
    "                \"run\",\n",
    "                \"python\",\n",
    "                \"-m\",\n",
    "                \"brown.mcp.server\"\n",
    "            ],\n",
    "            \"cwd\": \"${workspaceFolder}\",\n",
    "            \"env\": {\n",
    "                \"ENV_FILE_PATH\": \"${workspaceFolder}/.env\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "This configuration tells Cursor:\n",
    "\n",
    "- The name of the MCP server (\"brown\", \"nova\")\n",
    "- How to launch the server using `uv` and Python\n",
    "- The working directory\n",
    "- Where to load the environment variables. Ensure your `.env` file is located at `${workspaceFolder}/.env`, where `${workspaceFolder}` is the root of the project.\n",
    "\n",
    "Once configured, go to Cursor's settings `Tools & MCP` section and ensure it has discovered `brown` (with a green light) as an MCP server with tools, prompts and resources available.\n",
    "\n",
    "More on how to use Brown with Cursor in the video from the lesson!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 The Brown + Human-in-the-Loop Writing Experience\n",
    "\n",
    "With Brown integrated into Cursor, your writing workflow becomes:\n",
    "\n",
    "1. **Generate Initial Article**: Use the `generate_article` tool or prompt to create a first draft\n",
    "2. **Review as Human Expert**: Read through the generated article with your domain expertise\n",
    "3. **Provide Feedback**: Select sections that need improvement and provide specific feedback\n",
    "4. **AI Edits**: Use `edit_article` or `edit_selected_text` tools to refine based on your feedback\n",
    "5. **Iterate**: Repeat steps 2-4 until satisfied with the article\n",
    "\n",
    "This creates a collaborative experience where:\n",
    "- AI handles the heavy lifting of writing and editing\n",
    "- You guide the direction with your expertise and feedback\n",
    "- The workflow is fast and iterative\n",
    "- You maintain full control over the final output\n",
    "\n",
    "### Video Demonstration\n",
    "\n",
    "We've created a video demonstration showing Brown in action with Cursor. The video shows:\n",
    "- How to set up Brown in Cursor\n",
    "- Generating an article from research and guidelines\n",
    "- Reviewing the generated content\n",
    "- Using human-in-the-loop editing to refine specific sections\n",
    "- The iterative refinement process until the article is complete\n",
    "\n",
    "[Link to video demonstration will be provided]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusion\n",
    "\n",
    "In this lesson, we've explored how to implement human-in-the-loop capabilities in the Brown writing workflow. Let's recap what we've learned and look at future possibilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What We've Learned\n",
    "\n",
    "1. **Human-in-the-Loop**: We learned how to design AI workflows that balance automation with human expertise, allowing AI to handle heavy lifting while keeping humans in control\n",
    "\n",
    "2. **Edit Article Workflow**: We implemented the `edit_article` workflow that reviews and edits entire articles based on human feedback and expected requirements\n",
    "\n",
    "3. **Edit Selected Text Workflow**: We built the `edit_selected_text` workflow for precise, focused edits on specific article sections\n",
    "\n",
    "4. **MCP Server**: We served Brown as an MCP server with tools, prompts, and resources while respecting clean architecture principles\n",
    "\n",
    "5. **Cursor Integration**: We learned how to integrate Brown with Cursor to create a coding-like writing experience with human-in-the-loop editing\n",
    "\n",
    "The key insight is that we can use a low number of review loops during initial article generation, then dynamically run additional editing workflows with human feedback when necessary. This approach is both efficient and effective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future Extension Ideas\n",
    "\n",
    "Here are some ways you can extend Brown to fit your needs:\n",
    "\n",
    "1. **Hook Brown to Claude Desktop**: Instead of using Cursor, integrate Brown with Claude Desktop for a different AI assistant experience\n",
    "\n",
    "2. **Use Resource Templates** to parameterize the writing profiles and easily add support for all the available profiles. [More here](https://gofastmcp.com/servers/resources#resource-templates).\n",
    "\n",
    "3. **Serve Brown through FastAPI**: Replace the MCP server with a FastAPI REST API for web-based integrations\n",
    "\n",
    "4. **Add Guideline Review Tool**: Create a workflow to review and edit your article guidelines themselves, helping you refine your inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapping Up Brown\n",
    "\n",
    "With this lesson, we've wrapped up our exploration of the Brown writing workflow. We've covered:\n",
    "\n",
    "- **Lesson 22**: Foundation of the writing workflow with entities, nodes, and basic workflows\n",
    "- **Lesson 23**: The evaluator-optimizer pattern with article reviews and iterative refinement\n",
    "- **Lesson 24**: Human-in-the-loop implementation with MCP server integration\n",
    "\n",
    "Together, these lessons show how to build a production-ready AI writing system that:\n",
    "- Generates high-quality articles from research and guidelines\n",
    "- Uses multiple review loops to ensure quality\n",
    "- Integrates human feedback for iterative refinement\n",
    "- Exposes capabilities through standard protocols like MCP\n",
    "- Maintains clean architecture for extensibility and maintainability\n",
    "\n",
    "In future lessons, we'll explore how to orchestrate Nova and Brown as a multi-agent system. \n",
    "\n",
    "### Resources\n",
    "\n",
    "- **Brown Package**: Explore `lessons/writing_workflow`\n",
    "- **Configuration Examples**: Check `configs/` for different configurations\n",
    "- **Test Data**: Use `inputs/tests/` for additional testing scenarios\n",
    "- **Writing Profiles**: Check `inputs/profiles/` for more profile templates"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
