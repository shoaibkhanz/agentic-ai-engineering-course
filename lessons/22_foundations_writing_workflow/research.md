# Research

## Important IMAGE URLS

[Prompt Anatomy](https://substackcdn.com/image/fetch/$s_!PWPl!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F82059d64-50f7-4788-ba38-d2a340e9f9f3_3840x1948.png)
[Lesson 22 Writing Workflow Architecture](https://github.com/iusztinpaul/agentic-ai-engineering-course-data/blob/main/images/l22_writing_workflow.png)

## Research Results

<details>
<summary>What are best practices for converting complex Python objects, like Pydantic models, into structured, token-efficient context for large language models?</summary>

### Source [1]: https://pydantic.dev/articles/llm-intro

Query: What are best practices for converting complex Python objects, like Pydantic models, into structured, token-efficient context for large language models?

Answer: Best practices for converting complex Python objects like Pydantic models into structured, token-efficient context for large language models (LLMs) involve leveraging Pydantic's data validation and serialization features combined with prompt engineering. Pydantic models define the expected structure and data types of the output using Python type annotations, which enables precise schema enforcement. When querying an LLM, you can instruct it to output data conforming to these models by specifying the response format in JSON matching the Pydantic schema. This approach helps ensure your outputs are both syntactically and semantically valid, reducing parsing errors.

Using Pydantic with OpenAI's GPT models, for example, you can send prompts asking for data structured to your Pydantic models and then validate the LLM's JSON response with Pydantic's `model_validate_json` method. This guarantees strong typing and reliable deserialization of the complex data. Additional tools like the "instructor" library can be used to patch OpenAI completions and simplify the extraction of structured data from unstructured text responses, improving output quality and reliability.

Key points include:
- Define clear Pydantic models with type annotations for expected data.
- Use these models to validate and parse LLM JSON outputs.
- Incorporate prompt instructions that specify the desired output structure.
- Use helper libraries (e.g., instructor) to streamline integration with LLM APIs.
- Validate outputs at runtime to catch inconsistencies early.
- Serialize models back to JSON/dict for token-efficient context embedding and downstream usage.

This methodology enables converting complex Python objects into structured, token-efficient context consumable by LLMs while maintaining robust validation and type safety[3].

-----

-----

### Source [2]: https://developer-service.blog/a-practical-guide-on-structuring-llm-outputs-with-pydantic/

Query: What are best practices for converting complex Python objects, like Pydantic models, into structured, token-efficient context for large language models?

Answer: The best practices for converting complex Python objects like Pydantic models into structured, token-efficient context for LLMs focus on using Pydantic for runtime data validation and type-safe schema definitions. Pydantic allows defining models with Python type annotations that describe complex data shapes clearly and concisely. This enables not only validation of outputs from language models but also serialization back to JSON or dictionaries suitable for efficient context construction.

Key practices include:
- Leveraging Pydantic models to define the exact schema expected from the LLM output.
- Designing prompts that instruct the LLM to generate output in JSON format matching the Pydantic schema.
- Using APIs or helper methods that accept Pydantic models as response formats, e.g., Mistral AI's `chat.parse` method, which can automatically parse and validate responses against the model.
- Handling typical LLM output issues such as missing fields, malformed formats, or wrong data types by enforcing strict validation rules with Pydantic.
- Structuring data efficiently to optimize token usage by restricting output strictly to necessary fields and types.

By combining Pydantic’s static typing and runtime validation with carefully designed prompts and API integrations, developers can reliably convert unstructured or semi-structured LLM outputs into well-structured, token-efficient context representations suitable for downstream consumption and further processing[2][5].

-----

-----

### Source [3]: https://www.leocon.dev/blog/2024/11/from-chaos-to-control-mastering-llm-outputs-with-langchain-and-pydantic/

Query: What are best practices for converting complex Python objects, like Pydantic models, into structured, token-efficient context for large language models?

Answer: Best practices for converting complex Python objects such as Pydantic models into structured, token-efficient context for large language models include a multi-step approach involving model definition, schema generation, prompt integration, and validation.

1. **Model Definition**: Start by defining clear Pydantic models that describe the exact data structure and types you want the LLM to output. This provides a strong contract and type safety.

2. **Schema Generation**: Use tools (e.g., LangChain’s Pydantic parser) to automatically generate JSON schemas from these models. These schemas serve as precise format instructions that can be injected into prompts.

3. **Prompt Integration**: Insert the JSON schema or detailed format instructions into the prompt template (using placeholders like `{format_instructions}`). This guides the LLM to produce outputs conforming exactly to your Pydantic models, reducing ambiguity and increasing consistency.

4. **Validation and Error Handling**: After receiving the LLM output, parse it with the Pydantic model to automatically validate types, detect missing fields, and handle nested structures. Pydantic provides clear error messages if the output deviates from the schema.

5. **Enhancements with Examples**: Including examples and extra schema metadata improves LLM understanding, reducing errors and improving first-time success rates.

6. **Token Efficiency**: By tightly specifying the required fields and their formats, you reduce unnecessary verbosity in the LLM output, thus optimizing token usage.

7. **Integration Ready**: Pydantic models integrate seamlessly with Python frameworks like FastAPI and ORMs such as SQLAlchemy, enabling easy storage and downstream processing.

This structured approach, particularly when combined with prompt engineering frameworks like LangChain, turns unpredictable free-text LLM outputs into reliable, strongly-typed Python objects suitable for production contexts, enhancing both reliability and token efficiency[4].

-----

-----

### Source [4]: https://www.youtube.com/watch?v=QYW3ETY7UpA

Query: What are best practices for converting complex Python objects, like Pydantic models, into structured, token-efficient context for large language models?

Answer: This source demonstrates a practical workflow for converting unstructured data into structured data using LLMs and Pydantic models. The process involves:

- Importing OpenAI and Pydantic BaseModel to define structured data types.
- Creating a generic Pydantic data model that specifies the key-value pairs expected from the LLM output.
- Using a patching mechanism (via the "instructor" library) to patch OpenAI completion methods so that the response can be automatically validated and parsed according to the Pydantic model.
- Formulating prompts that instruct the LLM (e.g., GPT-3.5-turbo) to return data matching the structured format.
- Running the code to convert unstructured text (e.g., cricket match text) into a structured Python object validated by Pydantic.

By defining the data schema upfront and patching the OpenAI completions, this method ensures that the LLM's output adheres to the expected structured format, enabling token-efficient context generation and reliable parsing. The approach highlights the importance of combining type-safe Python models with LLM prompt instructions and output validation to convert complex objects effectively[1].

-----

</details>

<details>
<summary>How can you use a set of detailed instructional profiles (e.g., for tone, structure, and terminology) to reliably control the output of a generative AI writing system?</summary>

### Source [5]: https://deeploy.ml/how-do-we-keep-control-of-generative-ai/

Query: How can you use a set of detailed instructional profiles (e.g., for tone, structure, and terminology) to reliably control the output of a generative AI writing system?

Answer: Detailed instructional profiles—defining tone, structure, and terminology—can help reliably control generative AI output by establishing clear parameters for the AI's behavior and style. According to Deeploy, setting the tone of the conversation is crucial since personalization, creativity, emotion, and empathy are still challenging for AI to generate appropriately. Humans are better at triggering the correct emotions and context, so specifying tone in instructional profiles guides AI to approximate desired styles more accurately. Furthermore, keeping a human in the loop for final decision-making ensures oversight and allows for correction of AI-generated text that may lack judgment or appropriateness. Explainability and feedback loops are also important: when AI systems are partly explainable and open to iterative feedback, humans can steer algorithms toward preferred output styles and content. Also, such profiles help mitigate risks by embedding ethical principles and bias detection within the AI's operational framework. Because generative AI relies on existing data and patterns, detailed profiles prevent the AI from producing generic or outdated content by enforcing current terminology and structure. Together, these factors—tone specification, human oversight, explainability, and ethical grounding—constitute a method to reliably control generative AI output using detailed instructional profiles focused on tone, structure, and terminology.

-----

-----

### Source [7]: https://genai.illinois.edu/best-practices-using-generative-ai-in-research/

Query: How can you use a set of detailed instructional profiles (e.g., for tone, structure, and terminology) to reliably control the output of a generative AI writing system?

Answer: This source highlights best practices in prompt engineering to control generative AI outputs effectively, which directly relates to using detailed instructional profiles. Key principles include being clear and specific in instructions, using the imperative voice (commands rather than questions), and employing positive language (stating what is wanted rather than what is not). Breaking down complex requests into smaller parts and engaging in iterative testing and refinement are also emphasized. Applying these principles to instructional profiles means crafting detailed, explicit, and structured prompts that define tone, structure, and terminology precisely, allowing the AI to generate outputs that reliably meet those specifications. Iterative refinement based on feedback ensures the AI output increasingly aligns with the desired profile. Thus, leveraging prompt engineering best practices is a fundamental mechanism to enforce control over generative AI writing by using detailed instructional profiles.

-----

</details>

<details>
<summary>What is the most effective way to structure a complex, multi-part system prompt for a content generation task like writing a technical article?</summary>

### Source [8]: https://palantir.com/docs/foundry/aip/best-practices-prompt-engineering/

Query: What is the most effective way to structure a complex, multi-part system prompt for a content generation task like writing a technical article?

Answer: The most effective way to structure a complex, multi-part system prompt for a content generation task is to apply best practices of prompt engineering that emphasize clarity, specificity, and contextual relevance. Prompt engineering is an iterative process focused on writing inputs that guide large language models (LLMs) toward producing desired outputs with accuracy and coherence. Key strategies include being clear and specific about the task requirements, refining and iterating on prompt designs based on output quality, and including relevant context to avoid ambiguity. Using examples within prompts can also significantly help the model understand the expected output format and style. Managing prompt length and complexity is critical; overly long or complex prompts should be balanced with the need to provide sufficient detail. Incorporating explicit constraints—such as format requirements or content boundaries—further directs the model's responses. Optimizing the interaction involves structuring prompts to facilitate the best model comprehension and output performance. These principles collectively help unlock the full potential of LLMs for complex tasks like writing a technical article by ensuring that the input is as informative and directive as possible, minimizing misinterpretation and enhancing output relevance and quality.

-----

-----

### Source [9]: https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-the-openai-api

Query: What is the most effective way to structure a complex, multi-part system prompt for a content generation task like writing a technical article?

Answer: When structuring a complex, multi-part system prompt, it is important to place instructions at the beginning of the prompt, clearly separated from context or input text by delimiters such as ### or """. This separation helps the model distinguish instructions from data and reduces confusion. Clarity in instructions is crucial: avoid vague or imprecise descriptions and instead specify exact requirements such as length, style, or format. For example, instead of saying "a few sentences," specify "a 3 to 5 sentence paragraph." It is also effective to tell the model what to do rather than only what not to do to guide behavior explicitly. For tasks like code generation, providing leading keywords or phrases that hint at the expected output pattern (e.g., starting code with "import" or SQL with "SELECT") nudges the model toward the correct structure. Using the latest, most capable model versions can facilitate easier prompt engineering due to improved understanding and reasoning capabilities. Overall, prompts should be concise but precise, structured to separate instructions from context, and include explicit guidance to ensure the model generates complex content accurately and consistently.

-----

-----

### Source [10]: https://www.claude.com/blog/best-practices-for-prompt-engineering

Query: What is the most effective way to structure a complex, multi-part system prompt for a content generation task like writing a technical article?

Answer: Effective structuring of complex, multi-part prompts involves giving clear, explicit instructions that state exactly what output is expected from the AI. Modern large language models respond best when the prompt directly specifies the desired features, style, and scope. Avoid ambiguity by detailing formatting preferences, such as preferring natural paragraphs over bullet points, and explaining the reasons behind preferences to help the model reason about objectives. Matching the prompt style to the desired output style influences how the model responds; for example, reducing markdown in the prompt tends to reduce markdown in the output. For complex tasks, techniques like prompt chaining—breaking the task into stages with focused instructions—can enhance accuracy and reliability, although at the cost of increased latency. However, the best prompt is not necessarily the longest or most complex but the one that reliably achieves the goal with minimal structure. Prompt engineering is a foundational part of broader context engineering, working with conversation history and system instructions to shape AI behavior. Therefore, an effective multi-part system prompt for a technical article should be clear, explicit, and possibly staged, matching the output style and including relevant context to guide the model precisely.

-----

-----

### Source [11]: https://www.lakera.ai/blog/prompt-engineering-guide

Query: What is the most effective way to structure a complex, multi-part system prompt for a content generation task like writing a technical article?

Answer: For structuring complex, multi-part prompts in content generation tasks like writing a technical article, the key is to be clear, direct, and specific. Ambiguity is a major cause of poor outputs, so prompt instructions should be precise, structured, and goal-oriented, including desired format, scope, tone, and length. Combining prompt types can enhance precision; for example, mixing context-rich prompts (used when input includes long documents or structured data) with completion-style prompts (used for creative continuation or story generation). It's beneficial to use structured elements such as hashtags, numbered lists, or consistent delimiters to organize prompts, especially with models like GPT-4o. Different models may benefit from different prompt structures—for instance, Claude 4 favors semantic clarity and tag-like markers (e.g., <task>, <context>), while Gemini 1.5 Pro prefers hierarchical prompts that start broad and zoom in, resembling an outline rather than a paragraph. Avoid vague or fluffy instructions; instead, specify exact parameters and desired output features. Tracking prompt versions and testing across different models help optimize the prompt structure over time. In summary, a complex system prompt should be modular, explicit, and formatted to suit the target model’s strengths, guiding the AI through clear instructions and structured context to reliably produce detailed technical content.

-----

</details>

<details>
<summary>What are common software architecture patterns for implementing the "Orchestrator-Worker" model in Python for parallel LLM tool use?</summary>

### Source [12]: https://arize.com/blog/orchestrator-worker-agents-a-practical-comparison-of-common-agent-frameworks/

Query: What are common software architecture patterns for implementing the "Orchestrator-Worker" model in Python for parallel LLM tool use?

Answer: The orchestrator-worker architecture in AI agent frameworks typically involves a central orchestrator agent that receives a user input and decomposes it into multiple subtasks. These subtasks are distributed to worker agents which process them independently and return results. The orchestrator then observes these results and dynamically determines subsequent subtasks or actions, iterating until the final output is produced. Different frameworks implement this pattern with varying primitives: for example, Agno (Python) uses a declarative team model where a 'Team' with coordinate mode set decides which agent acts based on instructions and success criteria, and workers are registered as team members. This allows routing without manual stitching and includes built-in task transfer tools that surface handoffs in the system. The key aspects for implementing this pattern in Python involve: 1) defining an orchestrator unit that controls task routing and coordination, 2) modeling workers as individual agents or tools registered with the orchestrator, and 3) enabling dynamic task assignment and result aggregation during execution. This approach supports workflows where the problem structure emerges at runtime rather than being statically defined upfront.

-----

-----

### Source [13]: https://forum.langchain.com/t/orchestrator-worker-example-in-langgraph-python-docs/1673

Query: What are common software architecture patterns for implementing the "Orchestrator-Worker" model in Python for parallel LLM tool use?

Answer: In the LangGraph Python framework, the orchestrator-worker pattern is implemented as a state graph where an orchestrator node generates a plan of subtasks (e.g., sub-queries), which are then assigned to multiple worker nodes that run concurrently and independently. Each worker processes its assigned subtask and returns results, which are aggregated and synthesized by a final node. The orchestrator creates a list of subtasks, and workers are spawned for each subtask using a 'Send' mechanism that passes specific inputs to worker nodes. The state is typed to track the overall topic, subqueries, partial results from workers, and the final summary. Crucially, order preservation in the final synthesis can be handled by passing indexes or keys along with subtasks through the graph, enabling the synthesizer to assemble results in the intended logical order. This pattern facilitates concurrency and parallelism in querying or processing large LLM tasks, such as concurrent web searches or multilingual translations, by decomposing the main task into smaller independent units processed by workers in parallel, with the orchestrator coordinating the workflow and consolidating outputs.

-----

-----

### Source [14]: https://www.workflows.guru/blogs/agentic-workflows-patterns

Query: What are common software architecture patterns for implementing the "Orchestrator-Worker" model in Python for parallel LLM tool use?

Answer: The Orchestrator-Worker pattern in agentic workflows consists of three main components: the orchestrator, the workers, and the synthesizer. The orchestrator is a central agent responsible for dividing a complex task into multiple subtasks and assigning these to worker agents. The workers execute these subtasks, often implemented as calls to individual large language model (LLM) instances or tools. Finally, the synthesizer collects the outputs from all workers and combines them to produce the final result. This pattern is particularly useful when tasks are complex and not easily decomposable upfront, requiring dynamic task division and coordination at runtime. Implementing this pattern in Python involves creating the orchestrator logic that manages task splitting and assignment, defining worker agents that execute subtasks, and a synthesizer that merges results. This modular design enables scalable and parallel processing of LLM tasks, improving throughput and flexibility in multi-agent AI workflows.

-----

-----

### Source [15]: https://www.youtube.com/watch?v=ObVNv49S9K4

Query: What are common software architecture patterns for implementing the "Orchestrator-Worker" model in Python for parallel LLM tool use?

Answer: The Orchestrator-Worker design pattern for AI agent workflows can be implemented in Python using LangGraph, an open-source framework by LangChain. LangGraph provides a 'send' API that allows the orchestrator node to spawn multiple worker nodes dynamically, each receiving a specific input subtask. Workers operate independently with their own state and process subtasks in parallel. Once workers complete their tasks, their independent states are returned to the orchestrator, which synthesizes the results into a final output. The shared state model in LangGraph allows orchestrator, workers, and synthesizer nodes to access and update a common state. To handle concurrent updates (e.g., multiple workers appending outputs to a list), LangGraph uses custom reducers that specify how to merge concurrent writes, such as using an 'add' operator to append results safely. This pattern supports dynamic task decomposition, parallel execution of subtasks by workers, and final aggregation by the orchestrator or synthesizer, making it well suited for complex LLM workflows requiring concurrency and coordination.

-----

</details>

<details>
<summary>What are the trade-offs between using a single, large LLM call with all context versus multiple, specialized LLM calls in a workflow?</summary>

### Source [16]: https://arxiv.org/html/2406.10786v3

Query: What are the trade-offs between using a single, large LLM call with all context versus multiple, specialized LLM calls in a workflow?

Answer: Using a single, large LLM call with all context, often referred to as multi-problem prompting (MPP), offers efficiency benefits by reducing redundant context tokens and lowering inference costs per problem. This approach places multiple problems after a shared context, avoiding repeated instructions seen in multiple smaller calls. Studies show that MPP can retain high accuracy with minimal performance loss compared to single-problem prompting (SPP), with some LLMs achieving at least 90% of the accuracy of single calls even as task size grows. For example, the overall drop in accuracy was about 3.2% absolute when batching multiple classification problems together. MPP also reduces inference costs significantly, from about 30.7% up to 82.0%, depending on the model and task. However, as more tasks are combined, slight accuracy declines can occur. Additionally, LLMs generally do not show strong positional biases or serial position effects when handling many problems at once, indicating robust multi-problem handling capabilities. Overall, single large calls improve cost-efficiency and token utilization but may experience minor accuracy trade-offs as complexity increases[1].

-----

-----

### Source [17]: https://blog.promptlayer.com/llm-agents-vs-function-calling/

Query: What are the trade-offs between using a single, large LLM call with all context versus multiple, specialized LLM calls in a workflow?

Answer: Multiple specialized LLM calls within a workflow, often implemented via function calling or agent frameworks, provide modularity and flexibility. Function calling enables LLMs to trigger specific external functions or APIs, allowing real-time data access, task automation, and structured output generation. This can improve accuracy by grounding responses in verified external knowledge and reduce hallucinations. However, it introduces orchestration complexity, potential multiple API calls, and depends on the LLM's ability to accurately map requests to functions. Agents extend this by adding memory and planning modules, allowing multi-step reasoning and breaking large tasks into smaller actionable steps. The trade-offs include managing limited memory capacity, potential inconsistency in outputs due to prompt sensitivity, and challenges in long-term planning. Despite these, specialized calls allow handling complex, multi-step workflows with adaptability and real-time integration, which single large calls may not support as effectively. Hence, multiple specialized calls support modular, stepwise workflows with external integrations, at the cost of increased complexity and possible overhead in managing multiple interactions[2].

-----

</details>

<details>
<summary>What are best practices for designing a "ContextMixin" or similar interface in Python to standardize the conversion of various domain objects into a structured, XML-tagged format for LLM prompts?</summary>

### Source [18]: https://dev.to/bikramjeetsingh/write-composable-reusable-python-classes-using-mixins-6lj

Query: What are best practices for designing a "ContextMixin" or similar interface in Python to standardize the conversion of various domain objects into a structured, XML-tagged format for LLM prompts?

Answer: Mixins in Python are regular classes designed to add specific reusable functionality to other classes without existing on their own. A "ContextMixin" style class typically provides a method like get_context_data, which returns context data as a dictionary. In Django, for example, ContextMixin is used to standardize how context data is prepared for templates. This pattern helps avoid repeating code (DRY principle) and promotes modular, composable class design. To design a ContextMixin for converting domain objects into structured data (e.g., XML tags), one should define a method that returns a structured context (such as a dictionary or an intermediate representation) which can then be serialized. The mixin can be combined with other classes that handle rendering or serialization. This separation supports clear responsibilities and code reuse. The example from Django’s TemplateResponseMixin and ContextMixin illustrates how context data is supplied consistently across different HTTP methods, which is a useful pattern for standardized data preparation in LLM prompt generation or XML tagging[1].

-----

-----

### Source [19]: https://docs.djangoproject.com/en/5.2/topics/class-based-views/mixins/

Query: What are best practices for designing a "ContextMixin" or similar interface in Python to standardize the conversion of various domain objects into a structured, XML-tagged format for LLM prompts?

Answer: Django's ContextMixin provides a standardized interface via the get_context_data method that returns a dictionary used for rendering templates. When designing a ContextMixin-like interface for domain objects and XML tagging, this method should be overridden to add or transform data relevant to the domain object. It is recommended to call super().get_context_data(**kwargs) to preserve existing context and then extend it. This approach supports composability and avoids code duplication. Furthermore, mixins can be combined with other mixins like TemplateResponseMixin or JSONResponseMixin, showing how different output formats can be supported through layered mixins. To handle XML-tagged formats for LLM prompts, you might create a mixin that transforms context into XML strings or structured elements. The design pattern encourages overriding get_context_data to return all necessary data as Python structures before serialization, keeping transformation logic isolated and testable. Also, the method render_to_response or similar can be overridden to produce the final XML-tagged output using the context data[3].

-----

-----

### Source [20]: https://tech.serhatteker.com/post/2019-09/django-usercontextmixin/

Query: What are best practices for designing a "ContextMixin" or similar interface in Python to standardize the conversion of various domain objects into a structured, XML-tagged format for LLM prompts?

Answer: A best practice demonstrated in creating custom ContextMixin subclasses is to inherit from a base ContextMixin and override get_context_data to add domain-specific data. For example, a UserContextMixin obtains the current user from the request and injects it into the context dictionary. This approach cleanly encapsulates the logic for preparing context data related to domain objects, making views simpler and more maintainable. When designing a ContextMixin interface to standardize conversion of domain objects to structured data (like XML), this pattern can be adapted: the mixin gathers or transforms relevant domain object data into a context dictionary, which can then be serialized into XML by other components. This keeps context preparation modular and reusable across different views or components needing the same structured output[2].

-----

-----

### Source [21]: https://www.thedigitalcatonline.com/blog/2020/03/27/mixin-classes-in-python/

Query: What are best practices for designing a "ContextMixin" or similar interface in Python to standardize the conversion of various domain objects into a structured, XML-tagged format for LLM prompts?

Answer: Mixins should be designed to be as independent and focused as possible, avoiding complex inheritance hierarchies. In Python, mixins typically add a single piece of functionality. For example, Django's TemplateView uses TemplateResponseMixin and ContextMixin to separate concerns: ContextMixin provides the get_context_data method for assembling context data, while TemplateResponseMixin handles rendering. This separation allows clear, reusable components. When designing a ContextMixin for standardized XML-tagged output for LLM prompts, you should ensure that the mixin focuses purely on gathering and structuring the domain data into a context dictionary or object representation. Serialization to XML tags should ideally be done by another mixin or class focused on rendering, maintaining single responsibility principles. This modular design improves testability and reuse[4].

-----

-----

### Source [22]: https://python.plainenglish.io/class-based-views-mixins-simple-mixins-202a8be8b367

Query: What are best practices for designing a "ContextMixin" or similar interface in Python to standardize the conversion of various domain objects into a structured, XML-tagged format for LLM prompts?

Answer: The ContextMixin pattern in Python class-based views allows adding extra context data to the view’s context dictionary that will be passed down to the rendering layer (templates or other serializers). Best practices include overriding get_context_data to add or transform data, calling super() to preserve existing data, and keeping the mixin focused on context preparation. This makes it easier to standardize how domain objects are converted into structured formats by centralizing data gathering in one place. For designing a ContextMixin that converts domain objects to XML-tagged format for LLM prompts, the mixin would gather all relevant fields and metadata into a context dictionary or structured object. Then, a separate serialization method or mixin can convert this context into XML. This layered approach aligns with good design principles, enabling composability and reuse across different output formats[5].

-----

-----

### Source [94]: https://dev.to/bikramjeetsingh/write-composable-reusable-python-classes-using-mixins-6lj

Query: What are best practices for designing a "ContextMixin" or similar interface in Python to standardize the conversion of various domain objects into a structured, XML-tagged format for LLM prompts?

Answer: When designing a ContextMixin or similar interface in Python to standardize conversion of domain objects into structured, XML-tagged formats for LLM prompts, best practices from Django's mixin approach offer useful guidance. Mixins should be composable, reusable, and follow the DRY principle to keep code modular and maintainable. For example, Django's ContextMixin provides a get_context_data method that returns a context dictionary, which can be overridden to add or modify context variables. TemplateResponseMixin works well alongside ContextMixin to render templates based on that context without duplicating template name or response code. Similarly, your ContextMixin should provide a method (e.g., get_context_data) that returns a structured dictionary representing the domain object’s data, which can then be converted into XML tags consistently. This allows subclasses to extend or override context data generation cleanly. Using mixins this way results in shorter and simpler code that cleanly separates concerns, facilitating the standardization of domain object conversions into the XML format needed for LLM prompts.

-----

-----

### Source [95]: https://tech.serhatteker.com/post/2019-09/django-usercontextmixin/

Query: What are best practices for designing a "ContextMixin" or similar interface in Python to standardize the conversion of various domain objects into a structured, XML-tagged format for LLM prompts?

Answer: A good practice for designing a ContextMixin is to subclass Django's built-in ContextMixin which provides a get_context_data method that returns a dictionary of context data. You override this method to add your own domain object data to the context. For example, the UserContextMixin adds the current user from the request to the context dictionary. This pattern allows you to standardize how various domain objects contribute to the context by encapsulating their conversion logic in mixins. By doing so, each mixin focuses on one piece of data to add, promoting single responsibility and reusability. When combining multiple mixins, the get_context_data methods merge their results, giving a consolidated context dictionary that can then be serialized or converted (e.g., into XML tags) for LLM prompts. This approach encourages clean, modular, and testable code by isolating the domain-to-context conversion within dedicated mixins.

-----

-----

### Source [96]: https://docs.djangoproject.com/en/5.2/topics/class-based-views/mixins/

Query: What are best practices for designing a "ContextMixin" or similar interface in Python to standardize the conversion of various domain objects into a structured, XML-tagged format for LLM prompts?

Answer: Django’s ContextMixin provides a get_context_data method that returns a dictionary of context data, typically keyword arguments passed into it. This method is commonly overridden to add specific data to the context. In designing a ContextMixin for structured XML-tagged output, a similar pattern applies: implement get_context_data to build a structured dictionary representing your domain object. You can then implement a method (akin to render_to_response) that converts this dictionary into an XML-tagged string suitable for LLM prompts. Django also demonstrates how to create mixins that standardize response formats, such as JSONResponseMixin, which provides render_to_json_response. Analogously, your ContextMixin could provide a render_to_xml_response method that serializes the context data into XML. This modular design enables reuse and composition, letting you combine multiple mixins for complex objects while maintaining a consistent interface for conversion and rendering.

-----

-----

### Source [97]: https://docs.djangoproject.com/en/5.2/ref/class-based-views/mixins-simple/

Query: What are best practices for designing a "ContextMixin" or similar interface in Python to standardize the conversion of various domain objects into a structured, XML-tagged format for LLM prompts?

Answer: Django’s simple ContextMixin offers a pattern for extending the context for rendering by defining an extra_context attribute and overriding get_context_data to merge keyword arguments with extra_context. This method returns a dictionary representing the context, making it easy to add or modify keys. For designing a ContextMixin that standardizes converting domain objects into structured XML for LLM prompts, you can follow this pattern: define a get_context_data method that builds a structured dictionary reflecting the object’s data and optionally an extra_context attribute for additional static data. Then provide a render_to_response or similar method that converts this dictionary into an XML structure with tagged elements. This separation of data preparation (context) and rendering (conversion to XML) aligns with Django’s approach and supports clean, reusable, and testable mixins.

-----

-----

### Source [98]: https://python.plainenglish.io/the-power-of-mixins-in-python-and-django-8f0d194f8a3c

Query: What are best practices for designing a "ContextMixin" or similar interface in Python to standardize the conversion of various domain objects into a structured, XML-tagged format for LLM prompts?

Answer: Best practices for designing mixins in Python, applicable to ContextMixins for standardized conversion, include adhering to the Single Responsibility Principle—each mixin should add one specific behavior or piece of data. You can create mixins that add methods for converting domain objects into structured data (e.g., dictionaries representing XML tags). Combining multiple mixins enables flexible composition so that complex domain objects can be represented by stacking mixins that each handle a part of the data or conversion logic. Implementing clear method names and ensuring that each mixin cleanly calls super() to integrate with other mixins is essential for predictable behavior. For example, a mixin might provide a method to generate XML fragments for a particular domain attribute, which can be combined with other mixins’ fragments to produce a full XML representation for LLM prompts. This modularity and composability make mixins powerful for standardizing data conversions.

-----

</details>

<details>
<summary>How can you apply prompt engineering principles, like those from Anthropic's "anatomy of a prompt," to structure a system prompt that combines diverse context elements such as writing profiles, research summaries, and few-shot examples for a complex article generation task?</summary>

### Source [23]: https://aimaker.substack.com/p/the-10-step-system-prompt-structure-guide-anthropic-claude

Query: How can you apply prompt engineering principles, like those from Anthropic's "anatomy of a prompt," to structure a system prompt that combines diverse context elements such as writing profiles, research summaries, and few-shot examples for a complex article generation task?

Answer: Anthropic's prompt engineering principles, as detailed in their 10-step prompt structure guide, emphasize building prompts with a clear, layered architecture resembling solid building design rather than mere decoration. To structure a system prompt combining diverse context elements such as writing profiles, research summaries, and few-shot examples for complex article generation, you should use three main layers:

1. **Foundation Layer (Steps 1-3):**
   - Provide clear context including your role or situation, the specific project or goal, the audience or stakeholders, and the primary and secondary objectives.
   - Define the tone context explicitly, specifying communication style, emotional approach, and reader relationship to ensure output matches the intended voice.
   - Supply detailed background data including any relevant documents, research summaries, writing profiles, or knowledge bases so the AI understands all pertinent information before responding.

2. **Structure Layer (Steps 4-6):**
   - Clearly define the task in precise terms.
   - Include few-shot examples (multi-shot prompting) within the prompt to guide the AI in style, format, and content expectations, reducing misinterpretation and enforcing consistency.
   - Optionally incorporate conversation history if applicable to maintain context over interactions.

3. **Execution Layer (Steps 7-10):**
   - Provide output guidance such as desired format, length, style, or sections.
   - Set quality control parameters to help the AI self-check or prioritize accuracy.
   - Specify formatting instructions to improve readability and usability of the generated article.

By layering your system prompt in this way, you explicitly communicate to the AI all critical context, expectations, and examples needed for complex content generation tasks. This approach helps avoid ambiguity, ensures consistency with writing profiles, leverages research summaries effectively, and harnesses few-shot examples to produce high-quality, coherent, and task-aligned articles.[2]

-----

-----

### Source [24]: https://www.businessinsider.com/anthropic-guide-prompt-engineering-2025-7

Query: How can you apply prompt engineering principles, like those from Anthropic's "anatomy of a prompt," to structure a system prompt that combines diverse context elements such as writing profiles, research summaries, and few-shot examples for a complex article generation task?

Answer: Anthropic's 'anatomy of a prompt' highlights several key principles applicable to structuring a system prompt that integrates diverse context elements:

- **Explicit and Specific Instructions:** Clearly communicate what you want by being very precise about the desired output, styles, guidelines, and norms. For complex article generation, explicitly state the purpose, audience, and goals of the article.

- **Role Prompting:** Assign the AI a specific role, such as "news editor" or "academic writer," to tailor tone and style to the task. This method enhances performance and ensures output aligns with writing profiles.

- **Use of Examples (Few-Shot or Multi-Shot Prompting):** Provide several well-crafted examples within the prompt to demonstrate the desired output style, structure, and content. This reduces misinterpretations and enforces uniformity.

- **Organized Directions:** Use bullet points or numbered lists to structure the prompt clearly, especially when combining multiple context elements like research summaries and writing profiles.

- **Chain of Thought (CoT) Prompting:** Encourage the AI to break down the task step-by-step within the prompt, allowing it to reason thoroughly before producing the final output.

- **Context Awareness:** Inform the AI about the end use and audience, so it can tailor the content accordingly.

Applying these principles means your system prompt should start by defining the AI's role, specify the article's purpose and audience, include relevant research summaries as background, present clear instructions in an organized format, and provide few-shot examples illustrating desired style and structure. Allowing space for the AI to reason through the task stepwise can further improve output quality.[1]

-----

-----

### Source [25]: https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents

Query: How can you apply prompt engineering principles, like those from Anthropic's "anatomy of a prompt," to structure a system prompt that combines diverse context elements such as writing profiles, research summaries, and few-shot examples for a complex article generation task?

Answer: Anthropic emphasizes that effective prompt engineering for complex tasks involves managing not just a single instruction but the entire context state, including system instructions, tools, external data, and conversation history. For structuring a system prompt that combines diverse context elements like writing profiles, research summaries, and few-shot examples, it is important to:

- Treat the prompt as a part of a broader context management strategy, ensuring all relevant information (background data, instructions, past interactions) is organized and accessible.

- Use system prompts to set high-level instructions and constraints clearly, defining the AI's role, the task's objectives, and the style or tone.

- Integrate external data or knowledge bases (such as research summaries) into the context so the AI can reference them accurately.

- Employ prompt chaining or decomposition techniques to break down complex article generation into manageable steps, each with clear instructions and context.

- Maintain message history or conversational context where relevant to preserve continuity and coherence across multiple interactions.

This approach aligns with Anthropic's view that as AI agents grow more capable, prompt engineering evolves into context engineering, combining multiple elements coherently for optimal performance on complex tasks like article generation.[4]

-----

-----

### Source [72]: https://www.businessinsider.com/anthropic-guide-prompt-engineering-2025-7

Query: How can you apply prompt engineering principles, like those from Anthropic's "anatomy of a prompt," to structure a system prompt that combines diverse context elements such as writing profiles, research summaries, and few-shot examples for a complex article generation task?

Answer: Anthropic's prompt engineering principles emphasize treating the AI (Claude) as a brilliant but forgetful new employee requiring explicit, specific instructions. To structure a system prompt combining diverse context elements such as writing profiles, research summaries, and few-shot examples, you should:  

- Be very specific about what you want, including the intended use, audience, and end goal of the task.  
- Organize directions clearly, using bullet points or numbered lists to lay out each part of the instruction.  
- Provide a few well-crafted examples (few-shot prompting) to improve accuracy and enforce style and structure consistency.  
- Allow the AI space to think step-by-step (chain-of-thought prompting) to improve reasoning on complex tasks.  
- Assign a specific role to the AI (role prompting), such as "news editor" or "financial planner," which is especially effective for complex tasks like legal or financial analysis.  

Applying these principles means your system prompt should explicitly define the writing profile (role and tone), include research summaries as background context, and provide few-shot examples to guide the output format and style. This layered, explicit, and example-rich prompt enables Claude to generate complex articles effectively with clarity and consistency[1].

-----

-----

### Source [73]: https://aimaker.substack.com/p/the-10-step-system-prompt-structure-guide-anthropic-claude

Query: How can you apply prompt engineering principles, like those from Anthropic's "anatomy of a prompt," to structure a system prompt that combines diverse context elements such as writing profiles, research summaries, and few-shot examples for a complex article generation task?

Answer: Anthropic's 10-step prompt engineering framework organizes prompts into three foundational layers—Foundation, Structure, and Execution—which can guide the creation of system prompts combining diverse context elements:  

1. Foundation (Steps 1-3): Provide clear context, tone, and background. For complex article generation, start by specifying your role and project, the target audience, primary and secondary objectives, and what success looks like. Define the tone explicitly (e.g., formal, analytical, friendly) and provide all relevant background data such as writing profiles and research summaries to equip the AI with necessary context.  

2. Structure (Steps 4-6): Define the task precisely, include few-shot examples to illustrate the desired output style and content, and incorporate any relevant conversation history or prior instructions. Few-shot examples serve as a key tool to reduce ambiguity and enforce uniformity in the outputs.  

3. Execution (Steps 7-10): Guide the AI on output formatting, quality control criteria, and specific ways to organize the response. This may include instructions on length, sections, style consistency, and how to handle citations or references.  

This layered approach ensures clarity, reduces ambiguity, and systematically integrates diverse context elements—writing profiles, research data, examples—into a coherent prompt architecture. It moves beyond ad hoc instructions to a solid, logical structure that supports complex tasks reliably across changing contexts[2].

-----

-----

### Source [74]: https://www.caneraras.com/learn/master-prompt-engineering-anthropic-course

Query: How can you apply prompt engineering principles, like those from Anthropic's "anatomy of a prompt," to structure a system prompt that combines diverse context elements such as writing profiles, research summaries, and few-shot examples for a complex article generation task?

Answer: Anthropic's comprehensive prompt engineering course highlights key principles relevant for structuring system prompts with diverse context:  

- Start with a clear and explicit prompt anatomy that separates instructions from data. For example, use structured formatting (e.g., XML tags) to delineate research summaries from the writing profile and few-shot examples.  

- Assign specific roles with detailed descriptions to the AI, such as "You are a senior UX researcher with 10 years of experience," to tailor the AI's perspective and tone. This role assignment improves relevance and style consistency in complex article generation.  

- Use the evidence-first approach for accuracy-critical tasks: first extract relevant quotes or data in designated tags, then provide analysis or synthesis in conclusion tags. This method organizes research summaries and reasoning clearly within the prompt.  

- Employ chain-of-thought prompting by instructing the AI to reason step-by-step, improving the quality of complex outputs.  

Applying these principles, a system prompt for complex article generation should explicitly separate the diverse context elements (writing profile, research, examples) using structured tags, assign a precise role for tone and style, and guide the AI through a reasoning process that integrates all context for a coherent final output[3].

-----

</details>

<details>
<summary>What are effective strategies for designing the "application layer" in an AI system to cleanly orchestrate and inject domain-layer components (like context loaders and specialized nodes) without creating tight coupling?</summary>

### Source [26]: https://learn.microsoft.com/en-us/azure/well-architected/ai/application-design

Query: What are effective strategies for designing the "application layer" in an AI system to cleanly orchestrate and inject domain-layer components (like context loaders and specialized nodes) without creating tight coupling?

Answer: Effective strategies for designing the application layer in an AI system to cleanly orchestrate and inject domain-layer components involve adopting a microservices architecture where AI models, data pipelines, and other components like context loaders or specialized nodes are independently deployable. Containerizing microservices and AI models ensures isolation of dependencies and configurations, enabling flexible integration without tight coupling. This approach facilitates independent deployment, scaling, and efficient updates.

Colocation of components can be considered to optimize latency and resource utilization, such as placing AI models near API hosting or data stores for faster inference and data access. However, designers must balance this against potential loss of independent scalability and increased blast radius in case of failures.

For orchestration, an agent-based (agentic) design pattern is recommended. Agents provide context-bound functionality and register capabilities with an orchestrator, which dynamically routes queries to appropriate agents. Alternatively, agents can collaborate dynamically without a central orchestrator. This pattern supports extensibility, allowing new domain-layer components to plug into the system without tight coupling.

Other design patterns useful for managing complexity and coupling include event-driven architectures for decoupled, real-time processing, and model ensembling for combining outputs from multiple specialized models.

Key considerations for the application layer include managing complex workflows (e.g., preprocessing, model chaining), conditional logic for dynamic routing, resource scaling based on demand, state management of user interactions, and efficient data retrieval from augmentation indexes. Implementing chunking strategies with clear data lineage supports efficient context loading while maintaining traceability.

Overall, leveraging microservices, containerization, agentic orchestration, and event-driven design enables a modular, scalable, and loosely coupled application layer that can cleanly orchestrate domain-layer components like context loaders and specialized nodes[2].

-----

-----

### Source [27]: https://zenvanriel.nl/ai-engineer-blog/understanding-ai-design-patterns/

Query: What are effective strategies for designing the "application layer" in an AI system to cleanly orchestrate and inject domain-layer components (like context loaders and specialized nodes) without creating tight coupling?

Answer: To design the application layer in AI systems that cleanly orchestrate and inject domain-layer components such as context loaders and specialized nodes without tight coupling, adopting architectural patterns that emphasize modularity and separation of concerns is essential.

Key architectural patterns relevant here include:

- Microservices Pattern: This pattern enables modular, independently deployable AI components, allowing domain-layer components to be integrated as separate services. This reduces tight coupling by isolating functionality and dependencies.

- Layered Architecture: Provides clear separation between layers, such as application and domain layers, ensuring that orchestration logic in the application layer interacts with domain components through well-defined interfaces.

- Event Driven Architecture: Facilitates reactive and decoupled communication between components, allowing the application layer to trigger or respond to events from domain components without direct dependencies.

These patterns promote best practices such as modularity, generalizability, and standardized approaches for managing complex AI systems. By encapsulating domain-specific logic within specialized nodes or loaders and orchestrating them through loosely coupled patterns like microservices and event-driven communication, the application layer can maintain clean boundaries and flexibility.

The use of these architectural patterns aligns with broader software engineering principles in AI system design, helping reduce development complexity and improve scalability and maintainability[1].

-----

-----

### Source [28]: https://www.infoq.com/articles/practical-design-patterns-modern-ai-systems/

Query: What are effective strategies for designing the "application layer" in an AI system to cleanly orchestrate and inject domain-layer components (like context loaders and specialized nodes) without creating tight coupling?

Answer: Effective design of the application layer in AI systems that orchestrate domain-layer components, like context loaders and specialized nodes, relies on practical patterns that optimize system throughput, scalability, and modularity.

One key pattern is Intelligent Model Routing, which acts as a lightweight gateway or reverse proxy at the application layer. This pattern routes queries dynamically to appropriate downstream models or components based on request complexity or context, avoiding tight coupling by abstracting routing logic and enabling load balancing, response caching, and fallback handling.

Another useful approach is Multi-Agent Orchestration, where multiple specialized AI agents collaborate dynamically to handle complex tasks. These agents operate independently but can coordinate through predefined protocols or shared context, allowing the application layer to manage orchestration without embedding domain logic directly, thus maintaining loose coupling.

Continuous Dynamic Batching is also critical when processing high volumes of requests. By batching queries dynamically, the system maximizes resource utilization and throughput without tightly coupling processing components.

These patterns collectively enable the application layer to orchestrate domain-layer components efficiently while preserving modularity, extensibility, and scalability. Platforms like Hugging Face or AWS Bedrock support such patterns by enabling model customization and fine-tuning, facilitating clean injection of domain-specific functionality[4].

-----

-----

### Source [29]: https://dev.to/stellaacharoiro/5-essential-api-design-patterns-for-successful-ai-model-implementation-2dkk

Query: What are effective strategies for designing the "application layer" in an AI system to cleanly orchestrate and inject domain-layer components (like context loaders and specialized nodes) without creating tight coupling?

Answer: Designing the application layer to orchestrate and inject domain-layer components in AI systems benefits greatly from robust API design patterns that promote clean interfaces and reduce tight coupling.

Key API design patterns include:

- RESTful Resource Modeling: Define clear, consistent API endpoints representing domain entities or functions, enabling straightforward interaction between the application layer and domain components like context loaders or specialized nodes.

- Asynchronous Processing with Webhooks: For long-running or complex operations, asynchronous patterns decouple request handling from processing, allowing the application layer to manage workflows without blocking or tight coupling.

- Consistent Error Handling: Implement uniform error responses to simplify integration and fault isolation between layers.

- Strategic Versioning: Manage API evolution without breaking existing integrations, supporting gradual injection of new domain components.

- Secure Authentication with Rate Limiting: Protect domain-layer components by controlling access centrally at the API gateway.

By leveraging these patterns, the application layer serves as a stable, secure, and scalable interface to domain components, enabling clean orchestration without embedding domain logic directly and minimizing coupling.

Additional best practices include implementing pagination and filtering for large datasets, comprehensive API documentation, and monitoring to ensure reliable operation and maintainability[3].

-----

</details>

<details>
<summary>What are the practical applications and implementation details of the "evaluator-optimizer" pattern in generative AI workflows for automated content review and refinement?</summary>

### Source [30]: https://docs.praison.ai/docs/features/evaluator-optimiser

Query: What are the practical applications and implementation details of the "evaluator-optimizer" pattern in generative AI workflows for automated content review and refinement?

Answer: The Evaluator-Optimizer pattern in generative AI workflows enables iterative solution generation and refinement through automated quality evaluation and feedback-driven optimization, creating continuous improvement loops. This pattern involves two specialized agents: a Generator that produces solutions based on requirements and feedback, and an Evaluator that automatically assesses the quality and completeness of these solutions. The process is implemented as a feedback loop where after generation, the solution is evaluated, and based on the evaluation output (e.g., 'more' to continue or 'done' to finish), the Generator produces refined solutions. The pattern supports process control mechanisms to monitor and manage the optimization cycle. Configuration involves setting up agents with defined roles, goals, and instructions, and defining tasks with conditions that govern transitions between generation and evaluation steps. This enables automated content review and refinement in a controlled, iterative manner, balancing generation creativity with evaluative quality assurance.

-----

-----

### Source [31]: https://sebgnotes.substack.com/p/evaluator-optimizer-llm-workflow

Query: What are the practical applications and implementation details of the "evaluator-optimizer" pattern in generative AI workflows for automated content review and refinement?

Answer: The Evaluator-Optimizer pattern strategically balances innovation and consistency by separating generation and evaluation roles within AI systems. A generator LLM creates solutions, while an evaluator LLM assesses them against explicit criteria in a continuous feedback loop until predefined success criteria are met. Implementation details emphasize rigorous loop control with circuit breakers to prevent infinite loops, machine-readable and business-relevant evaluation criteria, clear success and failure conditions, and graceful degradation strategies for fallback when optimal solutions are elusive. Monitoring, observability, and memory management are crucial for tracking iterative improvements and long-term learning. The pattern supports progressive automation, enabling autonomous quality control with reduced human oversight and natural audit checkpoints. Practical applications include autonomous code review, content generation with quality guarantees, self-improving chatbot responses, and automated documentation refinement. The pattern shifts system design focus from prompt perfection to robust evaluation criteria, facilitating scalable, self-improving AI workflows with embedded quality assurance.

-----

-----

### Source [32]: https://spring.io/blog/2025/01/21/spring-ai-agentic-patterns

Query: What are the practical applications and implementation details of the "evaluator-optimizer" pattern in generative AI workflows for automated content review and refinement?

Answer: The Evaluator-Optimizer pattern implements a dual-LLM process where one model generates responses and another model evaluates and provides feedback iteratively, akin to a human's refinement process. Using Spring AI's ChatClient, the workflow involves generating an initial solution for a task, evaluating this solution, and then either accepting it if it passes or incorporating evaluator feedback to generate improved versions. This loop continues until the solution meets satisfaction criteria. This approach supports controlled iteration with clear pass/fail criteria, enabling automated refinement of content or solutions within AI workflows. It is particularly suitable for tasks requiring high-quality, precise outputs through iterative improvement cycles.

-----

-----

### Source [33]: https://icepick.hatchet.run/patterns/evaluator-optimizer

Query: What are the practical applications and implementation details of the "evaluator-optimizer" pattern in generative AI workflows for automated content review and refinement?

Answer: Evaluator-Optimizer is a pattern that uses iterative cycles of generation and evaluation to improve output quality in generative AI. One component (the Generator) creates content, while another (the Evaluator) assesses it and provides feedback. This feedback loop repeats until the evaluator is satisfied or a maximum iteration limit is reached, trading computational cost for higher quality results. The pattern enforces controlled iteration with clear termination criteria, ensuring progress and preventing endless loops. The Generator not only creates initial content but also focuses on improving outputs by addressing evaluator feedback, while maintaining the core message and constraints. This iterative feedback-driven optimization is key for automated content review and refinement workflows, enabling continuous quality improvement.

-----

-----

### Source [34]: https://github.com/BootcampToProd/spring-ai-evaluator-optimizer-workflow

Query: What are the practical applications and implementation details of the "evaluator-optimizer" pattern in generative AI workflows for automated content review and refinement?

Answer: This Spring AI repository demonstrates practical implementation of the Evaluator-Optimizer Workflow Pattern, a self-improving AI system that produces high-quality outputs via iterative refinement. It uses two specialized LLM personas: a 'Generator' (Writer) to create initial content and an 'Evaluator' (Editor) to critique and guide improvements. The project includes setting up a Spring Boot application integrated with Google Gemini, implementing recursive improvement loops with safety limits and error handling, and creating REST APIs for professional email generation with built-in quality assurance. This real-world example illustrates how to operationalize the pattern in production, emphasizing role specialization, prompt engineering for AI personas, iterative feedback incorporation, and safeguards to control the refinement process, making it suitable for automated content review and continuous improvement.

-----

-----

### Source [35]: https://huggingface.co/blog/dcarpintero/design-patterns-for-building-agentic-workflows

Query: What are the practical applications and implementation details of the "evaluator-optimizer" pattern in generative AI workflows for automated content review and refinement?

Answer: The Evaluator-Optimizer pattern is inspired by self-reflection, enabling LLMs to iteratively refine their outputs through systematic feedback. Architecturally, it involves two main components: an Evaluator that critically assesses the generated output, and a Generator/Optimizer that applies this feedback to improve subsequent outputs. The workflow starts with the Generator producing an initial response, which may be imperfect. The Evaluator then reviews this output against defined criteria, providing actionable feedback. The Generator uses this feedback to optimize and produce a refined response. This pattern helps avoid accepting first-pass outputs and promotes continuous enhancement in generative AI workflows, facilitating automated content review and refinement by embedding evaluation and optimization in a closed feedback loop.

-----

-----

### Source [36]: https://www.hopx.ai/blog/ai-agents/evaluator-optimizer-loop

Query: What are the practical applications and implementation details of the "evaluator-optimizer" pattern in generative AI workflows for automated content review and refinement?

Answer: The Evaluator-Optimizer loop pattern works by separating generation, evaluation, and optimization tasks among different components or LLM mindsets to improve output quality. The Generator focuses on creating functional solutions; the Evaluator concentrates on identifying all flaws or errors in the output; and the Optimizer uses the Evaluator's feedback to fix specific issues. This separation of concerns prevents compromises that happen when a single model tries to do all three tasks simultaneously. The loop repeats generation and evaluation cycles until the output meets quality standards. This approach supports automated content review and refinement by enabling focused, iterative improvement driven by explicit evaluation feedback, ensuring higher quality and consistency in generative AI workflows.

-----

-----

### Source [55]: https://docs.praison.ai/docs/features/evaluator-optimiser

Query: What are the practical applications and implementation details of the "evaluator-optimizer" pattern in generative AI workflows for automated content review and refinement?

Answer: The Evaluator-Optimizer pattern in generative AI workflows enables iterative solution generation and refinement through a continuous feedback loop between two AI agents: a generator and an evaluator. The generator agent creates initial solutions based on task requirements and prior context, while the evaluator agent automatically assesses the quality and completeness of those solutions against predefined criteria. This pattern supports automated quality evaluation, feedback-driven optimization, and continuous improvement loops, allowing the system to refine outputs until satisfactory results are achieved. Implementation involves defining the roles and goals of each agent, creating task objects that represent generation and evaluation phases, and linking these tasks in a loop controlled by conditions such as "more" (continue refinement) or "done" (stop). The pattern also supports detailed logging and process control to monitor optimization progress. Configuration typically includes explicit instructions for both generation and evaluation phases, enabling detailed feedback to guide iterative improvements. This approach integrates solution generation, quality evaluation, feedback loops, and process control into a cohesive workflow for automated content review and refinement in generative AI applications.

-----

-----

### Source [56]: https://bootcamptoprod.com/spring-ai-evaluator-optimizer-pattern-guide/

Query: What are the practical applications and implementation details of the "evaluator-optimizer" pattern in generative AI workflows for automated content review and refinement?

Answer: The Evaluator-Optimizer pattern is a workflow design in generative AI where two specialized agents collaborate iteratively: the Generator (writer) produces initial content or solutions, and the Evaluator (editor) critically reviews this output against explicit criteria. If the Evaluator deems the output acceptable (“PASS”), the process ends; if not (“NEEDS_IMPROVEMENT”), feedback is sent back to the Generator to create a refined version. This loop continues until quality standards are met. The pattern mimics human creative processes by separating generation and critique, leading to more reliable and higher-quality results than a single complex prompt. Practical applications include automated content review, quality assurance, and self-improving AI systems that can progressively enhance outputs. Implementation details emphasize modular roles for generation and evaluation, explicit feedback mechanisms, and iterative cycles to drive continuous improvement. This pattern is especially useful for tasks requiring nuanced judgment and refinement, such as document generation, code review, or chatbot responses, providing a structured, step-by-step reasoning process that enhances output quality.

-----

-----

### Source [57]: https://sebgnotes.substack.com/p/evaluator-optimizer-llm-workflow

Query: What are the practical applications and implementation details of the "evaluator-optimizer" pattern in generative AI workflows for automated content review and refinement?

Answer: The Evaluator-Optimizer LLM workflow is a pattern that uses two separate large language models (LLMs) or AI agents: one to generate solutions and another to evaluate them in an iterative feedback loop until success criteria are met. This separation balances innovation (generator) with consistency (evaluator) and represents a shift from static to dynamic AI systems. Key components include the Generator (which creates initial solutions and refines them based on feedback), the Evaluator (which assesses solutions against clear, machine-readable, and business-relevant criteria), a Feedback Loop facilitating iterative improvement, and defined Success Criteria to decide when to stop. Technical implementation requires rigorous loop control to avoid infinite cycles, clear exit conditions, quality assurance embedded within the system architecture (potentially with human or autonomous oversight), and graceful degradation strategies to provide fallback solutions when optimal outputs are not found. This pattern enables scalable, self-improving AI applications such as autonomous code review, content generation with quality guarantees, and chatbot response refinement. It supports progressive automation, governance with audit trails, and reduces the burden on prompt engineering by focusing on evaluation criteria design. Overall, it provides a framework for building AI systems that can improve autonomously while maintaining alignment with business objectives.

-----

-----

### Source [58]: https://spring.io/blog/2025/01/21/spring-ai-agentic-patterns

Query: What are the practical applications and implementation details of the "evaluator-optimizer" pattern in generative AI workflows for automated content review and refinement?

Answer: The Evaluator-Optimizer pattern as implemented with Spring AI involves a dual-LLM process where one model generates an initial response (the Generator), and another evaluates and provides feedback (the Evaluator), creating an iterative loop similar to a human writer’s refinement process. The workflow typically follows these steps: generate an initial solution based on the task; evaluate the solution against criteria; if the evaluation passes, the solution is returned; if it needs improvement, the feedback is incorporated to generate a new solution; this loop repeats until satisfactory results are achieved. The pattern leverages Spring AI’s ChatClient for LLM interactions and is designed to modularize generation and evaluation for better control, quality, and refinement of AI outputs. This approach allows developers to implement automated content review and refinement workflows with clear iteration control, enabling continuous improvement of generative AI outputs in practical applications such as document generation, email drafting, and other content creation tasks.

-----

-----

### Source [59]: https://icepick.hatchet.run/patterns/evaluator-optimizer

Query: What are the practical applications and implementation details of the "evaluator-optimizer" pattern in generative AI workflows for automated content review and refinement?

Answer: The Evaluator-Optimizer pattern is based on iterative cycles of content generation and evaluation to improve output quality. One component (the Generator) creates initial content and subsequent refinements by addressing feedback from the Evaluator, which assesses the output and provides targeted critiques. This loop continues until the Evaluator is satisfied or a maximum number of iterations is reached, balancing computational cost with output quality. Implementation details include controlled iteration loops with clear termination criteria, systematic incorporation of feedback into subsequent generations, and maintaining core message and constraints while improving outputs. The pattern involves components such as a client interface, generator and evaluator agents, and iteration control mechanisms. It is based on frameworks like Anthropic’s "Building Effective Agents" and is designed to enhance AI workflows for automated content review and refinement by embedding quality assurance directly into the generation process rather than applying it afterwards.

-----

-----

### Source [60]: https://github.com/BootcampToProd/spring-ai-evaluator-optimizer-workflow

Query: What are the practical applications and implementation details of the "evaluator-optimizer" pattern in generative AI workflows for automated content review and refinement?

Answer: This GitHub project demonstrates the implementation of the Spring AI Evaluator Optimizer Workflow Pattern, which embodies a self-improving AI system for generating high-quality content through iterative refinement. It employs two specialized LLM roles: a Generator (writer) that creates initial content and an Evaluator (editor) that critiques and guides improvements. The workflow recursively loops, refining output until it meets predefined quality standards. The project details include setting up a Spring Boot application integrated with Spring AI and Google Gemini, defining AI personas through system prompts for Writer and Editor roles, and building a REST API for professional email generation with quality assurance. Additional implementation specifics cover managing iterative refinement with safety limits, comprehensive error handling, and recursive feedback incorporation. This practical example showcases how the evaluator-optimizer pattern can be applied in real-world automated content generation and review tasks, emphasizing modular design, iterative feedback loops, and quality control mechanisms to achieve refined outputs.

-----

-----

### Source [75]: https://docs.praison.ai/docs/features/evaluator-optimiser

Query: What are the practical applications and implementation details of the "evaluator-optimizer" pattern in generative AI workflows for automated content review and refinement?

Answer: The Evaluator-Optimizer pattern in generative AI workflows enables iterative solution generation and refinement, automated quality evaluation, feedback-driven optimization, and continuous improvement loops. It functions by creating two specialized agents: a Generator agent that produces initial solutions based on requirements and feedback, and an Evaluator agent that automatically assesses the quality and completeness of those solutions. The pattern incorporates a feedback loop where the evaluator’s assessment guides the generator to improve the solution iteratively. Additionally, process control mechanisms monitor and regulate the optimization cycle to ensure convergence or termination based on predefined conditions. Implementation typically involves defining agents with specific roles and goals, setting up tasks that alternate between generation and evaluation phases, and establishing conditions for continuing or ending the loop (e.g., “more” to generate again or “done” to finish). This pattern is designed to embed quality assurance directly into the generative process rather than applying it post hoc, enabling a controlled and self-improving AI workflow for content review and refinement.

-----

-----

### Source [76]: https://bootcamptoprod.com/spring-ai-evaluator-optimizer-pattern-guide/

Query: What are the practical applications and implementation details of the "evaluator-optimizer" pattern in generative AI workflows for automated content review and refinement?

Answer: The Evaluator-Optimizer pattern structures AI workflows into two distinct roles: the Generator (Writer), which creates an initial response or solution, and the Evaluator (Editor), which reviews the output against predefined criteria and decides if it meets quality standards. If the Evaluator’s verdict is "NEEDS_IMPROVEMENT," the feedback is sent back to the Generator for refinement; if it is "PASS," the output is finalized. This iterative generate-evaluate-decide loop mimics human problem-solving and often yields higher-quality, more reliable results than complex single-prompt approaches. Implementation does not require different underlying LLM models—distinct roles are defined via system prompts shaping the AI’s persona rather than by separate models. The pattern’s practical application includes automated content review and refinement by structuring generation and critique as separate, interlinked steps, improving output quality progressively through feedback cycles.

-----

-----

### Source [77]: https://sebgnotes.substack.com/p/evaluator-optimizer-llm-workflow

Query: What are the practical applications and implementation details of the "evaluator-optimizer" pattern in generative AI workflows for automated content review and refinement?

Answer: The Evaluator-Optimizer LLM Workflow balances innovation with consistency by separating the generation of solutions and their evaluation into distinct LLM roles within a continuous feedback loop, iterating until success criteria are met. Key components include the Generator that creates solutions, the Evaluator that assesses these solutions against explicit, machine-readable, and business-relevant criteria, and a feedback loop enabling iterative refinement. Implementation requires defining rigorous loop exit criteria to avoid infinite cycles, embedding quality assurance into system architecture (either via autonomous LLM oversight or human intervention), and managing risks like hallucination and computational costs. The pattern supports scalable, self-improving AI systems with natural checkpoints for governance, audit, and progressive automation. Practical applications span autonomous code review, content generation with quality guarantees, self-improving chatbots, and automated documentation refinement. Successful implementation focuses on robust evaluation criteria design rather than perfect generation prompts, and includes monitoring, observability, memory management, and graceful degradation strategies.

-----

-----

### Source [78]: https://icepick.hatchet.run/patterns/evaluator-optimizer

Query: What are the practical applications and implementation details of the "evaluator-optimizer" pattern in generative AI workflows for automated content review and refinement?

Answer: The evaluator-optimizer pattern uses iterative cycles of generation and evaluation to enhance output quality by having one component (the generator) produce content and another (the evaluator) assess it and provide feedback. This creates a loop that continues until the evaluator is satisfied or a maximum iteration count is reached, trading computational cost for higher quality. The pattern includes clear termination criteria—either evaluator satisfaction or hitting iteration limits—and builds outputs progressively by incorporating feedback. The generator handles both initial creation and iterative improvements, focusing on addressing evaluator-identified issues while preserving core message and constraints. This design supports controlled iteration and systematic refinement, making it suitable for automated content review and refinement workflows where continuous quality improvement is required.

-----

-----

### Source [79]: https://github.com/BootcampToProd/spring-ai-evaluator-optimizer-workflow

Query: What are the practical applications and implementation details of the "evaluator-optimizer" pattern in generative AI workflows for automated content review and refinement?

Answer: This open-source project demonstrates the implementation of the Spring AI Evaluator Optimizer Workflow pattern, showcasing a practical use case: a Smart Email Generator that employs recursive improvement loops. The system sets up a Spring Boot application integrated with Google Gemini and Spring AI to create distinct AI personas for the Generator (Writer) and Evaluator (Editor) roles through tailored system prompts. It implements iterative refinement workflows with safety limits and comprehensive error handling to ensure both quality and robustness. The pattern facilitates high-quality content creation by continuously critiquing and improving outputs until they meet predefined quality standards. The project also includes a complete REST API enabling professional email generation with embedded quality assurance, illustrating practical implementation details such as persona creation, recursive feedback loops, and safety controls in an enterprise-ready environment.

-----

</details>

<details>
<summary>How can the orchestrator-worker pattern be implemented to generate and manage diverse, independent media items like Mermaid diagrams and images in parallel within a single AI workflow?</summary>

### Source [37]: https://ai-sdk.dev/docs/agents/workflows

Query: How can the orchestrator-worker pattern be implemented to generate and manage diverse, independent media items like Mermaid diagrams and images in parallel within a single AI workflow?

Answer: The orchestrator-worker pattern involves a primary model, called the orchestrator, which coordinates the execution of specialized worker models. Each worker is optimized for a specific subtask, allowing the system to handle complex tasks that require diverse expertise or processing types. The orchestrator maintains the overall context and ensures coherent results. This pattern supports parallel processing by running independent tasks simultaneously, making it suitable for generating and managing diverse, independent media items like Mermaid diagrams and images in parallel within a single AI workflow. The orchestrator plans and delegates subtasks to the workers, collects their outputs, and synthesizes the final result, ensuring that the workflow is structured and reliable.

-----

-----

### Source [38]: https://github.com/BootcampToProd/spring-ai-orchestrator-workers-workflow

Query: How can the orchestrator-worker pattern be implemented to generate and manage diverse, independent media items like Mermaid diagrams and images in parallel within a single AI workflow?

Answer: This implementation demonstrates the orchestration of complex AI workflows by decomposing requests into specialized subtasks handled by multiple worker LLMs in parallel. The orchestrator (manager LLM) analyzes the initial request, breaks it down into independent tasks (e.g., accommodations, activities, dining, transportation in a travel planning example), and assigns them to specialized worker LLMs optimized for each subtask. Workers execute their tasks concurrently, improving efficiency. Finally, a synthesizer component combines all workers' outputs into a cohesive final product. This dynamic task orchestration system can be applied to generate various media items in parallel, such as Mermaid diagrams and images, by assigning specialized workers for each media type and combining their outputs under the orchestrator's supervision.

-----

-----

### Source [39]: https://javaaidev.com/docs/agentic-patterns/patterns/orchestrator-workers-workflow/

Query: How can the orchestrator-worker pattern be implemented to generate and manage diverse, independent media items like Mermaid diagrams and images in parallel within a single AI workflow?

Answer: The orchestrator-workers workflow pattern allows a central LLM (the orchestrator) to select and coordinate the best approach for completing a complex task by using multiple specialized agents (workers) registered as tools. The orchestrator uses reasoning models (e.g., GPT-5 or similar) to plan and decompose the task into subtasks, which are then executed by worker models (e.g., GPT-4.1). This approach naturally supports parallel execution since subtasks are independent and can be handled concurrently. The orchestrator maintains context and uses prompt templates and reasoning techniques to manage coordination. For generating diverse media items like Mermaid diagrams and images, the orchestrator can allocate subtasks to specialized workers optimized for each media type, run them in parallel, and then synthesize their outputs into a unified workflow result.

-----

-----

### Source [40]: https://arize.com/blog/orchestrator-worker-agents-a-practical-comparison-of-common-agent-frameworks/

Query: How can the orchestrator-worker pattern be implemented to generate and manage diverse, independent media items like Mermaid diagrams and images in parallel within a single AI workflow?

Answer: The orchestrator-worker architecture is designed for dynamic workflows where problem structure emerges at runtime. A user input is passed to the orchestrator agent, which decomposes it into multiple subtasks. Each subtask is routed to a specialized worker agent that returns a local result. The orchestrator observes these results, updates the task graph state, and iteratively generates new subtasks or refines existing ones until the final output is produced. This multi-stage reasoning process supports complex workflows requiring diverse expertise. To implement parallel generation and management of diverse media items like Mermaid diagrams and images, the orchestrator must dynamically route subtasks to workers specialized in each media type, maintain context continuity across cycles, and handle seamless handoffs to avoid loss of context or duplication. Execution unfolds in cycles of subtask generation, worker assignment, result collection, and state updating until task completion.

-----

-----

### Source [41]: https://www.youtube.com/watch?v=NyJbDkY14fY

Query: How can the orchestrator-worker pattern be implemented to generate and manage diverse, independent media items like Mermaid diagrams and images in parallel within a single AI workflow?

Answer: This tutorial on the Spring AI Orchestrator-Workers pattern illustrates how to implement the pattern in a real-world application. The orchestrator analyzes the incoming request and dynamically identifies the number and types of specialized workers required. It submits tasks to these workers, which execute in parallel using asynchronous methods (e.g., CompletableFuture in Java). Workers perform their subtasks independently, such as different aspects of travel planning, and their results are collected and synthesized into a final output. The orchestrator maintains the overall context, strategy, and analysis, enabling dynamic and scalable parallel execution. This approach can be adapted to generate and manage diverse, independent media items like Mermaid diagrams and images concurrently within a single AI workflow, by defining worker roles specialized for each media type and orchestrating their parallel execution and result aggregation.

-----

-----

### Source [61]: https://ai-sdk.dev/docs/agents/workflows

Query: How can the orchestrator-worker pattern be implemented to generate and manage diverse, independent media items like Mermaid diagrams and images in parallel within a single AI workflow?

Answer: The orchestrator-worker pattern involves a primary model (the orchestrator) that coordinates the execution of specialized workers. Each worker is optimized for a specific subtask, allowing complex tasks requiring different types of expertise or processing to be handled efficiently. In the context of generating and managing diverse, independent media items like Mermaid diagrams and images in parallel within a single AI workflow, the orchestrator would maintain the overall context and delegate subtasks to specialized workers responsible for generating each type of media. This enables parallel processing of independent subtasks while ensuring coherent results across the workflow. The orchestrator manages the decomposition of the main task into subtasks and collects the outputs from each worker to produce a unified final output.

-----

-----

### Source [62]: https://github.com/BootcampToProd/spring-ai-orchestrator-workers-workflow

Query: How can the orchestrator-worker pattern be implemented to generate and manage diverse, independent media items like Mermaid diagrams and images in parallel within a single AI workflow?

Answer: The Spring AI Orchestrator-Workers Workflow Pattern demonstrates a dynamic task orchestration system that decomposes complex problems into specialized subtasks executed in parallel by multiple worker LLMs. For generating and managing diverse media items like Mermaid diagrams and images, the orchestrator analyzes the request, identifies the required specialized subtasks, and dispatches them to corresponding workers that handle different media generation aspects. These workers operate concurrently to improve efficiency. After all workers complete their tasks, a synthesizer component combines their outputs into a cohesive final product. This approach allows independent media items to be generated in parallel within a single AI workflow, maintaining modularity and scalability.

-----

-----

### Source [63]: https://javaaidev.com/docs/agentic-patterns/patterns/orchestrator-workers-workflow/

Query: How can the orchestrator-worker pattern be implemented to generate and manage diverse, independent media items like Mermaid diagrams and images in parallel within a single AI workflow?

Answer: In the Orchestrator-Workers Workflow pattern, an LLM acting as the orchestrator uses reasoning models (e.g., GPT-5) to plan and decompose the overall task into subtasks, which are then executed by worker models (e.g., GPT-4.1) specialized for those subtasks. This pattern naturally integrates with the Agent as Tool approach by registering all possible worker agents as tools. The orchestrator dynamically decides which workers to invoke based on the task requirements. For generating various media items like Mermaid diagrams and images in parallel, the orchestrator would break down the workflow into independent subtasks, assign these to the appropriate specialized workers, and then collect and synthesize the results. The use of advanced prompt engineering and reasoning models simplifies orchestrator prompt design, enabling effective management of diverse, independent parallel processes.

-----

-----

### Source [64]: https://arize.com/blog/orchestrator-worker-agents-a-practical-comparison-of-common-agent-frameworks/

Query: How can the orchestrator-worker pattern be implemented to generate and manage diverse, independent media items like Mermaid diagrams and images in parallel within a single AI workflow?

Answer: The orchestrator-worker architecture is designed for workflows where the problem structure emerges dynamically at runtime. A user input is passed to an orchestrator agent, which decomposes it into a set of subtasks. Each subtask is routed to a specialized worker agent that returns a local result. The orchestrator observes these results and iteratively determines subsequent subtasks or refinements until the final output is achieved. Key challenges include dynamic routing of subtasks to specialized workers, maintaining context continuity across cycles, and seamless handoff between agents without losing information. In generating diverse media items like Mermaid diagrams and images, the orchestrator assigns these independent subtasks to specialized workers in parallel, tracks their progress and results, and synthesizes the final output. Execution unfolds in cycles of generating subtasks, assigning workers, collecting results, updating state, and repeating as needed.

-----

-----

### Source [65]: https://www.youtube.com/watch?v=NyJbDkY14fY

Query: How can the orchestrator-worker pattern be implemented to generate and manage diverse, independent media items like Mermaid diagrams and images in parallel within a single AI workflow?

Answer: This tutorial on Spring AI's Orchestrator-Workers Pattern explains an implementation where a controller receives a request and submits it to an orchestrator workflow class. The orchestrator analyzes the requirement to identify the number and type of specialist workers needed. It then submits tasks to these workers, which execute in parallel using asynchronous methods (e.g., CompletableFuture). The orchestrator collects the results from these workers and synthesizes them into a final output. For diverse media generation such as Mermaid diagrams and images, the orchestrator would dynamically create tasks for each media type, dispatching them to respective specialized workers that operate concurrently. The final synthesized result integrates all media outputs coherently, providing an efficient workflow for parallel generation and management of independent media items.

-----

-----

### Source [66]: https://www.anthropic.com/research/building-effective-agents

Query: How can the orchestrator-worker pattern be implemented to generate and manage diverse, independent media items like Mermaid diagrams and images in parallel within a single AI workflow?

Answer: The orchestrator-workers workflow involves a central large language model (LLM) that dynamically breaks down tasks and delegates them to worker LLMs specialized in subtasks. After workers execute their subtasks, the orchestrator synthesizes their results into a coherent final output. This pattern naturally supports parallel execution of diverse independent subtasks, such as generating Mermaid diagrams and images within a single AI workflow. The orchestrator maintains the overall context, manages task decomposition, delegates to specialized workers, and combines their outputs efficiently, enabling scalable and modular processing of heterogeneous media generation tasks.

-----

</details>

<details>
<summary>What are best practices for abstracting LLM API calls in a Python application to allow for easy swapping and configuration of different models, such as those from OpenAI, Anthropic, or Google?</summary>

### Source [42]: https://www.twosigma.com/articles/a-guide-to-large-language-model-abstractions/

Query: What are best practices for abstracting LLM API calls in a Python application to allow for easy swapping and configuration of different models, such as those from OpenAI, Anthropic, or Google?

Answer: This article introduces a comprehensive taxonomy and layered abstraction model for large language model (LLM) frameworks, which is useful for abstracting LLM API calls in Python applications. It organizes LLM abstractions into seven layers, from low-level interaction with model parameters to high-level prompt generation and optimization tools. For easy swapping and configuration of different models like OpenAI, Anthropic, or Google, it emphasizes using abstractions that act as simple wrappers around models exposing a uniform interface (text in, text out), such as the core OpenAI API or LangChain-core. More advanced schema-driven generation abstractions allow specifying output formats or data structures with tools like Pydantic, enabling controlled generation that can validate and parse outputs uniformly across providers. Examples include libraries like Instructor and Marvin that use typed function signatures or JSON schemas to integrate with different LLMs through a common interface. The article also highlights the importance of separating concerns in design to avoid tight coupling between prompts, substitutions, models, and output parsers, thus enabling flexible model swapping and extensibility within a Python codebase.

-----

-----

### Source [43]: https://proxai.co/blog/archive/llm-abstraction-layer

Query: What are best practices for abstracting LLM API calls in a Python application to allow for easy swapping and configuration of different models, such as those from OpenAI, Anthropic, or Google?

Answer: This source advocates for implementing a dedicated LLM abstraction layer in your codebase to avoid vendor lock-in and facilitate easy switching between models from providers like OpenAI, Anthropic, Meta, and others. Key best practices for such an abstraction layer include: - Prioritizing simplicity and low overhead so the layer does not become a maintenance burden, with a clean, intuitive API for fast team adoption. - Ensuring robustness with reliable error handling, intelligent retries, and stability suitable for production environments. - Incorporating built-in features beyond simple API routing, such as caching to reduce latency and cost, standardized logging for observability, and usage dashboards. - Maintaining strict data privacy by avoiding routing data through third-party servers outside your infrastructure. The ProxAI platform is cited as an example that provides a unified API to over 100 models across 10+ providers with minimal code changes, focusing on simplicity, privacy, and cost optimization. This approach allows swapping models with just configuration changes, supporting workflows that dynamically choose cheaper or more expensive models based on task complexity.

-----

-----

### Source [44]: https://www.abstractapi.com/guides/other/the-production-ready-llm-api-playbook

Query: What are best practices for abstracting LLM API calls in a Python application to allow for easy swapping and configuration of different models, such as those from OpenAI, Anthropic, or Google?

Answer: This guide recommends implementing an architectural pattern known as the AI Gateway or Filter between your application and the LLM API. This proxy layer acts as an abstraction to: - Uniformly manage requests and responses across different LLM providers. - Handle authentication, rate limiting, retries, and error handling centrally. - Enable logging, monitoring, and usage tracking in a standardized manner. - Facilitate easy swapping and configuration of different LLM models by decoupling the application logic from specific LLM APIs. This pattern improves robustness, observability, and security and simplifies maintenance by centralizing integration concerns. It also enables adding cross-cutting features like caching or prompt preprocessing transparently to downstream LLM calls, making it easier to switch between providers such as OpenAI, Anthropic, or Google by changing only gateway configurations rather than application code.

-----

-----

### Source [45]: https://llm.datasette.io/en/stable/python-api.html

Query: What are best practices for abstracting LLM API calls in a Python application to allow for easy swapping and configuration of different models, such as those from OpenAI, Anthropic, or Google?

Answer: The LLM Python API described here demonstrates a practical approach to abstracting LLM calls by providing a uniform interface to multiple models through a single Python package. Key practices include: - Obtaining a model object via a factory function (e.g., llm.get_model("gpt-4.1-mini")) that hides provider-specific details. - Interacting with the model through standard methods like model.prompt() or conversational chains, accepting optional parameters such as API keys. - Supporting callback functions and asynchronous calls for flexible response handling. - Utilizing environment variables or explicit key passing to manage credentials uniformly. This abstraction allows a developer to switch between different models by simply changing the model identifier string without modifying the rest of the application logic. The interface focuses on simplicity and consistency, enabling easy configuration and swapping of models like those from OpenAI, Anthropic, or others supported by the underlying system.

-----

-----

### Source [46]: https://dev.to/vaibhav3002/10-essential-practices-for-building-robust-llm-applications-9l7

Query: What are best practices for abstracting LLM API calls in a Python application to allow for easy swapping and configuration of different models, such as those from OpenAI, Anthropic, or Google?

Answer: This article lists essential practices for robust LLM application development, emphasizing abstraction for flexibility. The first practice is to leverage middleware frameworks such as LiteLLM or LangChain, which provide abstraction layers over various LLM providers. These middleware solutions: - Avoid vendor lock-in by enabling easy switching between different models and providers. - Offer standardized APIs that hide the underlying differences in model interfaces. - Support extensions and customizations for prompt management, caching, and orchestration. Additionally, the article advises implementing retry mechanisms to handle API rate limits and failures gracefully. Using such abstraction layers in Python applications allows developers to configure different LLM models (OpenAI, Anthropic, Google, etc.) with minimal code changes, facilitating quick model swaps and robust error handling.

-----

-----

### Source [67]: https://www.twosigma.com/articles/a-guide-to-large-language-model-abstractions/

Query: What are best practices for abstracting LLM API calls in a Python application to allow for easy swapping and configuration of different models, such as those from OpenAI, Anthropic, or Google?

Answer: Two Sigma proposes a structured approach to abstracting Large Language Model (LLM) API calls by organizing LLM frameworks into a seven-layer Language Model System Interface (LMSI) model. This stratification allows developers to select the appropriate abstraction level that balances detail and usability. At the lowest level, direct interaction with model parameters occurs, while higher levels provide more user-friendly abstractions such as Base Prompting, which uniformly wraps different LMs as simple text-in, text-out interfaces, akin to OpenAI's standard APIs.

More advanced abstractions include schema-driven generation, where LLM outputs are controlled via predefined schemas (often JSON) and validated using libraries like Pydantic. For example, Instructor provides clients that accept Pydantic models for response validation and support multiple querying modes like JSON, function calling, and tool usage, each mapping to OpenAI's APIs. Marvin uses Python type annotations to define AI functions that are dynamically intercepted, feeding type hints and descriptions to the LLM to produce structured, type-safe results.

This layered approach promotes separation of concerns, enabling developers to easily swap or configure different models by encapsulating their interactions within well-defined abstraction boundaries. It also facilitates leveraging existing frameworks and infrastructure while allowing extensions or alternative implementations at different layers, thus supporting easy integration and interchangeability among providers such as OpenAI, Anthropic, or Google.

-----

-----

### Source [68]: https://www.abstractapi.com/guides/other/the-production-ready-llm-api-playbook

Query: What are best practices for abstracting LLM API calls in a Python application to allow for easy swapping and configuration of different models, such as those from OpenAI, Anthropic, or Google?

Answer: Abstract API's Production-Ready LLM API Playbook recommends best practices for efficient, secure, and cost-effective use of LLM APIs, which are applicable to abstracting LLM calls in Python applications for easy swapping and configuration:

- Token Optimization: Craft concise prompts, avoid redundant context, use system messages for global instructions, and employ prompt templates to standardize interactions, reducing cost and improving speed.

- Parameter Tuning: Control behaviors through parameters such as temperature (randomness), top_p (nucleus sampling), and stop_sequences (output termination), ensuring outputs are focused and relevant.

- Choosing the Right Model: Match model capabilities to task requirements, e.g., use lightweight models for classification/filtering and larger models for creative generation. This selective matching improves efficiency and cost.

- Security: Employ input validation to detect and block injection attempts, sanitize outputs for safe rendering, and treat all LLM responses as untrusted data.

- Architectural Pattern: Implement an AI Gateway or proxy layer between the app and LLM API to centralize logging, auditing, content moderation, and removal of sensitive data, which standardizes and secures access across multiple providers.

These practices support a design that abstracts provider APIs behind configurable layers, enabling easy swapping of models (OpenAI, Anthropic, Google, etc.) by centralizing control over prompt management, tuning parameters, security, and monitoring.

-----

-----

### Source [69]: https://proxai.co/blog/archive/llm-abstraction-layer

Query: What are best practices for abstracting LLM API calls in a Python application to allow for easy swapping and configuration of different models, such as those from OpenAI, Anthropic, or Google?

Answer: ProxAI advocates for using a dedicated LLM abstraction layer in Python applications to manage interactions with multiple LLM providers like OpenAI, Anthropic, Meta, and others. Key best practices highlighted include:

- Simplicity and Low Overhead: The abstraction layer should have a clean, intuitive API that minimizes complexity and adoption friction.

- Robustness: It must be reliable and battle-tested, handling errors gracefully and managing retries to ensure production stability.

- Built-in Features: Look for integrated caching to reduce latency and cost, standardized logging for observability, and dashboards for usage monitoring.

- Data Privacy: Ensure the abstraction does not route data through third-party servers to avoid privacy and compliance risks; ideally, keep all data processing local or within controlled infrastructure.

ProxAI itself offers a unified API supporting over 100 models across multiple providers, focusing on ease of switching models with minimal code changes, cost optimization, and strong privacy guarantees. Although still in beta, such abstraction layers exemplify best practices for modular, configurable LLM integration in Python applications, enabling easy swapping and consistent management of diverse LLM APIs.

-----

-----

### Source [70]: https://realpython.com/practical-prompt-engineering/

Query: What are best practices for abstracting LLM API calls in a Python application to allow for easy swapping and configuration of different models, such as those from OpenAI, Anthropic, or Google?

Answer: When abstracting LLM API calls in Python, managing prompt parameters like temperature is a best practice to control model output behavior. Setting the temperature parameter to 0 ensures mostly deterministic results, ideal for predictable outputs. Temperatures above 0 introduce randomness by allowing the model to select less probable tokens, which can lead to diverse or creative responses. Understanding and exposing such parameters in the abstraction layer allows developers to configure models flexibly according to application needs, such as deterministic responses for factual tasks or creative generation for open-ended tasks. This parameter tuning capability is fundamental to designing abstractions that support different models (OpenAI, Anthropic, Google) with varying capabilities and use cases.

-----

-----

### Source [71]: https://llm.datasette.io/en/stable/python-api.html

Query: What are best practices for abstracting LLM API calls in a Python application to allow for easy swapping and configuration of different models, such as those from OpenAI, Anthropic, or Google?

Answer: The LLM Python API demonstrates a practical approach to abstracting model interactions by providing a unified interface to multiple LLMs through a model registry (e.g., llm.get_model("gpt-4.1-mini")). It supports conversational contexts with memory and allows chaining prompts with callbacks. Models that require API keys accept them as parameters or retrieve from environment variables, enabling flexible configuration per provider without changing core application logic.

This design pattern—centralizing model access via a factory method and encapsulating conversation state and API credentials—facilitates easy swapping between models from OpenAI, Anthropic, or others by changing the model identifier and credentials. It abstracts provider-specific details behind a common programming interface, promoting modularity and configurability in Python applications.

-----

-----

### Source [84]: https://www.twosigma.com/articles/a-guide-to-large-language-model-abstractions/

Query: What are best practices for abstracting LLM API calls in a Python application to allow for easy swapping and configuration of different models, such as those from OpenAI, Anthropic, or Google?

Answer: This article from Two Sigma provides a comprehensive taxonomy and framework for abstracting interactions with large language models (LLMs) in Python applications. It emphasizes the importance of layered abstractions that separate concerns and enable easier swapping and configuration of different LLM providers such as OpenAI, Anthropic, or Google. The authors propose seven layers of abstraction ranging from low-level direct interaction with LLM parameters to higher-level abstractions like base prompting and schema-driven generation. The base prompting layer acts as a simple wrapper around LLMs, providing a uniform text-in, text-out interface, which is the core of many providers' APIs and frameworks like LangChain-core. A more advanced abstraction involves schema-driven generation, where outputs are structured according to predefined schemas (e.g., JSON) using tools like Pydantic for validation, allowing safer and more controlled generation. Frameworks like Instructor and Marvin leverage such schema-driven approaches with function calling and type-safe annotations to guide the LLM output and allow natural language invocation of functions, enhancing modularity and interchangeability. Overall, the article advocates designing abstractions at the right functional layer to enable reusability, maintainability, and ease of swapping LLMs by focusing on uniform interfaces, schema validation, and separation of prompting logic from model-specific details.

-----

-----

### Source [85]: https://www.abstractapi.com/guides/other/the-production-ready-llm-api-playbook

Query: What are best practices for abstracting LLM API calls in a Python application to allow for easy swapping and configuration of different models, such as those from OpenAI, Anthropic, or Google?

Answer: The Abstract API's Production-Ready LLM API Playbook includes best practices for designing robust, maintainable LLM API abstractions in Python applications. Key recommendations for easy swapping and configuration of different models include: 1) Token Optimization - craft concise prompts, avoid redundant context, and use standardized prompt templates to reduce cost and latency. 2) Parameter Tuning - expose parameters such as temperature, top_p, and stop sequences in the abstraction layer to control model behavior flexibly for different providers. 3) Choosing the Right Model - design abstractions so that lightweight models can be swapped in for classification/filtering and larger models for creative generation, matching the task to the model. 4) Security and Sanitization - centralize input validation to prevent prompt injections and encode outputs to avoid XSS vulnerabilities, ensuring safe interaction regardless of underlying LLM. 5) Architectural Pattern - implement an AI gateway or proxy layer that sits between your application and LLM APIs to log, audit, remove sensitive data, and enforce moderation uniformly across different providers. These practices help build a unified interface that abstracts away provider-specific details, supports multiple models, and enhances security and cost efficiency.

-----

-----

### Source [86]: https://proxai.co/blog/archive/llm-abstraction-layer

Query: What are best practices for abstracting LLM API calls in a Python application to allow for easy swapping and configuration of different models, such as those from OpenAI, Anthropic, or Google?

Answer: This source highlights the urgency and benefits of implementing an LLM abstraction layer in your Python codebase to facilitate easy swapping and configuration of different models from providers like OpenAI, Anthropic, Meta, and others. It stresses the importance of simplicity and low overhead in the abstraction layer's design, so it does not become a maintenance burden. Robustness is critical: the abstraction must handle errors gracefully, manage retries, and provide stable production usage. Additional features like built-in caching, standardized logging, and usage dashboards enhance observability and cost control. Privacy is a major concern; a good abstraction keeps data within your infrastructure without routing through third-party servers, avoiding compliance risks. The example of ProxAI is cited as a developer-first abstraction layer with a simple API that supports over 100 models from 10+ providers, focusing on cost optimization and privacy. Using such an abstraction layer allows minimal code changes when switching models or providers, fosters maintainability, and reduces technical debt from provider lock-in.

-----

</details>

<details>
<summary>How can multiple distinct writing profiles (e.g., for tonality, structure, terminology, and character voice) be programmatically combined and injected into a single LLM prompt to ensure a cohesive and compliant output?</summary>

### Source [47]: https://arxiv.org/html/2410.07283v1

Query: How can multiple distinct writing profiles (e.g., for tonality, structure, terminology, and character voice) be programmatically combined and injected into a single LLM prompt to ensure a cohesive and compliant output?

Answer: The paper discusses prompt injection in multi-agent systems where multiple LLM agents interact. It introduces a novel attack called Prompt Infection, where one compromised agent spreads malicious prompts to others, coordinating them to exchange data and issue instructions, potentially compromising the entire system. To defend, the authors propose LLM Tagging—appending markers to agent responses to differentiate user inputs from agent-generated outputs, reducing prompt injection risk. However, LLM Tagging alone is insufficient; combining it with other mechanisms like instruction defense or marking (adding special symbols to distinguish prompts) significantly improves protection. This layered defense approach ensures that multiple distinct instruction sets or profiles (e.g., for tonality, terminology, character voice) can be programmatically managed by clearly marking and tagging prompt segments, thus maintaining coherence and reducing injection risks. The work highlights the importance of structured prompt management and tagging in multi-agent or multi-profile LLM prompt systems to ensure cohesive and compliant output without cross-contamination or injection vulnerabilities.

-----

-----

### Source [48]: https://www.evidentlyai.com/llm-guide/prompt-injection-llm

Query: How can multiple distinct writing profiles (e.g., for tonality, structure, terminology, and character voice) be programmatically combined and injected into a single LLM prompt to ensure a cohesive and compliant output?

Answer: This guide emphasizes the importance of prompt formatting and structured separation of instructions to safely combine multiple writing profiles in prompts. It suggests isolating user input and system instructions using clear delimiters or tags (e.g., <user>...</user>) so that different instruction sets—such as those for tonality, structure, terminology, and character voice—can be injected into a single prompt without merging or confusing them. By visibly separating user input from system instructions and using structured prompt templates, an LLM can maintain a cohesive output that respects each profile's constraints. Additionally, prompt-level strategies such as explicit instructions to the model about how to blend or prioritize multiple profiles help ensure compliance and consistency. This approach minimizes the risk of prompt injection by preventing user input from unintentionally overriding system instructions or writing profiles.

-----

-----

### Source [87]: https://arxiv.org/html/2410.07283v1

Query: How can multiple distinct writing profiles (e.g., for tonality, structure, terminology, and character voice) be programmatically combined and injected into a single LLM prompt to ensure a cohesive and compliant output?

Answer: The paper explores prompt injection attacks within multi-agent LLM systems and highlights the complexity when multiple agents interact with distinct roles such as strategist, summarizer, editor, and writer. To programmatically combine multiple distinct writing profiles (e.g., tonality, structure, terminology, character voice) into a single LLM prompt, the study implies the need to carefully coordinate instructions across agents to maintain cohesion and compliance. They introduce a defense mechanism called LLM Tagging, which appends markers to agent responses to distinguish user inputs from agent-generated outputs, helping prevent prompt injection and maintain output integrity. The experiments show that combining LLM Tagging with other strategies (like instruction defense and marking) is necessary to effectively prevent malicious prompt manipulations that could disrupt the coherent application of writing profiles. This suggests that programmatic combination of multiple profiles should include explicit tagging and separation of instruction layers to ensure each profile's instructions do not interfere or override each other, thus preserving a cohesive and compliant single output. The multi-agent setup also demonstrates that a scalable approach involves chaining specialized agents, each refining different aspects of a prompt or output, but coordinated through tagging and strict instruction controls to avoid injection risks and ensure compliance with the intended profiles.

-----

-----

### Source [88]: https://docs.aws.amazon.com/prescriptive-guidance/latest/llm-prompt-engineering-best-practices/common-attacks.html

Query: How can multiple distinct writing profiles (e.g., for tonality, structure, terminology, and character voice) be programmatically combined and injected into a single LLM prompt to ensure a cohesive and compliant output?

Answer: This source emphasizes the use of prompt templates to define personas and roles in LLM prompts to tailor outputs, which is directly relevant to combining writing profiles programmatically. Combining distinct profiles such as tonality, structure, terminology, and character voice can be done by crafting prompt templates that specify these aspects upfront — for example, by including a persona statement like "You are a formal financial analyst" to set tone and terminology. The guidance also warns about prompt injection attacks that try to switch personas maliciously and suggests careful prompt construction to avoid conflicting instructions. Thus, ensuring a cohesive and compliant output involves defining clear, consistent persona and style instructions in the prompt template, and possibly layering these instructions in a way that prevents adversarial switching or dilution of profiles. Using structured prompt formats and explicit instructions helps maintain the integrity of combined writing profiles in a single prompt.

-----

-----

### Source [89]: https://www.evidentlyai.com/llm-guide/prompt-injection-llm

Query: How can multiple distinct writing profiles (e.g., for tonality, structure, terminology, and character voice) be programmatically combined and injected into a single LLM prompt to ensure a cohesive and compliant output?

Answer: This guide suggests best practices for prompt engineering to reduce injection risks and maintain compliance when combining multiple instructions. Key recommendations include separating system instructions (e.g., writing profiles) from user input by using delimiters or tags to isolate trusted logic from untrusted input. For combining multiple distinct writing profiles, this means structuring the prompt to clearly delineate instructions for tonality, structure, terminology, and character voice as separate, well-defined sections within the system prompt. The use of delimiters or markdown tags to separate each profile instruction helps the LLM understand and apply them cohesively. Additionally, prompt instructions should be explicit, avoiding ambiguity or conflicting commands. The guide also suggests testing and validating prompt outputs to ensure the combined profiles produce a unified and policy-compliant response. Overall, programmatically injecting multiple profiles involves clear, modular prompt formatting that visibly segments and specifies each profile aspect, reducing the risk of interference and ensuring compliance.

-----

-----

### Source [90]: https://labs.withsecure.com/publications/multi-chain-prompt-injection-attacks

Query: How can multiple distinct writing profiles (e.g., for tonality, structure, terminology, and character voice) be programmatically combined and injected into a single LLM prompt to ensure a cohesive and compliant output?

Answer: This source discusses multi-chain prompt injection attacks in workflows where multiple LLM chains refine and pass outputs sequentially to each other. The concept of multi-chain interactions can be leveraged beneficially to combine distinct writing profiles programmatically by assigning each profile aspect (tonality, structure, terminology, character voice) to a different LLM chain or step. Each chain can specialize in applying or refining one profile dimension, and the output is passed onward, resulting in a cohesive final output. However, the source warns about the complexity and security risks in such multi-step systems, emphasizing the need for careful prompt design to prevent injection attacks that exploit chain interactions. To ensure cohesion and compliance, developers should implement strict input validation, tagging, and clear separation of instructions between chains. This approach enables modular injection and combination of multiple profiles while maintaining control over the overall output consistency.

-----

</details>

<details>
<summary>What are effective prompt engineering strategies for instructing an LLM to generate long-form, multi-section technical articles that seamlessly integrate pre-generated media items like Mermaid diagrams at specific points in the text?</summary>

### Source [49]: https://palantir.com/docs/foundry/aip/best-practices-prompt-engineering/

Query: What are effective prompt engineering strategies for instructing an LLM to generate long-form, multi-section technical articles that seamlessly integrate pre-generated media items like Mermaid diagrams at specific points in the text?

Answer: Effective prompt engineering requires designing inputs that guide large language models (LLMs) to generate desired outputs with accuracy, relevance, and coherence. Key strategies include being clear and specific about what you want the model to produce, especially for complex tasks such as long-form, multi-section technical articles. Incorporating relevant context and examples within the prompt helps the model understand the structure and content expectations. Managing prompt length and complexity is essential to maintain focus and coherence throughout the response. Explicitly incorporating constraints or instructions to embed pre-generated media items, like Mermaid diagrams, at specific points can help the model integrate these elements seamlessly by signaling where and how to place them. Iteratively refining and optimizing prompts based on output quality is recommended to improve results. These best practices support generating structured, multi-part documents with integrated media, emphasizing clarity, specificity, and contextual guidance throughout the prompt engineering process.

-----

-----

### Source [50]: https://pmc.ncbi.nlm.nih.gov/articles/PMC12191768/

Query: What are effective prompt engineering strategies for instructing an LLM to generate long-form, multi-section technical articles that seamlessly integrate pre-generated media items like Mermaid diagrams at specific points in the text?

Answer: This review highlights advanced prompt engineering techniques that enhance LLM output quality, relevant for generating long-form technical articles with media integration. Techniques such as "chain of thought" (CoT) prompting encourage the model to produce a logical, step-by-step reasoning path, which can be adapted to structure multi-section articles coherently. The "generated knowledge" approach directs the model to first produce relevant information before composing the final output, improving logical coherence and content completeness. Active prompting involves selecting and annotating examples that guide the model's reasoning and output style, which can include instructions about where to embed media like Mermaid diagrams. The use of prompt pattern catalogs—standardized, reusable prompt templates—can help manage complex tasks by combining patterns for document structure and media integration. Reinforcement learning (RL) methods can further optimize prompts iteratively, refining how the model incorporates external media references in the generated text. These methodologies collectively support producing detailed, well-structured, multi-section technical content seamlessly integrating pre-generated diagrams at designated points.

-----

-----

### Source [51]: https://cloud.google.com/discover/what-is-prompt-engineering

Query: What are effective prompt engineering strategies for instructing an LLM to generate long-form, multi-section technical articles that seamlessly integrate pre-generated media items like Mermaid diagrams at specific points in the text?

Answer: Google Cloud's guide on prompt engineering emphasizes the importance of carefully crafting prompts by providing clear instructions, relevant context, and examples to guide large language models (LLMs) toward desired outputs. For generating long-form, multi-section technical articles, specifying the desired length, format, and structure explicitly in the prompt is critical. Including action verbs and detailed requests helps the model understand the task, such as instructing it to "Write a multi-section technical article" or "Insert the following Mermaid diagram code after section 2." The guide also highlights the value of chain-of-thought (CoT) prompting to break down complex outputs into intermediate reasoning steps, which is beneficial for structuring long articles logically. To seamlessly integrate pre-generated media items like Mermaid diagrams, the prompt can specify exact insertion points and formatting instructions, ensuring the model outputs text with placeholders or embedded code snippets appropriately. Providing examples of sections with integrated diagrams within the prompt further improves accuracy and coherence. Overall, strategic prompt design involving clarity, specificity, examples, and formatting instructions enables effective generation of comprehensive technical articles with embedded media.

-----

</details>

<details>
<summary>In the context of AI-native applications, what are established design patterns for an "application layer" that orchestrates domain-layer components (like data loaders and processing nodes) while maintaining loose coupling and allowing for dependency injection?</summary>

### Source [52]: https://learn.microsoft.com/en-us/azure/well-architected/ai/application-design

Query: In the context of AI-native applications, what are established design patterns for an "application layer" that orchestrates domain-layer components (like data loaders and processing nodes) while maintaining loose coupling and allowing for dependency injection?

Answer: In AI-native applications, the application layer that orchestrates domain-layer components such as data loaders and processing nodes benefits from established design patterns that promote loose coupling and support dependency injection. Key design patterns include:

- **Microservices architecture:** Decompose the application into independently deployable services handling specific functions like data processing, model inference, or authentication. This separation enables independent scaling, maintenance, and updates, reducing tight coupling between components.

- **Containerization of AI models:** Each AI model is containerized with its dependencies and configurations to isolate environments and ensure consistent behavior across deployments.

- **Event-driven architecture:** Using events to trigger actions decouples components, enabling real-time processing and responsiveness. This pattern supports loosely coupled, asynchronous communication between domain-layer components.

- **Complex workflow orchestration:** For workflows involving multiple processing steps (e.g., preprocessing, model chaining, postprocessing), and conditional logic (e.g., routing results based on outputs), orchestration patterns are used to manage flow while maintaining component independence.

- **Agentic approaches and AI agent orchestration:** When multiple agents or models collaborate dynamically, agent orchestration patterns allow flexible coordination without a central orchestrator enforcing tight coupling.

- **Model ensembling:** Combining outputs from several models improves robustness and accuracy, allowing domain components to remain modular.

Together, these patterns support loose coupling by isolating concerns into services or agents and enable dependency injection by allowing components to be instantiated and connected dynamically based on configuration or runtime context. Such architecture is suitable when scaling, complex workflows, and data-driven responsiveness are required in AI-native applications.

-----

-----

### Source [53]: https://hypermode.com/blog/ai-native-app-development-guide

Query: In the context of AI-native applications, what are established design patterns for an "application layer" that orchestrates domain-layer components (like data loaders and processing nodes) while maintaining loose coupling and allowing for dependency injection?

Answer: A foundational design pattern for AI-native applications is the **model-first architecture with orchestration**, where the application is built around AI models and their coordination rather than traditional structures. This pattern involves:

- Designing the application to optimize AI performance by orchestrating multiple AI models and tools.

- Implementing orchestration systems that manage model interactions, enabling seamless integration of new models and capabilities over time.

- Building modular, composable application layers that can be updated and extended easily.

- Ensuring observability at every layer to provide transparency into model behavior, logic, and data flows, which supports debugging, trust, and iterative improvement.

The orchestration layer acts as an application layer that connects domain components like data loaders and processing nodes, enabling dynamic coordination while maintaining loose coupling through modularity and composability. Dependency injection is facilitated by the flexible infrastructure and model layers that allow components to be integrated or replaced without tight dependencies.

This approach is complemented by infrastructure designed for scalable, dynamic AI workloads and observability layers that track performance and decision paths, reinforcing maintainability and extensibility of the system.

-----

-----

### Source [54]: https://www.catio.tech/blog/emerging-architecture-patterns-for-the-ai-native-enterprise

Query: In the context of AI-native applications, what are established design patterns for an "application layer" that orchestrates domain-layer components (like data loaders and processing nodes) while maintaining loose coupling and allowing for dependency injection?

Answer: Emerging architectural patterns for AI-native applications emphasize the need for new abstractions and orchestration mechanisms beyond traditional API gateways. Relevant patterns for the application layer orchestrating domain components include:

- **LLM as Interface Layer:** Using large language models (LLMs) as a central interface, enhanced with supporting components like prompt routers, memory layers, guardrails, and feedback evaluators.

- **Agent-Based Decomposition:** Breaking down application logic into multiple autonomous agents, each responsible for specific tasks such as interpreting environment topology, surfacing risks, or making recommendations.

- **AI-Orchestrated Workflows:** Instead of a monolithic orchestrator, workflows are managed through collaboration between agents that communicate and coordinate dynamically, maintaining loose coupling.

These patterns support dependency injection by allowing components and agents to be composed and configured flexibly. The orchestration is distributed, reducing tight coupling and enabling scalable, maintainable AI-native applications where domain-layer components like data loaders and processing nodes can be plugged in or replaced independently.

-----

-----

### Source [80]: https://learn.microsoft.com/en-us/azure/well-architected/ai/application-design

Query: In the context of AI-native applications, what are established design patterns for an "application layer" that orchestrates domain-layer components (like data loaders and processing nodes) while maintaining loose coupling and allowing for dependency injection?

Answer: In AI-native applications, an effective "application layer" design pattern involves leveraging a microservices architecture where domain-layer components such as data loaders and processing nodes are encapsulated as independently deployable services. This enables loose coupling and facilitates dependency injection by isolating functionalities like data processing, model inference, and user authentication into containerized microservices. Containerizing AI models separately ensures consistent environments and version control, preventing conflicts and improving maintainability. For orchestration, patterns such as event-driven architecture allow actions to be triggered by events, supporting decoupled components and real-time responsiveness. Complex workflows involving multiple steps like preprocessing, model chaining, or postprocessing are managed by dynamic orchestration systems that can include conditional logic for routing results based on model outputs. Additionally, agentic approaches support multiple evolving features and allow agents to dynamically collaborate without a central orchestrator, which can be suitable for applications requiring flexible, plug-in skills. Model ensembling is another pattern used to combine predictions from multiple models for improved accuracy. Overall, these design patterns enable scalability, resource management, state management, and data retrieval while maintaining modularity and loose coupling in AI-native applications[1].

-----

-----

### Source [81]: https://hypermode.com/blog/ai-native-app-development-guide

Query: In the context of AI-native applications, what are established design patterns for an "application layer" that orchestrates domain-layer components (like data loaders and processing nodes) while maintaining loose coupling and allowing for dependency injection?

Answer: AI-native applications are best built using a model-first architecture that emphasizes orchestration at the application layer to manage and coordinate multiple AI models and domain components efficiently. This approach prioritizes designing the app around AI capabilities rather than retrofitting AI into existing systems. The application layer acts as an orchestrator that integrates AI models, data loaders, and processing nodes into a modular and composable system. Such systems are designed for transparency and observability, allowing visibility into logic, model behavior, and data flows, which is critical for debugging, trust, and continuous improvement. Dependency injection is supported by modular design, enabling easy updates and extension of individual components without tight coupling. Observability at every layer captures signals like inference latency, token usage, and decision paths, enhancing orchestration effectiveness and system reliability. This layered architecture includes an infrastructure layer preparing for dynamic intelligence and a model layer embedding core AI capabilities, with the application layer coordinating workflows and interactions across components, all built to scale and adapt flexibly in production environments[2].

-----

-----

### Source [82]: https://www.catio.tech/blog/emerging-architecture-patterns-for-the-ai-native-enterprise

Query: In the context of AI-native applications, what are established design patterns for an "application layer" that orchestrates domain-layer components (like data loaders and processing nodes) while maintaining loose coupling and allowing for dependency injection?

Answer: Emerging architecture patterns for AI-native enterprises emphasize that the application layer must go beyond simple API gateways to orchestrate complex AI workflows involving large language models (LLMs) and other components. Key design patterns include LLM-as-interface and AI-orchestrated workflows, where the application layer manages prompt routing, memory layers, guardrails, feedback evaluation, and tool execution. This design promotes loose coupling by decomposing tasks into agents with specific jobs (such as interpreting topology or surfacing risks) that communicate and coordinate dynamically. Such multi-agent workflows allow flexible orchestration without a rigid centralized controller. The architecture supports dependency injection through modular components like prompt routers and memory managers that can be replaced or extended independently. This agent-based decomposition pattern is well-suited for AI-native applications that require collaboration among multiple domain-layer components and dynamic decision-making. The supporting infrastructure layers handle routing, context retrieval, and execution, enabling the application layer to orchestrate components while maintaining scalability and modularity[3].

-----

-----

### Source [83]: https://martinfowler.com/articles/gen-ai-patterns/

Query: In the context of AI-native applications, what are established design patterns for an "application layer" that orchestrates domain-layer components (like data loaders and processing nodes) while maintaining loose coupling and allowing for dependency injection?

Answer: In the context of AI-native applications, design patterns that support orchestration at the application layer focus on modularity and composability with clear separation of concerns to maintain loose coupling and enable dependency injection. Patterns such as Retrieval Augmented Generation (RAG) demonstrate how an application layer orchestrates data retrieval components and AI models by retrieving relevant document fragments and combining them with queries before sending to the model. Guardrails and hybrid retrievers are employed to sanitize inputs and control outputs, ensuring safety and correctness without tightly coupling components. The layering includes tokenizers, embedding models, and attention layers, each performing distinct roles. This layered architecture allows dependency injection by swapping components like embedding models or rerankers independently. The approach supports complex workflows with multiple AI models and data processing steps, managed through well-defined interfaces and event-driven triggers. Selective fine-tuning and semantic routing enhance flexibility, allowing components to be replaced or upgraded without affecting the entire system. These patterns together form a robust orchestration framework that maintains loose coupling while coordinating domain-layer components effectively[4].

-----

-----

### Source [99]: https://learn.microsoft.com/en-us/azure/well-architected/ai/application-design

Query: In the context of AI-native applications, what are established design patterns for an "application layer" that orchestrates domain-layer components (like data loaders and processing nodes) while maintaining loose coupling and allowing for dependency injection?

Answer: Established design patterns for the application layer in AI-native applications emphasize managing complex workflows and maintaining loose coupling of domain-layer components through microservices and event-driven architectures. The microservices architecture pattern advocates containerizing individual services responsible for specific functions such as data processing, model inference, and authentication, allowing independent deployment, scaling, and updates. This separation enhances maintainability and scalability while supporting dependency injection by decoupling components.

Event-driven architecture further supports loose coupling by enabling components to react to events asynchronously, promoting real-time responsiveness and adaptability to changing data. This pattern is especially useful when handling extensive data processing and dynamic workflows.

For orchestration, complex workflows involving multiple steps (preprocessing, model chaining, postprocessing) and conditional logic (dynamic routing of outputs) require design patterns that allow flexible coordination without tight integration. Agentic approaches allow multiple AI agents to collaborate dynamically rather than relying on a centralized orchestrator, facilitating modular extensibility.

Additional practices include colocating AI components with related services to reduce latency and improve data proximity, which aids performance without compromising modularity. Combining these patterns supports scalable, maintainable AI-native applications with loosely coupled domain components and facilitates dependency injection by isolating responsibilities and enabling clear interfaces between services[1].

-----

-----

### Source [100]: https://hypermode.com/blog/ai-native-app-development-guide

Query: In the context of AI-native applications, what are established design patterns for an "application layer" that orchestrates domain-layer components (like data loaders and processing nodes) while maintaining loose coupling and allowing for dependency injection?

Answer: In AI-native applications, the application layer often employs a model-first architecture centered on AI capabilities and sophisticated orchestration systems that coordinate multiple AI models and tools. This pattern enables efficient processing and seamless integration of new models, supporting loose coupling through modularity and dependency injection.

Key features include composability and observability at every layer, ensuring transparency and flexibility. Modular components can be updated or extended independently, which aligns with maintaining loose coupling. Observability provides visibility into data flows, model behaviors, and decision processes, critical for debugging and trust.

The architecture typically involves distinct layers: infrastructure for scalable AI workloads; model layer for integrating foundation and fine-tuned models; and the application layer that orchestrates workflows across these models using context engines (e.g., knowledge graphs combined with retrieval-augmented generation). This layered approach facilitates clear separation of concerns and dependency injection by exposing well-defined interfaces and enabling dynamic coordination.

Observability is integrated as a core feature to trace inference latency, token usage, and decision paths, which supports managing complex orchestrations while maintaining modularity and loose coupling among domain components. Thus, design patterns here focus on orchestrating AI models and processing nodes through modular, observable, and composable components orchestrated by a model-first, layered architecture[2].

-----

-----

### Source [101]: https://www.catio.tech/blog/emerging-architecture-patterns-for-the-ai-native-enterprise

Query: In the context of AI-native applications, what are established design patterns for an "application layer" that orchestrates domain-layer components (like data loaders and processing nodes) while maintaining loose coupling and allowing for dependency injection?

Answer: Emerging AI-native application design patterns highlight the need for new abstractions beyond traditional API gateways to orchestrate domain-layer components such as data loaders and processing nodes. Key patterns include 'LLM as Interface Layer' and 'AI-Orchestrated Workflows,' where orchestration is managed dynamically by AI agents rather than static orchestrators.

This agent-based decomposition pattern involves multiple specialized agents, each responsible for distinct tasks (e.g., interpreting topology, surfacing risks, generating recommendations) that communicate and coordinate asynchronously. This supports loose coupling by isolating responsibilities and enabling flexible collaboration.

Supporting infrastructure includes prompt routers, memory layers, guardrails, feedback evaluators, and tool execution frameworks that collectively enable dependency injection and modular orchestration. The pattern moves away from monolithic orchestration to a distributed, agent-driven approach, enhancing scalability and maintainability.

Such AI-orchestrated workflows allow domain-layer components to be composed dynamically at runtime with minimal tight coupling. The architecture leverages routing and context retrieval to integrate components seamlessly while preserving modularity, facilitating dependency injection, and enabling evolving AI-native applications to adapt and scale effectively[3].

-----

-----

### Source [102]: https://jit.pro/blog/what-is-ai-native-app-arch

Query: In the context of AI-native applications, what are established design patterns for an "application layer" that orchestrates domain-layer components (like data loaders and processing nodes) while maintaining loose coupling and allowing for dependency injection?

Answer: AI-native application architecture proposes a structured, modular approach to orchestrate domain-layer components while maintaining loose coupling and enabling dependency injection. JitAi introduces a Unified Module Discovery Mechanism through a structured interpretive protocol (JAAP) that defines application system construction standards and modular architecture at a structural level.

This protocol ensures that each module's capabilities and interfaces are declaratively described, enabling AI components to discover and interact with modules dynamically without tight integration. The architecture is organized into a three-tier system module structure:

- Meta: Defines domains and categories
- Type: Encapsulates technical implementations with controlled configuration options
- Instance: Contains application-layer personalized business configurations

This layered modularity facilitates loose coupling, as modules interact through well-defined interfaces and configurations rather than direct dependencies. Dependency injection is supported by exposing configuration options and enabling dynamic binding of modules at runtime. This approach allows AI-native applications to orchestrate data loaders, processing nodes, and other domain components flexibly and extensibly while preserving modularity and maintainability[4].

-----

-----

### Source [103]: https://www.sidetool.co/post/ai-native-architectures-building-smarter-systems/

Query: In the context of AI-native applications, what are established design patterns for an "application layer" that orchestrates domain-layer components (like data loaders and processing nodes) while maintaining loose coupling and allowing for dependency injection?

Answer: AI-native architectures extend cloud-native principles such as microservices, containerization, automation, and scalability to build smarter, adaptive systems. In the application layer, design patterns focus on leveraging AI models and intelligent agents that orchestrate workflows by interacting with model outputs, prompt chains, and enterprise data sources.

Key architectural pillars include real-time data pipelines, hybrid compute distribution for low latency and heavy AI workloads, automated model retraining, elastic scaling, and embedded security and ethical governance. These pillars support loosely coupled components that communicate via event-driven or API-based interactions, enabling dependency injection through modular service boundaries.

The use of prompt chains and model outputs for real-time orchestration enables dynamic routing and processing of data through various domain components such as data loaders and processing nodes. Intelligent agents act as orchestrators that can compose and sequence tasks adaptively, which supports loose coupling by abstracting coordination logic away from individual components.

Thus, established patterns at the application layer include microservices-based modularization, agent-driven orchestration, and real-time AI pipeline management that collectively maintain loose coupling and enable dependency injection for scalable AI-native applications[5].

-----

</details>

<details>
<summary>What are effective prompt engineering strategies for instructing an LLM to generate a long-form, multi-section technical article that seamlessly integrates placeholders for pre-generated media items, like Mermaid diagrams, at specific points in the text?</summary>

### Source [91]: https://palantir.com/docs/foundry/aip/best-practices-prompt-engineering/

Query: What are effective prompt engineering strategies for instructing an LLM to generate a long-form, multi-section technical article that seamlessly integrates placeholders for pre-generated media items, like Mermaid diagrams, at specific points in the text?

Answer: Effective prompt engineering involves crafting clear, specific, and contextually relevant prompts that guide large language models (LLMs) to produce desired outputs. For a long-form, multi-section technical article that integrates placeholders for pre-generated media (like Mermaid diagrams), the prompt should explicitly specify the structure, length, and where to insert placeholders. Key strategies include:

- **Be clear and specific:** Clearly instruct the LLM to write multiple sections, define the technical scope, and specify exact points where placeholders for media items should appear.
- **Provide relevant context:** Include background information on the topic and the role of the media items so the LLM can integrate them meaningfully.
- **Incorporate constraints:** Specify formatting rules, placeholder syntax (e.g., `{{MermaidDiagram1}}`), and section headings.
- **Manage length and complexity:** Guide the LLM on approximate length per section and the overall article.
- **Use examples:** Provide sample sections or exemplar placeholders to illustrate the expected format.
- **Refine and iterate:** Use iterative prompting to improve structure, coherence, and placeholder integration.

By combining these strategies, the prompt can instruct the LLM to generate a cohesive, well-organized technical article with seamless integration of placeholders for external media elements, enhancing clarity and utility[1].

-----

-----

### Source [92]: https://pmc.ncbi.nlm.nih.gov/articles/PMC12191768/

Query: What are effective prompt engineering strategies for instructing an LLM to generate a long-form, multi-section technical article that seamlessly integrates placeholders for pre-generated media items, like Mermaid diagrams, at specific points in the text?

Answer: Advanced prompt engineering methods improve LLM output quality by structuring prompts to encourage logical coherence and comprehensive responses. For generating a long-form, multi-section technical article integrating media placeholders, consider these approaches:

- **Chain of Thought (CoT) prompting:** Encourage the model to generate content step-by-step or section-by-section, enhancing reasoning and organization.
- **Generated knowledge approach:** Prompt the LLM first to outline or generate relevant information or intermediate content before writing the full text, improving logical flow and content completeness.
- **Prompt pattern catalog:** Use standardized prompt templates to maintain consistency in article structure and placeholder usage.
- **Active prompting:** Iteratively refine prompts based on uncertain or ambiguous outputs to improve clarity in integrating placeholders.

For example, appending instructions like “Let us think step by step” can lead the LLM to produce a sequentially organized article, inserting placeholders at specified points. Similarly, instructing the model to first create an outline with named placeholders (e.g., for Mermaid diagrams) ensures that media items are integrated at intended locations. Employing these techniques enhances the model’s ability to generate complex, multi-part technical documents with embedded media references[2].

-----

-----

### Source [93]: https://cloud.google.com/discover/what-is-prompt-engineering

Query: What are effective prompt engineering strategies for instructing an LLM to generate a long-form, multi-section technical article that seamlessly integrates placeholders for pre-generated media items, like Mermaid diagrams, at specific points in the text?

Answer: To instruct an LLM to generate a long-form technical article with integrated placeholders for pre-generated media items, effective prompt engineering involves:

- **Providing explicit instructions:** Specify the desired length, multi-section format, and exact placeholder syntax (e.g., `{{MermaidDiagram1}}`) to guide the model.
- **Using action verbs and clear directives:** For example, "Write a detailed, multi-section technical article on [topic], and insert placeholders for Mermaid diagrams at the specified points."
- **Providing context and examples:** Include background on the article’s purpose and sample placeholder usage to improve output accuracy.
- **Defining the target audience and style:** Tailor the prompt to ensure that the technical depth and tone are appropriate.
- **Incorporating chain-of-thought prompting:** Encourage the model to break down the article creation into logical steps or sections, enhancing coherence.

These tactics help the LLM understand the structure and content requirements, ensuring that placeholders are seamlessly integrated where intended, and the article meets length and technical standards[3].

-----

</details>

<details>
<summary>What are best practices for designing a Python factory function and configuration model (like Pydantic's BaseModel) to abstract and unify API calls for various LLM providers?</summary>

### Source [104]: https://dagster.io/blog/python-factory-patterns

Query: What are best practices for designing a Python factory function and configuration model (like Pydantic's BaseModel) to abstract and unify API calls for various LLM providers?

Answer: Best practices for designing a Python factory function to abstract and unify API calls for various LLM providers include leveraging Python's dynamic typing and first-class functions to create flexible and clean factories that decouple object creation from the main application logic. This promotes loose coupling and scalability, where the factory is responsible for producing the appropriate API client objects without the rest of the application needing to know the specifics of instantiation or configuration. Using dictionaries or mappings inside factory functions to select classes or functions based on input criteria simplifies implementation and enhances adaptability. This approach ensures that the factory handles all details of object creation, making the codebase cleaner and easier to maintain when supporting multiple LLM providers with potentially different initialization parameters or behaviors[1].

-----

-----

### Source [105]: https://realpython.com/factory-method-python/

Query: What are best practices for designing a Python factory function and configuration model (like Pydantic's BaseModel) to abstract and unify API calls for various LLM providers?

Answer: A robust design for a Python factory function when abstracting API calls for different LLM providers involves implementing an Object Factory pattern that supports registration of new provider implementations dynamically. The factory should maintain a mapping of provider identifiers to their respective creator classes or functions, allowing the application to instantiate the correct client based on configuration or parameters. This design avoids complex conditional logic and supports extensibility by enabling new providers to be added through registration without modifying existing code. Using a class-based factory that provides methods like `register_format()` and `get_serializer()` analogously, you can register new LLM provider clients and retrieve instances as needed. This pattern also facilitates configuration modeling (akin to Pydantic's BaseModel) by clearly defining the expected parameters for each provider, which the factory can validate or enforce during instantiation. Such separation improves maintainability, testability, and the ability to evolve the system as new providers emerge or API interfaces change[2].

-----

-----

### Source [106]: https://python-patterns.guide/gang-of-four/factory-method/

Query: What are best practices for designing a Python factory function and configuration model (like Pydantic's BaseModel) to abstract and unify API calls for various LLM providers?

Answer: When designing a factory function or class to unify API calls for various LLM providers, consider whether users will need to customize object creation parameters. If customization is expected, implement the factory as a parameter in the class's `__init__` method (dependency injection), allowing flexibility for clients to specify configuration or override creation behavior. If customization is rare, the factory can be a class attribute that can be overridden as needed. Python's dynamic capabilities reduce the need for strict Factory Method patterns seen in other languages; instead, passing callables or classes directly as factory functions is a preferred approach. This design facilitates clean abstraction and extensibility, ensuring that the factory can generate API clients configured with the necessary parameters, possibly validated via a configuration model similar to Pydantic's BaseModel[3].

-----

-----

### Source [107]: https://refactoring.guru/design-patterns/factory-method/python/example

Query: What are best practices for designing a Python factory function and configuration model (like Pydantic's BaseModel) to abstract and unify API calls for various LLM providers?

Answer: The Factory Method design pattern in Python is well-suited for scenarios requiring the creation of product objects (such as LLM API clients) without specifying their concrete classes. The best practice involves defining a factory method responsible for creating objects, which subclasses can override to change the exact class instantiated. Applying this to LLM providers, you define a base factory interface or class that returns an API client instance based on configuration, and concrete factories for each provider subclass this to instantiate the appropriate client. This approach encapsulates the creation logic, supports extensibility, and keeps the client code independent of specific provider implementations. Coupling this with a configuration model like Pydantic's BaseModel allows you to validate and standardize the parameters required by each provider's factory, enhancing robustness and clarity[5].

-----

</details>


## Sources Scraped From Research Results

<details>
<summary>The provided markdown content does not align with the article guidelines for "L22 — Implementing the foundations of Brown the writing workflow". The content focuses on a generic tutorial about structuring LLM outputs with Pydantic, whereas the guidelines describe a specific lesson about building a writing workflow, applying context engineering principles, and implementing the orchestrator-worker pattern for media items within that workflow.</summary>

The provided markdown content does not align with the article guidelines for "L22 — Implementing the foundations of Brown the writing workflow". The content focuses on a generic tutorial about structuring LLM outputs with Pydantic, whereas the guidelines describe a specific lesson about building a writing workflow, applying context engineering principles, and implementing the orchestrator-worker pattern for media items within that workflow.

Therefore, no content from the provided markdown is pertinent to the given article guidelines.

</details>

<details>
<summary>From Chaos to Control: Mastering LLM Outputs with LangChain & Pydantic</summary>

# From Chaos to Control: Mastering LLM Outputs with LangChain & Pydantic

This practical guide shows developers how to implement type safety and automatic validation, turning unpredictable language model responses into production-ready Python objects.

# 1. Introduction

Working with Large Language Models (LLMs) like GPT-4 can feel like trying to have a conversation with a brilliant but sometimes chaotic professor. While they provide incredibly sophisticated responses, the free-form nature of these outputs can be a developer's nightmare when building production applications. How do you reliably extract specific fields? What happens when the format changes unexpectedly? How can you ensure the data types match your application's requirements? Enter the powerful combination of LangChain and Pydantic - a duo that brings structure and reliability to the wild world of LLM outputs. In this tutorial, we'll explore how to transform unpredictable LLM responses into strongly-typed, validated data structures that seamlessly integrate with your Python applications.

# 2. The Power of Structured Outputs

Working with raw LLM outputs is like trying to parse a JSON string that's been written by a human - it might look right at first glance, but it's prone to inconsistencies and errors. Here's what typically goes wrong:

#### 1. Inconsistent Formatting

- One response might return a list as "1, 2, 3"
- Another might return it as "\[1,2,3\]"
- And yet another as "1\n2\n3"

#### 2. Error-Prone Parsing

- String manipulation to extract fields
- Regular expressions that break with slight variations
- Manual type conversion and validation

#### 3. Integration Headaches

- No type hints for your IDE
- Runtime errors when formats don't match
- Difficulty maintaining code that depends on output format

By using structured outputs with Pydantic models, we transform this chaos into order:

- **Type Safety**: Every field has a defined type that's checked at runtime
- **Automatic Validation**: Values are validated against your rules before they reach your application code
- **IDE Support**: Full autocomplete and type hints for your structured data
- **Clear Contract**: The model serves as documentation for what data to expect
- **Easy Integration**: Direct compatibility with FastAPI, SQLAlchemy, and other modern Python tools

Think of it like having a strict but helpful assistant that ensures every piece of data from your LLM fits exactly where it should in your application. Let's see how to build this in practice...

## 2.1 The Perils of DIY JSON Parsing with LLMs

Let's start with a common approach many developers take when first working with LLMs - asking for JSON directly in the prompt:

```python
def get_raw_json_prompt(review: str) -> str:
    return f"""
		Analyze this product review and return a JSON with fields:
		- sentiment (string)
		- pros (list of strings)
		- cons (list of strings)

		Review: "{review}"

		Return only the JSON, no other text.
		"""
```

While this might seem to work at first, it's a fragile solution. Some days you'll get perfectly formatted JSON:

```json
{
    "sentiment": "positive",
    "pros": ["Great battery", "Fast performance"],
    "cons": ["Expensive", "Heavy"]
}
```

But other times, you might get:

```text
Here's the analysis:
{sentiment: "positive", pros: ["Great battery", "Fast performance"], cons: ["Expensive", "Heavy"]}
```

Or even:

```text
The sentiment is positive
Pros:
- Great battery
- Fast performance
Cons:
- Expensive
- Heavy
```

As your needs grow, you might try to get more sophisticated with your JSON structure:

```python
def get_complex_json_prompt(review: str) -> str:
    return f"""
		Analyze this product review and return a JSON with the following structure:
		{{
		    "overall_sentiment": "string (positive/negative/mixed)",
		    "summary": "brief summary string",
		    "main_pros": ["list", "of", "strings"],
		    "main_cons": ["list", "of", "strings"],
		    "aspects": [\
		        {{\
		            "aspect": "specific feature or aspect name",\
		            "sentiment": "positive/negative/neutral",\
		            "confidence": "float between 0 and 1",\
		            "relevant_text": "quoted text from review that supports this"\
		        }}\
		    ]
		}}
		...
		"""
```

Now we're really in trouble. Not only do we have all the previous problems, but we also have to deal with:

- Nested structures that might be malformed
- Type mismatches (getting strings instead of floats for confidence scores)
- Missing fields in nested objects
- Inconsistent field names
- No type hints for your IDE
- Complex error handling

## 2.2 Enter Pydantic: Your Type Safety Savior

Instead of fighting with JSON parsing, let's define our expectations clearly with Pydantic models:

```python
from pydantic import BaseModel, Field
from typing import List

class SentimentAspect(BaseModel):
    aspect: str = Field(description="The specific aspect being discussed (e.g., durability, ease of use)")
    sentiment: str = Field(description="The sentiment (positive/negative/neutral)")
    confidence: float = Field(description="Confidence score between 0 and 1")
    relevant_text: str = Field(description="The specific text that supports this analysis")

class ReviewAnalysis(BaseModel):
    overall_sentiment: str = Field(description="Overall sentiment of the review")
    aspects: List[SentimentAspect] = Field(description="List of analyzed aspects")
    main_pros: List[str] = Field(description="Main positive points")
    main_cons: List[str] = Field(description="Main negative points")
    summary: str = Field(description="Brief summary of the review")
```

This approach gives us:

**1. Type Safety**: Each field has a defined type that's validated automatically

```python
# This will fail if confidence isn't a float between 0 and 1
aspect = SentimentAspect(
    aspect="battery",
    sentiment="positive",
    confidence="not a number",# TypeError!
    relevant_text="Great battery life"
```

**2. Clear Contract**: The model serves as documentation and a schema

```python
# Your IDE will know exactly what fields are available
analysis = ReviewAnalysis(...)
print(analysis.overall_sentiment)# Auto-complete works!
```

**3. Validation Out of the Box**: Pydantic handles all the edge cases

```undefined
# Missing fields? Pydantic will tell you exactly what's wrong
# Wrong types? You'll get clear error messages# Nested structures? No problem!
```

**4. Integration Ready**: The models work seamlessly with modern Python tools

```python
# Works great with FastAPI
@app.post("/analyze")
def analyze(review: ReviewAnalysis):
    return review

# Easy database integration
review_dict = analysis.dict()# Convert to dict for storage
```

When combined with LangChain's output parsers, this approach ensures your LLM outputs will always match your application's expectations. No more crossing your fingers hoping the JSON is valid - you get guaranteed structure every time.

# 3. Understanding How Pydantic Shapes Your Prompts

When working with LLMs, getting the output format right is crucial. Let's look at how Pydantic models influence the actual prompt that gets sent to the model.

Remember our Pydantic models? Here's how they get transformed into instructions for the LLM:

1.  **Model Definition**: We start with our Pydantic models that define the structure we want
2.  **Schema Generation**: The LangChain parser takes these models and automatically generates a JSON schema that gets inserted into your prompt.
3.  **Integration with Prompts**: When we use the pipe syntax:

```python
    prompt = PromptTemplate(
        template="""
        You are an expert product review analyzer. Analyze the following review in detail.
        Break down each major aspect mentioned and provide a thorough analysis.

        Review: {review}

        {format_instructions}

        Analyze the review following the exact format specified above.
        Make sure to identify distinct aspects and provide specific evidence from the text.
        """,
        input_variables=["review"],
        partial_variables={"format_instructions": parser.get_format_instructions()}
    )

chain = prompt | llm | parser
```

The image shows how our Pydantic models get converted into a detailed JSON schema specification. Notice how each field's description, type, and requirements are preserved and formatted in a way the LLM can understand.

The parser injects these format instructions automatically into your prompt template wherever you use the `{format_instructions}` placeholder.

**Why This Matters**:

- The LLM gets precise instructions about the expected output format
- Each field's purpose is clearly defined through descriptions
- Required fields are explicitly marked
- Type constraints are preserved (like confidence being a number between 0 and 1)
- The schema provides a contract that both the LLM and your code understand

This structured approach means you spend less time fighting with output parsing and more time building features. The LLM knows exactly what format to return, and your application gets strongly-typed data it can trust.

> Warning Important Note: The effectiveness of structured output heavily depends on the underlying LLM model you're using. While this tutorial demonstrates examples using GPT-4, different models may require different prompting strategies or might have varying capabilities in following structured output instructions.
>
> If you're:
>
> Using a different LLM provider (like Anthropic, Llama, etc.)
>
> Not getting the expected structured outputs
>
> Need more advanced parsing strategies
>
> Want to explore different output formats
>
> Please refer to LangChain's [official documentation](https://python.langchain.com/docs/how_to/structured_output/) on structured outputs:
>
> This resource provides model-specific guidance and alternative approaches for handling structured outputs.

## 3.1 Supercharging Your Models with Examples and Schemas

While basic Pydantic models work well, we can make them even more powerful by adding examples and extra schema information. This helps the LLM understand exactly what we expect:

```python
class SentimentAspect(BaseModel):
    aspect: str = Field(
        description="The specific feature or aspect being discussed",
        examples=["battery life", "screen quality", "keyboard feel", "performance"]
    )
    sentiment: str = Field(
        description="The sentiment towards this aspect",
        examples=["positive", "negative", "neutral"]
    )
    confidence: float = Field(
        description="Confidence score for this sentiment analysis",
        examples=[0.95, 0.87, 0.76],
        ge=0,# greater than or equal to 0
        le=1# less than or equal to 1
    )
    relevant_text: str = Field(
        description="The specific quote from the review that supports this analysis",
        examples=[\
            "battery life is incredible, lasting over 8 hours",\
            "screen has significant glare issues in sunlight"\
        ]
    )

# Complete example of how fields work together
    class Config:
        json_schema_extra = {
            "example": {
                "aspect": "battery life",
                "sentiment": "positive",
                "confidence": 0.95,
                "relevant_text": "battery life is incredible, lasting over 8 hours"
            }
        }
```

### Why Add Examples?

**Field-Level Examples**:

```python
aspect: str = Field(
    description="The specific feature or aspect being discussed",
    examples=["battery life", "screen quality", "keyboard feel"]
)
```

- Shows valid values for individual fields
- Helps the LLM understand the expected granularity
- Demonstrates the preferred formatting

**Complete Object Examples** (using `json_schema_extra`):

```python
class Config:
    json_schema_extra = {
        "example": {
            "aspect": "battery life",
            "sentiment": "positive",
            "confidence": 0.95,
            "relevant_text": "battery life is incredible..."
        }
    }
```

- Shows how fields relate to each other
- Demonstrates complete, valid objects
- Helps LLM understand the overall structure

### When to Use What?

#### Use Field Examples When:

- You want to show valid values for a specific field
- The field has a limited set of possible values
- You want to demonstrate formatting patterns

```python
sentiment: str = Field(
    description="The sentiment towards this aspect",
    examples=["positive", "negative", "neutral"]# Clear, limited options
)
```

#### Use json\_schema\_extra When:

- You need to show how fields work together
- The relationships between fields matter
- You want to demonstrate complete, valid objects

```python
class Config:
    json_schema_extra = {
        "example": {
            "overall_sentiment": "positive",
            "aspects": [\
                {\
                    "aspect": "battery life",\
                    "sentiment": "positive",\
                    "confidence": 0.95,\
                    "relevant_text": "battery life is incredible..."\
                },\
# Multiple examples show different scenarios\
                {\
                    "aspect": "screen quality",\
                    "sentiment": "mixed",\
                    "confidence": 0.85,\
                    "relevant_text": "screen is crisp but has glare..."\
                }\
            ],
            "main_pros": ["Excellent battery life", "Fast performance"],
            "main_cons": ["Screen glare issues"],
            "summary": "A positive review highlighting battery life..."
        }
    }
```

### Real-World Impact

Adding these examples:

1.  **Improves Consistency**: The LLM learns your preferred formatting and terminology
2.  **Reduces Errors**: Clear examples prevent misunderstandings
3.  **Better First-Time Success**: LLM is more likely to get it right the first time
4.  **Clearer Documentation**: Your code becomes self-documenting

This becomes especially important in production systems where consistent output format is crucial for downstream processing.

## 4. Conclusion: Power and Pitfalls of Structured Outputs

Working with LLMs can feel like trying to have a conversation where both participants speak different languages. Through this tutorial, we've seen how the combination of LangChain and Pydantic brings structure and reliability to these conversations by:

1.  Converting unpredictable free-form text into strongly-typed Python objects
2.  Providing clear validation and error handling
3.  Making our code more maintainable and type-safe
4.  Enabling better integration with existing systems

Remember that while structured outputs give us more control and reliability, they also add complexity to our prompts. It's a trade-off worth making when you need guaranteed format compliance, but might be overkill for simpler use cases where free-form text would suffice.

The key is to choose the right tool for the job:

- Need strict type safety and validation? Use Pydantic models
- Want simple key-value extraction? Consider simpler parsers
- Building a production system? Structured outputs are your friend
- Just prototyping? Maybe start with basic prompts and add structure as needed

By understanding these patterns and pitfalls, you can build more robust and reliable LLM-powered applications that are easier to maintain and debug.

Happy prompting! 🚀

</details>

<details>
<summary>The provided markdown content is about "How to Use Pydantic for LLMs," which covers schema, validation, and prompt descriptions with code examples using Pydantic and OpenAI.</summary>

The provided markdown content is about "How to Use Pydantic for LLMs," which covers schema, validation, and prompt descriptions with code examples using Pydantic and OpenAI.

The article guidelines, however, describe Lesson 22 of a course on AI agents and LLM workflows, specifically titled "Implementing the foundations of Brown the writing workflow." This lesson focuses on topics like the orchestrator-worker pattern, context engineering for writing articles, domain entities for LLM context, writing profiles, unifying LLM calls, and specific nodes like `MediaGeneratorOrchestrator` and `ArticleWriter`.

Given the instruction: "Focus on keeping only the core textual content (and code content if there are code sections) that is pertinent to the article guidelines provided below," and the complete mismatch between the actual content (Pydantic for LLMs) and the subject matter outlined in the article guidelines (Brown writing workflow), **none** of the core textual or code content from the scraped markdown is pertinent to the provided guidelines.

Therefore, after removing the introductory/metadata elements and concluding promotional/related content, the entire body of the article also needs to be removed as it is irrelevant to the specified lesson guidelines.

```markdown

```

</details>

<details>
<summary>Orchestrator-Worker Architecture</summary>

# Orchestrator-Worker Architecture

The orchestrator-worker architecture is designed for workflows where problem structure emerges at runtime. A user input **S** is passed to an orchestrator agent **O**, which transforms it into a multiset of subtasks {s1,…,sn}. Each subtask si is routed to a corresponding worker agent Wi, which returns a local result ri.  The orchestrator observes these results and dynamically determines the next best subtasks or actions, repeating this loop until the final output **R** is produced.

This approach is distinct from prompt chaining or fixed tool routing. It operates under different assumptions:

- The number and type of subtasks depend on the properties of the input, not on a predefined flow.
- Subtasks may vary in complexity and require different strategies or tools.
- Workers may operate in parallel or sequentially; in some cases, specific models or tools are assigned to subtasks based on their content.

The orchestrator must go beyond simple decomposition. It needs **multi-stage reasoning**, maintaining a representation of the evolving task graph, tracking which nodes are resolved, deciding when partial results require refinement, and determining when to terminate or recurse.

Execution often unfolds in **cycles**: generate subtasks, assign workers, collect results, update state, and repeat until the goal is reached.

## Challenges

When building an orchestrator–worker agent framework, several key challenges emerge:

- **Dynamic Routing**: Deciding which worker agent should handle each subtask requires balancing workflow state, subtask complexity, and worker specialization.
- **Context Continuity**: Agents need to remember what has been decided so far, even across multiple cycles.
- **Seamless Handoffs**: Tasks must transfer between agents without losing context or duplicating work.
- **Error Handling**: The system must detect failures and recover gracefully without derailing the workflow.
- **Concurrency**: Orchestrators must schedule and run agents efficiently, whether tasks are independent or interdependent.
- **Memory Management**: Managing information across cycles and agents is especially complex. This includes:
  - **Short-term vs. Long-term State**: Distinguishing transient scratchpad data from persistent decisions.
  - **Context Propagation**: Passing only what’s necessary between agents to avoid bloat.

  - **Storage Patterns**: Using in-prompt embeddings, caches, or external stores depending on scale and reliability needs.

We compared common agent frameworks by building minimal orchestrator–worker prototypes in each. The purpose of this comparison is to highlight how the same high-level architecture maps onto different runtimes. We enabled only native routing, handoff, memory, and retry features, avoiding custom heuristics except where required for basic functionality. We inspected execution traces and code to evaluate **four pillars** and then summarized tradeoffs into a concise verdict for each framework.

## Architecture

Multi-agent frameworks may look different on the surface, but under the hood they all solve the same orchestration challenge: how to break down a top-level task, decide which agent should act next, and combine results into a coherent output. Each framework encodes this logic into two main primitives: Orchestrator and Worker.

Some frameworks, like **AutoGen**, lean into chat-style coordination where agents “talk” until a result emerges. Others, like **LangGraph** or **Mastra**, formalize orchestration as graphs or workflows. And few frameworks support both, such as **CrewAI**. The table below compares how various frameworks implement the orchestrator and worker roles, and highlights what stands out in each approach.

| Framework | Orchestrator primitive<br>Core control unit that routes work | Worker model<br>How individual agents or tools are represented | What stands out |
| Agno (Python) | Team (with coordinate mode set) decides who acts based on instructions and success criteria | Role agents registered as team members | Declarative team that routes without manual stitching; built-in task transfer tool surfaces handoffs in Phoenix. |
| AutoGen / AG2 (Python) | GroupChat Manager selects the next speaker | **AssistantAgent, UserProxyAgent** (human), other agents inside one chat | Conversation-centric orchestration that emerges from chat history and roles. |
| CrewAI (Python) | **Manager agent** in hierarchical process plans, delegates, validates | Role-based agents executing YAML or code-defined tasks | Structured ‘team with a manager’ model with a detailed event bus for coordination and visibility. |
| OpenAI Agents (Python/JS) | **Runner loop** in Agents SDK | Sub-agents registered for **handoffs** | Handoffs are first-class tool calls with built-in tracing identifiers. |
| LangGraph (Py/TS) | **Execution graph** with nodes and edges | Supervisor, dynamic workers via `Send API`, subgraphs | Graph-native orchestration with hierarchical supervision and modular subgraphs. |
| Mastra (TypeScript) | **Supervisor Agent** plus Workflows for sequencing and branching | Workers as tools or agents | TypeScript-native with structured streaming and OpenTelemetry spans across runs. |

## **Execution Model and Handoffs**

Beyond how agents are orchestrated conceptually, each framework has its own execution loop and rules for handing off control. These mechanics determine whether workflows feel conversational, procedural, or graph-driven, and they also shape how parallelism is supported.

Two dimensions matter most:

- **Execution loop** – the control cycle that advances the system until completion (stepwise coordination, chat turns, or graph traversal).
- **Handoff form** – how one agent (or node) transfers work, context, or control to another.

| Framework | Execution loop _(How steps advance end to end)_ | Handoff form _(How control and context move between agents)_ | Parallelism notes _(Concurrency: branching and join strategy)_ |
| Agno | Team coordinates actions per step using pre-defined rules and instructions. | **Explicit handoffs** via methods like `transfer_task_to_member()`. Handoff includes relevant state and expected output. | Supports concurrent agents working on different sub-tasks. |
| AutoGen | A manager-led conversation where the manager agent selects the next speaker from the group. | **Conversation-driven handoffs**; an agent implicitly hands off to another by generating a response that prompts the manager to select a new speaker. | Agents run sequentially by default; true parallelism needs manual orchestration, and `max_rounds` must be tuned to avoid dead-ends. |
| CrewAI | Central manager delegates tasks to agents and validates their outputs. | Explicit manager-to-agent task handoff with dependencies and shared context. | Supports both in-crew and multi-crew parallelism. |
| OpenAI Agents | The Runner drives tool calls and outputs until completion or stop. | Handoffs are effectively **tool calls**. `handoff_input_filter` can be used to limit the forwarded context. | Execution is sequential by default; broader parallelism requires explicit orchestration. |
| LangGraph | Execution is a directed graph traversal, with nodes as steps and edges defining branching and loops. | **Command API** moves control across parent/child graphs and subgraphs. | The graph natively supports parallel fan-out and structured fan-in with reducers, enabling efficient concurrent execution. |
| Mastra | A supervisor drives workflow execution, routing steps and tool calls; streams progress through a defined Workflow. | Delegation appears as a tool call, with the supervisor routing agent actions and passing context. | Supports parallel steps within a single Workflow. Also supports suspend-resume for long runs with nested streaming. |

## **Memory**

Memory in multi-agent systems can be analyzed along three core aspects:

- **Shared context model** – How information flows across agents. Some frameworks offer a global transcript or team-level state, while others externalize context and require explicit passing.
- **Per-agent or scoped memory** – What each agent retains individually, such as message history, role-specific preferences, allowing specialization without bloating the global context.
- **Durability and persistence** – Whether memory extends beyond a single run. Some frameworks checkpoint state for resumability, others persist embeddings across sessions, and some leave persistence to the developer.

Together, these dimensions shape how well an agent framework maintains continuity, recall, and grounding over time.

| Framework | Shared context model<br>Global state accessible to all agents | Per-agent or scoped memory<br>Private or limited-scope state per agent | Durability and persistence<br>How state survives long runs or restarts |
| Agno | The Team-level shared global context is automatically synced across members. No need for manual context passing. | Each agent can keep scoped memory; plug in vector stores or DBs. | Persistent global context with minimal glue, though highly custom pipelines may bypass defaults. |
| AutoGen | _GroupChat_ transcript is the primary shared context for all agents. | Each agent stores its own chat history and configurations. This allows for specialized, role-based behaviors. | Use `BufferedChatCompletionContext` to manage the context window. External RAG at agent or chat level for more durable memory. |
| CrewAI | Shared context can extend beyond the immediate chat history, allowing the Crew and the Manager to share information. | Short-term, long-term, and entity memory which are configurable with different embedding models. Allows for fine-grained control over agent memory. | Memory persists across runs, ensuring continuity for complex workflows |
| OpenAI Agents | The _session_ is the core mechanism for shared context, preserving the conversation history across runs. | The session is the core mechanism for shared context. When agents run under the same session, they automatically share conversation history across runs. | History can be filtered on handoff between workers to prevent data leakage and manage costs. |
| LangGraph | The framework stores state as a graph of nodes and edges, rather than a single shared transcript. | Each `thread_id` has its own checkpointed state. This enables agents to maintain their individual context within a specific thread. | Checkpointing enables resumability, time travel, and human‑in‑the‑loop interactions via via `interrupt()` and `resume`. |
| Mastra | Context passed via workflows or resource-scoped memory (per user/entity) | Built-in per-agent memory: working memory, message history, semantic recall | Configurable; persists with storage adapters (e.g. LibSQL, Postgres) or ephemeral if none set |

## **Error Handling**

Error handling determines how frameworks recover from failed steps, prevent runaway loops, and surface useful debugging signals. The mechanisms vary widely: some provide guardrails and retries at the task or node level (CrewAI, LangGraph), while others rely on conversation rules or termination checks (AutoGen).

The table below outlines the built-in mechanisms each framework offers, along with known caveats developers should watch for.

| Framework | Built-in mechanisms<br>Native guardrails, retries, and recovery | Known caveats |
| Agno | Team architecture rules reduce custom glue; decision metadata can explain why an agent was chosen | Abstractions could make it difficult to debug race conditions or task collisions in complex, potentially requiring custom orchestration logic to resolve. |
| AutoGen | Termination rules (like a `max_rounds` limit) and a manager-led turn-taking mechanism prevent infinite loops. The **UserProxyAgent** can serve as a human-in-the-loop for error resolution. | The **UserProxyAgent** can get re-selected repeatedly if the termination conditions aren’t met, potentially leading to a long, unproductive conversation. Long chats risk context loss without buffering. |
| CrewAI | Provides robust **guardrails and retries at the task level**. Enforces **structured outputs** to ensure data integrity and includes an event bus for real-time observability. | Enables teams to carefully design agent roles and structured output to ensure **clarity, consistency, and easier debugging** in complex workflows |
| OpenAI Agents | Features include **guardrails** and **tripwires** to prevent unwanted behavior. Tracing can show the flow of a conversation and who handled each part of a handoff. | Rationale behind the assistant’s routing or tool selection can be opaque. Parallelism is supported but requires explicit orchestration with asyncio or SDK helpers. |
| LangGraph | Offers node-level retries, checkpoint-based recovery, and selective resume (e.g. restart only the failed part after an error). | Developers must explicitly define **reducers** and **termination rules** to prevent infinite cycles or state conflicts. |
| Mastra | Delegations appear as tool calls within the same run; supports structured streaming, suspend/resume, and OpenTelemetry tracing | Developers can define orchestration and memory/error strategies explicitly. This adds flexibility, allowing teams to **tailor error handling to their system needs** instead of being locked into a fixed model |

## **Verdict**

- **CrewAI**: Event-driven manager–worker framework with strong guardrails, retries, and observability. Its event bus and async execution enable meaningful parallelism, while flexible memory types support continuity across runs. Well-suited for production teams that value transparency, control, and scalable coordination.

- **Mastra**: Strong TypeScript-native framework with built-in streaming, suspend-resume, and OpenTelemetry tracing. Provides a structured memory system (working memory, message history, semantic recall) with defaults for persistence, while allowing developers to configure custom strategies. Offers clear operational visibility and flexible orchestration, giving developers both strong defaults and the freedom to shape state management to their needs.
- **Agno**: Agno’s strength lies in its declarative coordination: Teams reduce custom glue by enforcing orchestration rules and surfacing handoffs. Memory is simple but persistent, and built-in decision metadata aids transparency. Agno suits developers who want low-friction coordination, though some customization is required for more complex logic.
- **OpenAI Agents**: Easiest way to get native handoffs, sessions, and guardrails if you are already on the OpenAI stack. This framework is ideal for developers who want tight integration with the OpenAI ecosystem, prioritizing structured APIsover maximal flexibility.
- **LangGraph**: Best when you want precise control, parallel fan‑out, recursion, and durable state management. Memory is externalized into a checkpointed state, enabling resumability, “time travel,” and human-in-the-loop interventions.
- **AutoGen / AG2**: AutoGen stands out for its conversation-centric orchestration, where roles and chat history drive emergent behavior. This makes it highly flexible for research use cases. Handoffs between agents rely on the shared conversation context, which can be truncated in long chats, and memory can be managed at both the agent and orchestrator level for continuity.

</details>


## Code Sources

<details>
<summary>Repository analysis for https://github.com/towardsai/agentic-ai-engineering-course/blob/dev/lessons/22_foundations_writing_workflow/notebook.ipynb</summary>

# Repository analysis for https://github.com/towardsai/agentic-ai-engineering-course/blob/dev/lessons/22_foundations_writing_workflow/notebook.ipynb

## Summary
Repository: towardsai/agentic-ai-engineering-course
Branch: dev
Commit: 932c80d96e8a43667dbc9ca063ce82bf73156d6c
File: notebook.ipynb
Lines: 3,255

Estimated tokens: 24.3k

## File tree
```Directory structure:
└── notebook.ipynb

```

## Extracted content
================================================
FILE: lessons/22_foundations_writing_workflow/notebook.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Lesson 22: Foundations of the Writing Workflow — Brown Agent

In this lesson, we'll dive deep into the Brown writing agent, a sophisticated system designed to generate high-quality technical articles about AI. This lesson focuses on the foundational components such as:

**Learning Objectives:**

- Understand context engineering techniques for manipulating multiple inputs, structure and reason accordingly
- Learn the orchestrator-worker pattern for generating media assets like Mermaid diagrams
- Master entity modeling using Pydantic to structure guidelines, research, profiles, and media
- Build custom workflow nodes for building modular components that can be reused across the application

"""

"""
## 1. Setup

First, we define some standard Magic Python commands to autoreload Python packages whenever they change:

"""

%load_ext autoreload
%autoreload 2

"""
### Set Up Python Environment

To set up your Python virtual environment using `uv` and load it into the Notebook, follow the step-by-step instructions from the `Course Admin` lesson from the beginning of the course.

**TL/DR:** Be sure the correct kernel pointing to your `uv` virtual environment is selected.

"""

"""
### Configure Gemini API

To configure the Gemini API, follow the step-by-step instructions in the `Course Admin` lesson.

Here is a quick checklist of what you need to run this notebook:

1.  Get your key from [Google AI Studio](https://aistudio.google.com/app/api-keys).
2.  From the root of your project, run: `cp .env.example .env` 
3.  Within the `.env` file, fill in the `GOOGLE_API_KEY` variable:

Now, the code below will load the key from the `.env` file:
"""

from utils import env

env.load(required_env_vars=["GOOGLE_API_KEY"])
# Output:
#   Environment variables loaded from `/Users/pauliusztin/Documents/01_projects/TAI/agentic-ai-engineering-course/.env`

#   Environment variables loaded successfully.


"""
### Import Key Packages

"""

import nest_asyncio
from utils import pretty_print

nest_asyncio.apply()  # Allow nested async usage in notebooks

"""
### Download Required Files

We need to download the configuration files and input data that Brown uses for article generation and editing.

First, let's download the configs folder:
"""

%%capture

!rm -rf configs
!curl -L -o configs.zip https://raw.githubusercontent.com/iusztinpaul/agentic-ai-engineering-course-data/main/data/configs.zip
!unzip configs.zip
!rm -rf configs.zip

"""
Now, let's download the inputs folder containing profiles, examples, and test data:
"""

%%capture

!rm -rf inputs
!curl -L -o inputs.zip https://raw.githubusercontent.com/iusztinpaul/agentic-ai-engineering-course-data/main/data/inputs.zip
!unzip inputs.zip
!rm -rf inputs.zip

"""
Let's verify what we downloaded:
"""

%ls
# Output:
#   article_guideline.md   [1m[36minputs[m[m/                notebook_guideline.md

#   [1m[36mconfigs[m[m/               notebook.ipynb


"""
### Set Up Directory Constants

Now let's define constants to reference these directories throughout the notebook:
"""

from pathlib import Path

CONFIGS_DIR = Path("configs")
INPUTS_DIR = Path("inputs")

print(f"Configs directory exists: {CONFIGS_DIR.exists()}")
print(f"Inputs directory exists: {INPUTS_DIR.exists()}")
# Output:
#   Configs directory exists: True

#   Inputs directory exists: True


EXAMPLES_DIR = Path("inputs/examples/course_lessons")
PROFILES_DIR = Path("inputs/profiles")

print(f"Examples directory exists: {EXAMPLES_DIR.exists()}")
print(f"Profiles directory exists: {PROFILES_DIR.exists()}")
# Output:
#   Examples directory exists: True

#   Profiles directory exists: True


"""
First, we will load a simpler example that runs faster and is easier to understand. At the end, we will load a larger sample that is closer to what we do on our end to generate professional articles:
"""

SAMPLE_DIR = Path("inputs/tests/01_sample_small")

print(f"Samples directory exists: {SAMPLE_DIR.exists()}")
# Output:
#   Samples directory exists: True


"""
### Understanding the Brown Package

Throughout this notebook, we'll be importing code from the `brown` package. This package contains all the code for the writing workflow which is located in the `lessons/writing_workflow/`. 

The Brown package is installed as a local Python package using `uv` and contains all the core functionality we'll explore:

- **Entities** (`brown.entities`): Pydantic models for articles, guidelines, research, profiles, and media
- **Nodes** (`brown.nodes`): Workflow components like ArticleWriter and MediaGeneratorOrchestrator
- **Models** (`brown.models`): LLM configuration and initialization utilities
- **Loaders** (`brown.loaders`): Classes to load markdown content into entities
- **Builders** (`brown.builders`): Factory patterns for creating loaders, renderers, and models
- **Renderers** (`brown.renderers`): Classes to save entities back to markdown files
- **Config** (`brown.config`): Settings management using Pydantic

Let's start by understanding how we load environment variables and configure the system.

> [!NOTE]
> 💡 You can also run `brown` as a standalone Python package by going to `lessons/writing_workflow/` and following the instructions from there.

"""

"""
## 2. Configuration and Settings

The Brown agent uses a centralized settings system built with Pydantic's `BaseSettings` to load all sensitive credentials, such as API Keys, from a centralized class. This ensures that all your credentials are stored within a local `.env` file or in memory, while it leverages all the type safety features that come with Pydantic.

Let's examine the `Settings` class from `brown.config`:

```python
import os
from functools import lru_cache
from typing import Annotated

from loguru import logger
from pydantic import Field, FilePath, SecretStr
from pydantic_settings import BaseSettings, SettingsConfigDict

ENV_FILE_PATH = os.getenv("ENV_FILE_PATH", ".env")
logger.info(f"Loading environment file from `{ENV_FILE_PATH}`")


class Settings(BaseSettings):
    model_config = SettingsConfigDict(env_file=ENV_FILE_PATH, extra="ignore", env_file_encoding="utf-8")

    # --- Gemini ---
    GOOGLE_API_KEY: SecretStr | None = Field(default=None, description="The API key for the Gemini API.")

    # --- Opik ---
    OPIK_ENABLED: bool = Field(default=False, description="Whether to use Opik for monitoring and logging.")
    OPIK_WORKSPACE: str | None = Field(default=None, description="Name of the Opik workspace containing the project.")
    OPIK_PROJECT_NAME: str = Field(default="brown", description="Name of the Opik project.")
    OPIK_API_KEY: SecretStr | None = Field(default=None, description="The API key for the Opik API.")

    # --- App Config ---
    CONFIG_FILE: Annotated[FilePath, Field(default="configs/course.yaml", description="Path to the application configuration YAML file.")]


@lru_cache(maxsize=1)
def get_settings() -> Settings:
    return Settings()
```

When the `Settings` class is instantiated, it first looks for environment variables in memory, then in the `.env` file. Thus, while working with these Notebooks, it's enough to have them only inside the memory, which is taken care of by the `env.load` function from the setup section.

But if you run the Notebook locally, you can also keep them inside a local `.env` file. The choice is yours. 

**Key Design Patterns:**

1. **Singleton Pattern**: The `@lru_cache(maxsize=1)` decorator ensures we only instantiate `Settings` once, making it behave like a singleton throughout the application.

2. **Type Safety**: Using `SecretStr` for sensitive data like API keys ensures they're handled securely and not accidentally logged.

3. **Environment Integration**: `SettingsConfigDict` automatically loads values from the `.env` file, with sensible defaults for missing values.

4. **Flexible Configuration**: The `CONFIG_FILE` field points to a YAML configuration that controls workflow behavior, model selection, and more. More on this in Lesson 23.
"""

"""
## 3. How the Writing Agent Works: High-Level Overview

Before diving into the implementation details, let's understand the three-step workflow that powers the Brown writing agent.

### The Three-Step Process

**Step 1: Load Context into Memory**

The first step involves gathering all the necessary context that will guide the article generation process:

- **Article Guideline**: The user's input describing what the article should contain, its outline, and any specific requirements
- **Research**: Factual data, references, and information that will support the article's claims
- **Few-Shot Examples**: Sample articles demonstrating the desired writing style and structure (your training set)
- **Content Generation Profiles**: Specialized profiles that control different aspects of the writing:
  - Character profile (voice and perspective)
  - Article profile (article-specific rules)
  - Structure profile (formatting rules)
  - Mechanics profile (writing mechanics)
  - Terminology profile (word choice)
  - Tonality profile (tone and style)

**Step 2: Generate Media Items (Orchestrator-Worker Pattern)**

Once we have the context, based on the article guideline, we need to generate any required media assets like diagrams or charts. This step uses the orchestrator-worker pattern:

- The **MediaGeneratorOrchestrator** analyzes the article guideline and research to identify what media items need to be generated
- For each identified requirement, the orchestrator delegates to specialized **worker tools** (e.g., `MermaidDiagramGenerator`)
- Workers generate their specialized content type in parallel
- All generated media items are collected and prepared for integration into the article

**Step 3: Write the Article**

With all context and media items ready, the **ArticleWriter** node generates the final article:

- Takes all the context from Step 1
- Integrates the media items from Step 2
- Follows all profile rules to generate content matching the desired style
- Produces a complete, high-quality article

Let's visualize this workflow with a diagram:

<img src="https://raw.githubusercontent.com/iusztinpaul/agentic-ai-engineering-course-data/main/images/l22_writing_workflow.png" alt="Workflow" height="800"/>
"""

"""
Now that we understand the high-level workflow, let's dive into the implementation details of each component, starting with how we structure and load the context.
"""

"""
## 4. Context Components: Understanding Entities

Now that we understand the high-level workflow, let's explore how we model and load the various context components. 

The Brown writing workflow uses Pydantic models (entities) to represent different types of content, and specialized loaders to read this content from disk. 

Everything within the domain layer, such as the article guideline, research, writing profiles or few-shot examples, is modeled as an entity. Like this, we can very easily attach functionality to each type, structure it and add data validation. As we presented in the project structure lesson, these entities will be passed all over the AI application. They will be used as inputs and outputs to all our business logic. They will be the only way to pass information between functions, classes and between LLMs (software 3.0) and our Python code (software 1.0).

"""

"""
### The ContextMixin: Foundation for Context Engineering

Before we dive into specific entities, let's understand the `ContextMixin` - a crucial abstraction that enables powerful context engineering throughout the system.

The `ContextMixin` provides a standardized way to convert any entity into a context representation surrounded by XML tags. This is essential for:

1. **Clear Boundaries**: XML tags clearly delineate different pieces of context in prompts
2. **Structured Prompts**: LLMs can easily parse and understand structured XML context
3. **Consistent Interface**: All entities follow the same pattern for converting Python objects to LLM input context
4. **Easy Composition**: Different context elements can be seamlessly combined

Here's the implementation from `brown.entities.mixins`:

```python
from abc import abstractmethod
from brown.utils.s import camel_to_snake


class ContextMixin:
    @property
    def xml_tag(self) -> str:
        return camel_to_snake(self.__class__.__name__)

    @abstractmethod
    def to_context(self) -> str:
        """Context representation of the object."""
        pass
```

**Key Features:**

- **Automatic Tag Generation**: The `xml_tag` property automatically converts the class name to snake_case (e.g., `ArticleGuideline` → `article_guideline`)
- **Abstract Method**: `to_context()` must be implemented by each entity, ensuring consistency
- **Simplicity**: The `ContextMixin` is a simple interface, that standardizes how we map Python objects to LLM context inputs

For example, an `ArticleGuideline` entity will be wrapped in `<article_guideline>...</article_guideline>` tags when passed to the LLM.

"""

"""
### Article Guideline Entity

The `ArticleGuideline` represents the user's input - what they want the article to contain, how it should be structured, and any specific requirements. It's the primary driver of content generation that is different for each article.

From `brown.entities.guidelines`:
```python
from pydantic import BaseModel
from brown.entities.mixins import ContextMixin


class ArticleGuideline(BaseModel, ContextMixin):
    content: str

    def to_context(self) -> str:
        return f"""
<{self.xml_tag}>
    <content>{self.content}</content>
</{self.xml_tag}>
"""

    def __str__(self) -> str:
        return f"ArticleGuideline(len_content={len(self.content)})"
```

⚠️ Note how we implemented the `to_context()` method.

The guideline typically contains:
- Article outline and section structure
- Specific instructions for each section
- Length constraints or requirements
- Important references to prioritize
- Any special formatting needs

Let's load the sample article guideline:

"""

from brown.loaders import MarkdownArticleGuidelineLoader

guideline_loader = MarkdownArticleGuidelineLoader(uri=Path("article_guideline.md"))
article_guideline = guideline_loader.load(working_uri=SAMPLE_DIR)

# Output:
#   [32m2025-11-29 17:55:09.155[0m | [1mINFO    [0m | [36mbrown.config[0m:[36m<module>[0m:[36m10[0m - [1mLoading environment file from `.env`[0m


"""
We will show only the first 1200 characters to see how the enclosing XML tags logic works:
"""

cropped_article_guideline = article_guideline.model_copy()
cropped_article_guideline.content = f"{cropped_article_guideline.content[:1200]}..."

pretty_print.wrapped(f"{cropped_article_guideline.content}", title="Article Guideline (First 1200 Chars)", width=150)
# Output:
#   [93m-------------------------------------------------------- Article Guideline (First 1200 Chars) --------------------------------------------------------[0m

#     ## Outline

#   

#   1. Introduction: The Critical Decision Every AI Engineer Faces

#   2. Understanding the Spectrum: From Workflows to Agents

#   3. Choosing Your Path

#   4. Conclusion: The Challenges of Every AI Engineer

#   

#   ## Section 1 - Introduction: The Critical Decision Every AI Engineer Faces

#   

#   - **The Problem:** When building AI applications, engineers face a critical architectural decision early in their development process. Should they create a predictable, step-by-step workflow where they control every action, or should they build an autonomous agent that can think and decide for itself? This is one of the key decisions that will impact everything from the product such as development time and costs to reliability and user experience.

#   - Quick walkthrough of what we'll learn by the end of this lesson

#   

#   - **Section length:** 100 words

#   

#   ## Section 2 - Understanding the Spectrum: From Workflows to Agents

#   

#   - In this section we want to take a brief look at what LLM workflows and AI agents are. At this point we don't focus on the technical specifics of each, but rather on their properties and how they are used.

#   - On **LLM workflows** we care about:

#   	- Definition: A sequence of tasks involving LLM call...

#   [93m------------------------------------------------------------------------------------------------------------------------------------------------------[0m


"""
Let's also call the `to_context()` method:
"""

pretty_print.wrapped(f"{cropped_article_guideline.to_context()}", title="Article Guideline as Context")
# Output:
#   [93m----------------------------------- Article Guideline as Context -----------------------------------[0m

#     

#   <article_guideline>

#       <content>## Outline

#   

#   1. Introduction: The Critical Decision Every AI Engineer Faces

#   2. Understanding the Spectrum: From Workflows to Agents

#   3. Choosing Your Path

#   4. Conclusion: The Challenges of Every AI Engineer

#   

#   ## Section 1 - Introduction: The Critical Decision Every AI Engineer Faces

#   

#   - **The Problem:** When building AI applications, engineers face a critical architectural decision early in their development process. Should they create a predictable, step-by-step workflow where they control every action, or should they build an autonomous agent that can think and decide for itself? This is one of the key decisions that will impact everything from the product such as development time and costs to reliability and user experience.

#   - Quick walkthrough of what we'll learn by the end of this lesson

#   

#   - **Section length:** 100 words

#   

#   ## Section 2 - Understanding the Spectrum: From Workflows to Agents

#   

#   - In this section we want to take a brief look at what LLM workflows and AI agents are. At this point we don't focus on the technical specifics of each, but rather on their properties and how they are used.

#   - On **LLM workflows** we care about:

#   	- Definition: A sequence of tasks involving LLM call...</content>

#   </article_guideline>

#   

#   [93m----------------------------------------------------------------------------------------------------[0m


"""
Note how the whole content is surrounded by the `<article_gudeline>...</article_guideline>` XML tags, while the content attribute is surrounded by the `<content>...</content>` XML tag.

We could always pass the `content` field directly as a plain string. If you want to add only one or two entities within the context, that would have worked. But in our use case, we will model up to 10 different context elements, each with its own attributes. Thus, having a clear way to tell the LLM how to differentiate between them, and how to reference them when writing instructions it's an essential skill for context engineering. 

The `to_context()` method maps the Pydantic model to its XML representation, where the root XML tag is inferred from the entity class name, and the fields use the attribute's name directly. Like this, when we pass these entities to an LLM, it can clearly reason what is what, such as separating between different entities and different attributes within an entity.

Now, whenever we write reasoning instructions to the LLM, we can just say "Do X based on <article_gudeline> ..." and it will know to reference the information from these particular XML tags.

We chose XML over JSON because it's easier to read and less verbose, which, on average, translates to fewer input tokens while still being easy for humans to read.
"""

"""
### Research Entity

The `Research` entity loads the research file generated by the Nova deep research agent that contains factual data, references, and information that supports the article's claims. It also extracts and validates image URLs from the research file.

From `brown.entities.research`:

```python
import re
from functools import cached_property

from loguru import logger
from pydantic import BaseModel

from brown.entities.mixins import ContextMixin
from brown.utils.a import asyncio_run, run_jobs
from brown.utils.network import is_image_url_valid


class Research(BaseModel, ContextMixin):
    content: str
    max_image_urls: int = 30

    @cached_property
    def image_urls(self) -> list[str]:
        # Extract image URLs using regex
        image_urls = re.findall(
            r"(?!data:image/)https?://[^\s]+\.(?:jpg|jpeg|png|bmp|webp)",
            self.content,
            re.IGNORECASE,
        )
        # Validate URLs asynchronously by pinging them.
        jobs = [is_image_url_valid(url) for url in image_urls]
        results = asyncio_run(run_jobs(jobs))

        urls = [url for url, valid in zip(image_urls, results) if valid]
        if len(urls) > self.max_image_urls:
            logger.warning(f"Found `{len(urls)} > {self.max_image_urls}` image URLs. Trimming to first {self.max_image_urls}.")
            urls = urls[: self.max_image_urls]

        return urls

    def to_context(self) -> str:
        return f"""
<{self.xml_tag}>
    {self.content}
</{self.xml_tag}>
"""

    def __str__(self) -> str:
        return f"Research(len_content={len(self.content)}, len_image_urls={len(self.image_urls)})"
```

**Advanced Features:**

1. **Image URL Extraction**: Automatically finds image URLs in the research content to manipulate them within the LLM context
2. **URL Validation**: Asynchronously validates that image URLs are accessible by pinging them
3. **Caching**: Uses `@cached_property` to avoid re-extracting URLs
4. **Safety Limits**: Caps the number of images to prevent context overflow

The extracted image URLs can be passed to multimodal models (like Gemini) along with text prompts for richer context.

Let's load the research data:

"""

from brown.loaders import MarkdownResearchLoader

research_loader = MarkdownResearchLoader(uri=Path("research.md"))
research = research_loader.load(working_uri=SAMPLE_DIR)

"""
As before let's load just the first 1500 characters to see everything within the Notebook:
"""

cropped_research = research.model_copy()
cropped_research.content = f"{cropped_research.content[:1200]}..."

pretty_print.wrapped(f"{cropped_research.content}", title="Research (First 1200 Chars)")
# Output:
#   [93m----------------------------------- Research (First 1200 Chars) -----------------------------------[0m

#     # Research

#   

#   ## Code Sources

#   

#   <details>

#   <summary>Repository analysis for https://github.com/google-gemini/gemini-cli/blob/main/README.md</summary>

#   

#   # Repository analysis for https://github.com/google-gemini/gemini-cli/blob/main/README.md

#   

#   ## Summary

#   Repository: google-gemini/gemini-cli

#   File: README.md

#   Lines: 211

#   

#   Estimated tokens: 1.6k

#   

#   ## File tree

#   ```Directory structure:

#   └── README.md

#   

#   ```

#   

#   ## Extracted content

#   ================================================

#   FILE: README.md

#   ================================================

#   # Gemini CLI

#   

#    https://github.com/google-gemini/gemini-cli/actions/workflows/ci.yml/badge.svg ](https://github.com/google-gemini/gemini-cli/actions/workflows/ci.yml)

#   

#    ./docs/assets/gemini-screenshot.png 

#   

#   This repository contains the Gemini CLI, a command-line AI workflow tool that connects to your

#   tools, understands your code and accelerates your workflows.

#   

#   With the Gemini CLI you can:

#   

#   - Query and edit large codebases in and beyond Gemini's 1M token context window.

#   - Generate new apps from PDFs or sketches, using Gemini's multimodal capabilities.

#   - Automate operational tasks, like querying pull requests or handling complex rebases.

#   - Use tools and MCP server...

#   [93m----------------------------------------------------------------------------------------------------[0m


"""
As context:
"""

pretty_print.wrapped(f"{cropped_research.to_context()}", title="Research")
# Output:
#   [93m--------------------------------------------- Research ---------------------------------------------[0m

#     

#   <research>

#       # Research

#   

#   ## Code Sources

#   

#   <details>

#   <summary>Repository analysis for https://github.com/google-gemini/gemini-cli/blob/main/README.md</summary>

#   

#   # Repository analysis for https://github.com/google-gemini/gemini-cli/blob/main/README.md

#   

#   ## Summary

#   Repository: google-gemini/gemini-cli

#   File: README.md

#   Lines: 211

#   

#   Estimated tokens: 1.6k

#   

#   ## File tree

#   ```Directory structure:

#   └── README.md

#   

#   ```

#   

#   ## Extracted content

#   ================================================

#   FILE: README.md

#   ================================================

#   # Gemini CLI

#   

#    https://github.com/google-gemini/gemini-cli/actions/workflows/ci.yml/badge.svg ](https://github.com/google-gemini/gemini-cli/actions/workflows/ci.yml)

#   

#    ./docs/assets/gemini-screenshot.png 

#   

#   This repository contains the Gemini CLI, a command-line AI workflow tool that connects to your

#   tools, understands your code and accelerates your workflows.

#   

#   With the Gemini CLI you can:

#   

#   - Query and edit large codebases in and beyond Gemini's 1M token context window.

#   - Generate new apps from PDFs or sketches, using Gemini's multimodal capabilities.

#   - Automate operational tasks, like querying pull requests or handling complex rebases.

#   - Use tools and MCP server...

#   </research>

#   

#   [93m----------------------------------------------------------------------------------------------------[0m


"""
As before, we can see how the Research object it's mapped to it's content surrounded by the `<research>...</research>` XML tags making sure that whenever we want to pass this object to an LLM we respect this rule.

As an important gotcha, we don't have to be too rigid about how we translate these objects to XML. For example, here we completely omitted the `<content>` field. As there is just one field, encoding the attribute name is redundant. The most important element is the root `<research>...</research>` XML tag. 

We intentionally implemented both options to highlight this method's flexibility. 
"""

"""
### Article Examples Entity

The `ArticleExamples` entity contains few-shot examples that demonstrate the desired writing style, structure, and quality. These serve as templates for the LLM to understand what kind of output is expected. Intuitively, these can be seen as the on-demand training set that specializes the LLM on our concrete task.

From `brown.entities.articles`:

```python
from pydantic import BaseModel
from brown.entities.mixins import ContextMixin


class ArticleExample(BaseModel, ContextMixin):
    content: str

    def to_context(self) -> str:
        return f"""
<{self.xml_tag}>
    {self.content}
</{self.xml_tag}>
"""

    def __str__(self) -> str:
        return f"ArticleExample(len_content={len(self.content)})"


class ArticleExamples(BaseModel, ContextMixin):
    examples: list[ArticleExample]

    def to_context(self) -> str:
        return f"""
<{self.xml_tag}>
        {"\n".join([example.to_context() for example in self.examples])}
</{self.xml_tag}>
"""
```

**Composition Pattern:**

- `ArticleExample`: Represents a single example article
- `ArticleExamples`: Contains multiple examples and composes their context representations
- When converted to context, all examples are nested within the parent XML tags

This pattern allows us to provide multiple examples to the LLM, showing consistency across different articles.

Let's load the few-shot examples:

"""

from brown.loaders import MarkdownArticleExampleLoader

examples_loader = MarkdownArticleExampleLoader(uri=EXAMPLES_DIR)
article_examples = examples_loader.load()

cropped_article_examples = article_examples.model_copy()
for example in cropped_article_examples.examples:
    example.content = f"{example.content[:500]}..."

pretty_print.wrapped(f"{article_examples.examples[0]}", title="First Article Example (First 500 Chars)", width=120)
# Output:
#   [93m--------------------------------------- First Article Example (First 500 Chars) ---------------------------------------[0m

#     ArticleExample(len_content=503)

#   [93m------------------------------------------------------------------------------------------------------------------------[0m


"""
Let's also call the `to_context()` method on this nested structure:
"""

pretty_print.wrapped(
    f"{cropped_article_examples.to_context()}", title="Article Examples as Context (First 500 Chars)", width=120
)
# Output:
#   [93m------------------------------------ Article Examples as Context (First 500 Chars) ------------------------------------[0m

#     

#   <article_examples>

#           

#   <article_example>

#       # Lesson 3: Context Engineering

#   

#   AI applications have evolved rapidly. In 2022, we had simple chatbots for question-answering. By 2023, Retrieval-Augmented Generation (RAG) systems connected LLMs to domain-specific knowledge. 2024 brought us tool-using agents that could perform actions. Now, we are building memory-enabled agents that remember past interactions and build relationships over time.

#   

#   In our last lesson, we explored how to choose between AI agents and LLM workflows when designing a sy...

#   </article_example>

#   

#   

#   <article_example>

#       # Lesson 4: Structured Outputs

#   

#   In our previous lessons, we laid the groundwork for AI Engineering. We explored the AI agent landscape, looked at the difference between rule-based LLM workflows and autonomous AI agents, and covered context engineering: the art of feeding the right information to an LLM. In this lesson, we will tackle a fundamental challenge: getting structured and reliable information *out* of an LLM.

#   

#   ```mermaid

#   flowchart LR

#       subgraph "Software 3.0"

#           A(("LLMs"))

#       e...

#   </article_example>

#   

#   </article_examples>

#   

#   [93m------------------------------------------------------------------------------------------------------------------------[0m


"""
Note how we represented a list of objects with XML tags, using the following nested structured:
```
<article_examples>
    <article_example>
    ...
    </article_example>
    <article_example>
    ...
    </article_example>
</article_examples>
```
Like this we can represent any list of objects when transforming them from Pydantic entities to LLM context.
"""

"""
### Profiles Entity

Profiles are the secret sauce of the Brown agent. They provide detailed instructions that control different aspects of how we expect the article to look and sound like.

First, let's look at how we structured them as Pydantic classes, and then we will explain in more detail the role of each one.

From `brown.entities.profiles`:

```python
from pydantic import BaseModel
from brown.entities.mixins import ContextMixin


class Profile(BaseModel, ContextMixin):
    name: str
    content: str

    def to_context(self) -> str:
        return f"""
<{self.xml_tag}>
    <name>{self.name}</name>
    <content>{self.content}</content>
</{self.xml_tag}>
"""


class CharacterProfile(Profile):
    pass


class ArticleProfile(Profile):
    pass


class StructureProfile(Profile):
    pass


class MechanicsProfile(Profile):
    pass


class TerminologyProfile(Profile):
    pass


class TonalityProfile(Profile):
    pass


class ArticleProfiles(BaseModel):
    character: CharacterProfile
    article: ArticleProfile
    structure: StructureProfile
    mechanics: MechanicsProfile
    terminology: TerminologyProfile
    tonality: TonalityProfile
```

**Profile Hierarchy:**

1. **Base `Profile` Class**: Provides common structure (name + content) and context conversion
2. **Specialized Profile Classes**: Inherit from `Profile` for specialized context elements
3. **`ArticleProfiles` Container**: Holds all six profile types together

**Why Separate Profile Classes?**

Even though they have the same structure, separate classes provide:
- Type safety (can't accidentally swap profile types)
- Context clarity (each profile will have its own XML tag when added as context)

Let's load all the profiles:

"""

from brown.loaders import MarkdownArticleProfilesLoader

profiles_input = {
    "article": PROFILES_DIR / "article_profile.md",
    "character": PROFILES_DIR / "character_profiles" / "paul_iusztin.md",
    "mechanics": PROFILES_DIR / "mechanics_profile.md",
    "structure": PROFILES_DIR / "structure_profile.md",
    "terminology": PROFILES_DIR / "terminology_profile.md",
    "tonality": PROFILES_DIR / "tonality_profile.md",
}

profiles_loader = MarkdownArticleProfilesLoader(uri=profiles_input)
article_profiles = profiles_loader.load()

profile_sizes = {
    "Character Profile": len(article_profiles.character.content),
    "Article Profile": len(article_profiles.article.content),
    "Structure Profile": len(article_profiles.structure.content),
    "Mechanics Profile": len(article_profiles.mechanics.content),
    "Terminology Profile": len(article_profiles.terminology.content),
    "Tonality Profile": len(article_profiles.tonality.content),
}
profile_sizes["Total"] = sum(profile_sizes.values())
pretty_print.wrapped(
    profile_sizes,
    title="Profile Sizes (in characters)",
)
# Output:
#   [93m---------------------------------- Profile Sizes (in characters) ----------------------------------[0m

#     {

#     "Character Profile": 3033,

#     "Article Profile": 13074,

#     "Structure Profile": 22660,

#     "Mechanics Profile": 4747,

#     "Terminology Profile": 10730,

#     "Tonality Profile": 4192,

#     "Total": 58436

#   }

#   [93m----------------------------------------------------------------------------------------------------[0m


"""
Now, let's take a look at the article character profile:
"""

article_profile_copy = article_profiles.article.model_copy()
article_profile_copy.content = f"{article_profile_copy.content[:400]}..."

pretty_print.wrapped(f"{article_profile_copy.content}", title="Article Profile (First 400 Chars)", width=170)
# Output:
#   [93m------------------------------------------------------------------- Article Profile (First 400 Chars) -------------------------------------------------------------------[0m

#     ## Tonality

#   

#   You should write in a humanized way as writing a blog article or book.

#   

#   Write the description of ideas as fluid as possible. Remember that you are writing a book or blog article. Thus, everything should flow naturally, without too many bullet points or subheaders. Use them only when it really makes sense. Otherwise, stick to normal paragraphs.

#   

#   ## General Article Structure

#   

#   The articl...

#   [93m--------------------------------------------------------------------------------------------------------------------------------------------------------------------------[0m


"""
Now, let's call the `to_context()` method. We will show only the first 500 characters to see how the enclosing XML tags logic works:
"""

pretty_print.wrapped(f"{article_profile_copy.to_context()}", title="Article Profile as Context (First 400 Chars)")

# Output:
#   [93m--------------------------- Article Profile as Context (First 400 Chars) ---------------------------[0m

#     

#   <article_profile>

#       <name>article</name>

#       <content>## Tonality

#   

#   You should write in a humanized way as writing a blog article or book.

#   

#   Write the description of ideas as fluid as possible. Remember that you are writing a book or blog article. Thus, everything should flow naturally, without too many bullet points or subheaders. Use them only when it really makes sense. Otherwise, stick to normal paragraphs.

#   

#   ## General Article Structure

#   

#   The articl...</content>

#   </article_profile>

#   

#   [93m----------------------------------------------------------------------------------------------------[0m


"""
And if we call the `to_context()` method on a different profile we will see it picked on its own XML tag, reason why it was important to create different classes for each:
"""

structure_profile_copy = article_profiles.structure.model_copy()
structure_profile_copy.content = f"{structure_profile_copy.content[:400]}..."

pretty_print.wrapped(f"{structure_profile_copy.to_context()}", title="Structure Profile as Context (First 400 Chars)")

# Output:
#   [93m-------------------------- Structure Profile as Context (First 400 Chars) --------------------------[0m

#     

#   <structure_profile>

#       <name>structure</name>

#       <content>## Sentence and paragraph length patterns

#   

#   Write sentences 5–25 words; allow occasional 30-word 'story' sentences. Keep paragraphs ≤ 80 words; allow an occasional 1-sentence paragraph to emphasize a point.

#   

#   - Good examples:

#     - four 18-word sentence, as a paragraph of 72 words.

#     - Ocassional 1-sentece paragraph.

#   - Bad examples:

#     - Frequent 40-word run-ons.

#     - five 18-word sentence, as a paragra...</content>

#   </structure_profile>

#   

#   [93m----------------------------------------------------------------------------------------------------[0m


"""
Note how we represent the `name` and `content` attributes under the `<name>...</name>` and `<content>...</content>` child XML tags, under the `<structure_profile>...</structure_profile>` parent XML tag.
"""

"""
## 5. Understanding the Content Generation Profiles

Now that we've loaded all the profiles, let's understand what each one does and why it's essential for generating high-quality content. Profiles are the key to making the Brown writing workflow produce consistent, high-quality output that matches your desired style and voice.

### Profile Categories

The profiles fall into two categories:

**1. General Profiles** (applicable to any content type):
- Mechanics Profile
- Structure Profile  
- Terminology Profile
- Tonality Profile

**2. Specialized Profiles** (specific to particular use cases):
- Article Profile (content-type specific)
- Character Profile (voice-specific)

Let's explore each profile and understand its role.

"""

"""
### 1. Mechanics Profile

**Purpose**: Controls the technical mechanics of writing words, sentences up to paragraphs.

**Location**: `inputs/profiles/mechanics_profile.md`

**What it covers**:
- Sentence length and complexity
- Paragraph structure and transitions
- Active vs passive voice usage
- Punctuation rules
- Readability guidelines

**Example rules**:
- "Vary sentence length for rhythm (mix short and long sentences)"
- "Use active voice unless passive is more appropriate"
- "Limit sentences to 20-25 words for clarity"

This profile ensures the writing is clean and easy to read.

"""

"""
### 2. Structure Profile

**Purpose**: Defines how the whole content should be organized and formatted. Instead of looking at the paragraph level, here we are interested at how the whole piece is structured.

**Location**: `inputs/profiles/structure_profile.md`

**What it covers**:
- Document structure (headings, sections)
- List formatting (when to use bullets vs numbers)
- Code block formatting
- Media placement rules
- Table of contents guidelines

**Example rules**:
- "Use ## for main sections, ### for subsections"
- "Code blocks must include language specification"
- "Use numbered lists for sequential steps, bullets for items"

This profile ensures consistent formatting across all generated content.

"""

"""
### 3. Terminology Profile

**Purpose**: Guides word choice and phrasing to match the target audience. Tries to avoid AI slop.

**Location**: `inputs/profiles/terminology_profile.md`

**What it covers**:
- Technical vs simple language decisions
- Jargon usage guidelines
- Acronym handling
- Preferred terminology
- Words to avoid

**Example rules**:
- "Define technical terms on first use"
- "Spell out acronyms on first mention: Machine Learning (ML)"
- "Avoid AI slop jargon like 'synergy' or 'leverage'"

This profile ensures the content speaks the right language for its audience.

"""

"""
### 4. Tonality Profile

**Purpose**: Sets the overall tone, voice, and style of the writing.

**Location**: `inputs/profiles/tonality_profile.md`

**What it covers**:
- Formality level
- Use of humor and personality
- Emotional tone
- Perspective (first/second/third person)
- Energy and enthusiasm level

**Example rules**:
- "Maintain a conversational but professional tone"
- "Use second person ('you') to engage readers"
- "Include occasional light humor when appropriate"

This profile gives the content its personality and makes it engaging.

"""

"""
### 5. Article Profile

**Purpose**: Defines rules specific to article generation (content-type specific).

**Location**: `inputs/profiles/article_profile.md`

**What it covers**:
- Article-specific structure requirements
- Introduction and conclusion patterns
- How to handle references and citations
- Examples and case study integration
- Call-to-action guidelines

**Example rules**:
- "Start with a hook that captures attention"
- "Include learning objectives near the beginning"
- "End with key takeaways and next steps"
- "Cite sources using footnotes or inline links"

This profile contains rules that only make sense for articles. For example, we could easily extend the agent to other formats such as video scripts or social media posts by adding `video_script_profile.md and `social_media_posts_profile.md`

"""

"""
### 6. Character Profile

**Purpose**: Injects a specific voice or persona into the writing.

**Location**: `inputs/profiles/character_profiles/paul_iusztin.md` (or other character files)

**What it covers**:
- Personal details (name, background)
- Professional expertise and experience
- Writing style preferences
- Content focus areas
- Unique phrases or expressions

**Example rules**:
- "Write from Paul's perspective as an AI engineer and educator"
- "Emphasize hands-on implementation over theory"
- "Use examples from real projects when possible"

**Why Character Profiles Matter:**

Character profiles allow the agent to write in a consistent voice:
- **Personal Brand**: Content feels like it comes from a real person
- **Authenticity**: References experiences and perspectives naturally
- **Flexibility**: Can switch between different voices (e.g., Paul Iusztin vs. Louis-Francois vs. Richard Feynman)
- **Consistency**: Maintains the same voice across multiple articles

You can create profiles for yourself or emulate famous figures' writing styles.

"""

"""
### How Profiles Work Together

The magic happens when all six profiles work together:

1. **Character Profile** → Establishes whose voice we're writing in
2. **Tonality Profile** → Sets the emotional tone and energy
3. **Terminology Profile** → Chooses the right words for the audience
4. **Mechanics Profile** → Ensures technical writing quality
5. **Structure Profile** → Organizes content logically
6. **Article Profile** → Applies article-specific best practices

**Example: How Profiles Shape a Single Paragraph**

Without profiles, an LLM might write:
> "Machine learning models require data. You need to preprocess this data. Then you train the model."

With all profiles applied:
> "Here's the thing about ML models - they're hungry for data, but not just any data. Before you can train your model, you'll need to clean and transform your raw data into a format the model can actually learn from. Think of it like preparing ingredients before cooking: the better your prep work, the better your final dish."

The profiles transform bland, generic text into engaging, personable, and technically accurate content that sounds like a real expert talking to a friend.

"""

"""
## 6. Model Configuration: Flexible LLM Management

Before we dive into the workflow nodes, let's understand how the Brown agent manages different LLM APIs and configurations.
### The `get_model` Function

The central function for model initialization is `get_model` from `brown.models.get_model`. It provides a unified interface for creating LLM instances across the codebase.

From `brown.models.get_model`:

```python
import json

from langchain.chat_models import init_chat_model
from langchain_core.language_models import BaseChatModel

from brown.config import get_settings
from brown.models.config import DEFAULT_MODEL_CONFIGS, ModelConfig, SupportedModels
from brown.models.fake_model import FakeModel

MODEL_TO_REQUIRED_API_KEY = {
    SupportedModels.GOOGLE_GEMINI_30_PRO: "GOOGLE_API_KEY",
    SupportedModels.GOOGLE_GEMINI_25_PRO: "GOOGLE_API_KEY",
    SupportedModels.GOOGLE_GEMINI_25_FLASH: "GOOGLE_API_KEY",
    SupportedModels.GOOGLE_GEMINI_25_FLASH_LITE: "GOOGLE_API_KEY",
}


def get_model(model: SupportedModels, config: ModelConfig | None = None) -> BaseChatModel:
    if model == SupportedModels.FAKE_MODEL:
        if config and config.mocked_response is not None:
            if hasattr(config.mocked_response, "model_dump"):
                mocked_response_json = config.mocked_response.model_dump(mode="json")
            else:
                mocked_response_json = json.dumps(config.mocked_response)
            return FakeModel(responses=[mocked_response_json])
        else:
            return FakeModel(responses=[])

    config = config or DEFAULT_MODEL_CONFIGS.get(model) or ModelConfig()
    model_kwargs = {
        "model": model.value,
        **config.model_dump(),
    }

    required_api_key = MODEL_TO_REQUIRED_API_KEY.get(model)
    if required_api_key:
        settings = get_settings()
        if not getattr(settings, required_api_key):
            raise ValueError(f"Required environment variable `{required_api_key}` is not set")
        else:
            model_kwargs["api_key"] = getattr(settings, required_api_key)

    return init_chat_model(**model_kwargs)
```

**Key Features:**

1. **API Key Management**: Automatically pulls credentials from settings
2. **Default Configurations**: Falls back to sensible defaults
3. **LangChain Integration**: Uses `init_chat_model` for consistent interface between different LLMs

"""

"""
### Model Configuration Structures

The system uses three key structures for model configuration:

From `brown.models.config`:

```python
from enum import StrEnum
from typing import Any

from pydantic import BaseModel, Field


class SupportedModels(StrEnum):
    GOOGLE_GEMINI_30_PRO = "google_genai:gemini-3-pro-preview"
    GOOGLE_GEMINI_25_PRO = "google_genai:gemini-2.5-pro"
    GOOGLE_GEMINI_25_FLASH = "google_genai:gemini-2.5-flash"
    GOOGLE_GEMINI_25_FLASH_LITE = "google_genai:gemini-2.5-flash-lite"
    FAKE_MODEL = "fake"


class ModelConfig(BaseModel):
    temperature: float = 0.7
    top_k: int | None = None
    n: int = 1
    response_modalities: list[str] | None = None
    include_thoughts: bool = False
    thinking_budget: int | None = Field(
        default=None,
        ge=0,
        description="If reasoning is available, the maximum number of tokens the model can use for thinking.",
    )
    max_output_tokens: int | None = None
    max_retries: int = 6

    mocked_response: Any | None = None


DEFAULT_MODEL_CONFIGS = {
    "google_genai:gemini-2.5-pro": ModelConfig(
        temperature=0.7,
        include_thoughts=False,
        thinking_budget=1000,
        max_retries=3,
    ),
    "google_genai:gemini-2.5-flash": ModelConfig(
        temperature=1,
        thinking_budget=1000,
        include_thoughts=False,
        max_retries=3,
    ),
    "google_genai:gemini-2.0-flash-exp": ModelConfig(
        temperature=0.7,
        thinking_budget=1000,
        include_thoughts=False,
        max_retries=3,
    ),
}
```

**Design Insights:**

1. **`SupportedModels` Enum**: Type-safe model selection
2. **`ModelConfig` Pydantic Model**: Validates configuration parameters
3. **`DEFAULT_MODEL_CONFIGS` Dict**: Pre-configured settings for each model

This allows different nodes in the workflow to use different models with different configurations, optimizing for specific tasks (e.g., using faster models for media generation, more powerful models for article writing).

"""

"""
### Usage Throughout the Codebase

Every node in the workflow uses `get_model` to instantiate its LLM. This provides:

- **Consistency**: Same interface everywhere
- **Flexibility**: Easy to swap models for different nodes
- **Configuration**: Centralized model settings
- **Testing**: Can use FakeModel for tests

Let's try it out with a simple example:

"""

from brown.models import ModelConfig, SupportedModels, get_model

# Create a model instance with custom configuration
model_config = ModelConfig(temperature=0.5, max_output_tokens=100)
model = get_model(SupportedModels.GOOGLE_GEMINI_25_FLASH, config=model_config)

# Test it with a simple prompt
response = await model.ainvoke([{"role": "user", "content": "Say hello in one sentence!"}])
print(f"Model response: {response.content}")
# Output:
#   Model response: 


"""
Let's see how the fake model works as well:
"""

model_config = ModelConfig(temperature=0.5, max_output_tokens=100, mocked_response="This is a fake response!")
model = get_model(SupportedModels.FAKE_MODEL, config=model_config)

# Test it with a simple prompt
response = await model.ainvoke([{"role": "user", "content": "Say hello in one sentence!"}])
print(f"Model response: {response.content}")
# Output:
#   Model response: "This is a fake response!"


"""
## 7. Media Generation: The Orchestrator-Worker Pattern

Now let's explore one of the most interesting architectural patterns in the Brown agent: the orchestrator-worker pattern for media generation. This pattern efficiently delegates specialized tasks to expert workers.

### Understanding Node Abstractions

First, let's understand the base abstractions that all workflow nodes inherit from.

From `brown.nodes.base`:

```python
from abc import ABC, abstractmethod
from typing import Any, Iterable, Literal, TypedDict

from langchain_core.runnables import Runnable
from langchain_core.tools import BaseTool


class ToolCall(TypedDict):
    name: str
    args: dict[str, Any]
    id: str
    type: Literal["tool_call"]


class Toolkit(ABC):
    """Base class for toolkits following LangChain's toolkit pattern."""

    def __init__(self, tools: list[BaseTool]) -> None:
        self._tools: list[BaseTool] = tools
        self._tools_mapping: dict[str, BaseTool] = {tool.name: tool for tool in self._tools}

    def get_tools(self) -> list[BaseTool]:
        """Get all registered media item generation tools."""
        return self._tools.copy()

    def get_tools_mapping(self) -> dict[str, BaseTool]:
        """Get a mapping of tool names to tool instances."""
        return self._tools_mapping

    def get_tool_by_name(self, name: str) -> BaseTool | None:
        """Get a specific tool by name."""
        return self._tools_mapping.get(name)


class Node(ABC):
    def __init__(self, model: Runnable, toolkit: Toolkit) -> None:
        self.toolkit = toolkit
        self.model = self._extend_model(model)

    def _extend_model(self, model: Runnable) -> Runnable:
        # Can be overridden to bind tools, structured output, etc.
        return model

    def build_user_input_content(self, inputs: Iterable[str], image_urls: list[str] | None = None) -> list[dict[str, Any]]:
        """Build multimodal input content with optional images."""
        messages: list[dict[str, Any]] = []
        if image_urls:
            for image_url in image_urls:
                messages.append({"type": "image_url", "image_url": {"url": image_url}})
        
        for input_ in inputs:
            messages.append({"type": "text", "text": input_})

        return messages

    @abstractmethod
    async def ainvoke(self) -> Any:
        pass
```

**Key Abstractions:**

1. **`ToolCall`**: TypedDict representing a tool call (name, args, id)
2. **`Toolkit`**: Manages a collection of tools that can be passed to a node
3. **`Node`**: Base class for all workflow nodes
   - Takes a model and toolkit
   - Can extend the model (bind tools, structured output)
   - Supports multimodal input (text + images)
   - Abstract `ainvoke` method for execution

"""

"""
### The MediaGeneratorOrchestrator Node

The orchestrator analyzes the article guideline and research to identify what media items need generation, then delegates to specialized worker tools.

Key implementation details from `brown.nodes.media_generator.MediaGeneratorOrchestrator`:

**1. Class Initialization:**
```python
class MediaGeneratorOrchestrator(Node):
    system_prompt_template = "..."

    def __init__(
        self,
        article_guideline: ArticleGuideline,
        research: Research,
        model: Runnable,
        toolkit: Toolkit,
    ) -> None:
        self.article_guideline = article_guideline
        self.research = research
        super().__init__(model, toolkit)
```

**2. Model Extension (Tool Binding):**
```python
def _extend_model(self, model: Runnable) -> Runnable:
    model = cast(BaseChatModel, super()._extend_model(model))
    model = model.bind_tools(self.toolkit.get_tools(), tool_choice="any")
    return model
```

The orchestrator binds all available worker tools to the model, allowing it to call multiple tools.

💡 Through tools we can easily extend the orchestrator with multiple worker types without even touching the orchestrator, making this a fully modular implementation.

**3. Async Invocation:**
```python
async def ainvoke(self) -> list[ToolCall]:
    system_prompt = self.system_prompt_template.format(
        article_guideline=self.article_guideline.to_context(),
        research=self.research.to_context(),
    )
    user_input_content = self.build_user_input_content(
        inputs=[system_prompt], 
        image_urls=self.research.image_urls
    )
    inputs = [{"role": "user", "content": user_input_content}]
    response = await self.model.ainvoke(inputs)
    
    if isinstance(response, AIMessage) and response.tool_calls:
        jobs = cast(list[ToolCall], response.tool_calls)
    else:
        jobs = []
    
    return jobs
```

Returns a list of `ToolCall` objects describing what media items to generate.

**4. System Prompt:**

The orchestrator uses a detailed system prompt that instructs it to:
- Analyze the article guideline for media requirements
- Look for explicit indicators like "Render a Mermaid diagram"
- Call appropriate worker tools with detailed descriptions
- Handle cases where no media is needed

Here is the full system prompt attached to the node:
```python
class MediaGeneratorOrchestrator(Node):
    system_prompt_template = """You are an Media Generation Orchestrator responsible for analyzing article 
guidelines and research to identify what media items need to be generated for the article.

Your task is to:
1. Carefully analyze the article guideline and research content provided
2. Identify ALL explicit requests for media items (diagrams, charts, visual illustrations, etc.)
3. For each identified media requirement, call the appropriate tool to generate the media item
4. Provide clear, detailed descriptions for each media item based on the guideline requirements and research context

## Analysis Guidelines

**Look for these explicit indicators in the article guideline:**
- Direct mentions: "Render a Mermaid diagram", "Draw a diagram", "Create a visual", "Illustrate with", etc.
- Visual requirements: "diagram visually explaining", "chart showing", "figure depicting", "visual representation"
- Process flows: descriptions of workflows, architectures, data flows, or system interactions
- Structural elements: hierarchies, relationships, comparisons, or step-by-step processes

**Key places to look:**
- Section requirements and descriptions  
- Specific instructions mentioning visual elements
- Complex concepts that would benefit from visual explanation
- Architecture or system descriptions
- Process flows or workflows

## Tool Usage Instructions

You will call multiple tools to generate the media items. You will use the tool that is most appropriate for the media item you are 
generating.

For each identified media requirement:

**When to use MermaidDiagramGenerator:**
- Explicit requests for Mermaid diagrams
- System architectures and workflows
- Process flows and data pipelines  
- Organizational structures or hierarchies
- Flowcharts for decision-making processes
- Sequence diagrams for interactions
- Entity-relationship diagrams
- Class diagrams for software structures
- State diagrams for system states
- Mind maps for concept relationships

**Description Requirements:**
When calling tools, provide detailed descriptions that include:
- The specific purpose and context from the article guideline
- Key components that should be included based on the research
- The type of diagram most appropriate (flowchart, sequence, architecture, etc.)
- Specific elements, relationships, or flows to highlight
- Any terminology or technical details from the research

## Example Analysis Process

1. **Scan the guideline** for phrases like:
   - "Render a Mermaid diagram of..."
   - "Draw a diagram showing..."
   - "Illustrate the architecture..."
   - "Visual representation of..."

2. **For each found requirement:**
   - Extract the specific context and purpose
   - Identify what should be visualized
   - Determine the most appropriate diagram type
   - Craft a detailed description incorporating research insights

3. **Call the appropriate tool** with the comprehensive description

## Input Context

Here is the article guideline:
{article_guideline}

Here is the research:
{research}

## Your Response

Analyze the provided article guideline and research, then call the appropriate tools for each 
identified media item requirement. Each tool call should include a detailed description that ensures 
the generated media item will be relevant, accurate, and valuable for the article's educational goals.

If no explicit media requirements are found in the guideline, respond with: 
"No explicit media item requirements found in the article guideline."
"""
```

💡 The only element from the orchestrator that is aware about its tools, such as the `MermaidDiagramGenerator`, is the system prompt. Here we did this to ensure it always picks up on what we want, but you can further experiment to make the system prompt more general and choose the right tools solely based on their tool description and interface. Ultimately, doing so, you make the orchestrator fully modular. But sometimes hardcoding stuff in the system prompt to get the job done is totally fine. You just have to be aware about the rules to know when to break them. 
"""

"""
### The ToolNode Abstraction

Worker tools inherit from `ToolNode`, a special type of `Node` that can be converted into a LangChain tool.

From `brown.nodes.base`:

```python
class ToolNode(Node):
    def __init__(self, model: Runnable) -> None:
        super().__init__(model, toolkit=Toolkit(tools=[]))

    def as_tool(self) -> StructuredTool:
        return StructuredTool.from_function(
            coroutine=self.ainvoke,
            name=f"{camel_to_snake(self.__class__.__name__)}_tool",
        )

    @abstractmethod
    async def ainvoke(self) -> Any:
        pass
```

**Key Features:**

- Inherits from `Node` but has no tools itself (empty toolkit)
- `as_tool()` method converts the node into a LangChain `StructuredTool`
- The tool name is automatically derived from the class name
- The tool's coroutine is the node's `ainvoke` method

This allows any `ToolNode` to be easily integrated as a tool into any node's toolkit. Like this we can easily transform each node into a tool that can be passed to another node, providing full composability between nodes.

In our use case we will create a `MediaGeneratorOrchestrator` node that we will transform into a tool that will be passed as a worker to the orchestrator. Let's see how that works.
"""

"""
### The MermaidDiagramGenerator Worker

Before running the `MediaGeneratorOrchestrator`, let's examine a concrete worker implementation from `brown.nodes.tool_nodes.MermaidDiagramGenerator`.

💭 Even if an LLM is capable of generating both the Mermaid diagram and the article at the same time, having a specialized worker that is carefully prompted for generating Mermaid diagrams gives us more control as we can customize how the Mermaid diagrams should look like, carefully prompt engineering special use cases such as treating special characters and guiding the LLM what we expect from the diagram. Ultimately ending up with richer, more beautiful, and working Mermaid diagrams.

**1. Structured Output Model:**
```python
class GeneratedMermaidDiagram(BaseModel):
    content: str = Field(description="The Mermaid diagram code formatted in Markdown format as: ```mermaid\n[diagram content here]\n```")
    caption: str = Field(description="The caption, as a short description of the diagram.")
```

**2. Class Structure:**
```python
class MermaidDiagramGenerator(ToolNode):
    prompt_template = "..."

    def _extend_model(self, model: Runnable) -> Runnable:
        model = cast(BaseChatModel, super()._extend_model(model))
        model = model.with_structured_output(GeneratedMermaidDiagram, include_raw=False)
        return model

    async def ainvoke(self, description_of_the_diagram: str, section_title: str) -> MermaidDiagram:
        """Specialized tool to generate a mermaid diagram from a text description. This tool uses a specialized LLM to
        convert a natural language description into a mermaid diagram.

        Use this tool when you need to generate a mermaid diagram to explain a concept. Don't confuse mermaid diagrams,
        or diagrams in general, with media data, such as images, videos, audio, etc. Diagrams are rendered dynamically
        customized for each article, while media data are static data added as URLs from external sources.
        This tool is used explicitly to dynamically generate diagrams, not to add media data.

        Args:
            description_of_the_diagram: Natural language description of the diagram to generate.
            section_title: Title of the section that the diagram is for.

        Returns:
            The generated mermaid diagram code in Markdown format.

        Raises:
            Exception: If diagram generation fails.

        Examples:
        >>> description = "A flowchart showing data flowing from user input to database"
        >>> diagram = await generate_mermaid_diagram(description)
        >>> print(diagram)
        ```mermaid
        graph LR
            A[User Input] --> B[Processing]
            B --> C[(Database)]
        ```
        """

        try:
            response = await self.model.ainvoke(
                [
                    {
                        "role": "user",
                        "content": self.prompt_template.format(
                            description_of_the_diagram=description_of_the_diagram,
                        ),
                    }
                ]
            )

        except Exception as e:
            logger.exception(f"Failed to generate Mermaid diagram: {e}")

            return MermaidDiagram(
                location=section_title,
                content=f'```mermaid\ngraph TD\n    A["Error: Failed to generate diagram"]\n    A --> B["{str(e)}"]\n```',
                caption=f"Error: Failed to generate diagram: {str(e)}",
            )

        if not isinstance(response, GeneratedMermaidDiagram):
            raise InvalidOutputTypeException(GeneratedMermaidDiagram, type(response))

        return MermaidDiagram(
            location=section_title,
            content=response.content,
            caption=response.caption,
        )
```

**Key Features:**

- Uses structured output to ensure valid diagram generation
- The ainvoke() method has a carefully designed pydoc and signature that will be used when the node is transformed into a tool used by the orchestrator.
- In case of error, we return a placeholder diagram to avoid failing the whole workflow because of a tool failure. Remember that we can run dozens of tools in parallel, thus the change of failure due to things such as rate limits is high.

The prompt template contains detailed

[... Content truncated due to length ...]

</details>


## YouTube Video Transcripts

<details>
<summary>[00:00] (Video starts with a black screen)</summary>

[00:00] (Video starts with a black screen)

[00:01] (Title slide appears: "Prompting 101" with "Code w/ Claude" in the top right. Below the title are two names: "Hannah Moran, Applied AI, Anthropic" and "Christian Ryan, Applied AI, Anthropic". A staircase graphic is on the right side of the slide.)

[00:05] (Two speakers, Hannah Moran and Christian Ryan, stand at a podium on a stage in front of an audience. The slide is projected on a large screen behind them.)
**Hannah Moran:** Hi everyone, thank you for joining us today for Prompting 101. Uh, my name is Hannah. I'm part of the Applied AI team here at Anthropic, and with me is Christian, also part of the Applied AI team. And what we're going to do today is take you through a little bit of prompting best practices and we're going to use a real-world scenario and build up a prompt together.

[00:25] (The slide changes to "What is 'prompt engineering' anyway?" with a definition and a 3x3 grid of "Skills involved:")
**Hannah Moran:** Uh, so a little bit about what prompt engineering is.
(The skills listed are: "Programming in natural language", "Clear, unambiguous, precise writing", "Conceptual engineering", "Creating evals with a scientific mindset", "Product thinking - what is the ideal model behavior for your product?", "Testing", "Understanding the LLMs", "Aggregating and analyzing failure modes + thinking of ways to fix", "Making LLMs scale to a wide range of inputs")
[00:30] **Hannah Moran:** Uh, prompt engineering, you're all probably a little bit familiar with this. This is the way that we communicate with a language model and try to get it to do what we want. So this is the practice of writing clear instructions for the model, giving the model the context that it needs to complete the task, and thinking through how we want to arrange that information in order to get the best result. Um, so there's a lot of detail here. A lot of different ways you might want to think about building out a prompt. Um, and as always, the best way to learn this is just to practice doing it.

[00:56] (The slide changes to "Hands-on scenario".)
**Hannah Moran:** Um, so today we're going to go through a hands-on scenario. Um, we're going to use an example inspired by a real customer that we worked with. So we've modified what the actual customer asked us to do, but this is a really interesting case of trying to analyze some images and get uh, factual information out of the images and have Claude make a judgement about what content it finds there. And I actually do not speak the language that this content is in, but luckily Christian and Claude both do. Um, so I'm going to pass it over to Christian to talk about the scenario and the content.

[01:28] (The slide changes to "Prompt Claude to analyze car accident reporting forms". Two images are displayed: a car accident report form with checkboxes and a hand-drawn sketch of a car accident at an intersection with Swedish text "stillastående" (stationary), "körde" (drove/turned), and a street name "KÖPMANGATAN".)
**Christian Ryan:** So, for this example that we have here, it's uh intended, so, so to set the stage, imagine you're working for a Swedish insurance company, and you deal with uh car insurance claims on a daily manner. Um, and the purpose of this is that you have two pieces of information. Um, we're going to these in details as well, but visually you can see on the left-hand side, we have an a car accident report form, um, just detailing out what transpired before the accident accident actually took place.

[01:59] **Christian Ryan:** Um, and then finally we have a sort of a human drawn, um, a sketch of how the accident took place as well. So these two pieces of information is what we're going to try to pass on to Claude. And to begin with, we could just take these two and throw them into the console and just see what what happens.

[02:14] (The screen transitions to the Anthropic Console Workbench. The current prompt is "Accident Report Form Analysis - v1".)
**Christian Ryan:** So if we transition over to your console as well, we can actually do this in a real manner. And this case here, you can see we have our shiny, beautiful Anthropic console. We're using the new Claude 4 Sonnet model as well.
(Christian clicks "Model settings", revealing "claude-sonnet-4-20250514 latest" is selected, temperature is 0, and max tokens is 2000.)
**Christian Ryan:** In this case, setting temperature to zero, uh, and having a a a huge max token budget as well, it's helping us make sure that there's no limitations to what Claude can do. In this case, you can see I have a very simple prompt just setting the stage [02:30] of what Claude's supposed to do, in this case, mentioning that this is um intended to review an accident report form, uh, and eventually also determine, um, what happened in the accident and who's at fault. So you can see here, with this very simple prompt, if I just run this, let me go to preview.
(Christian clicks "Run", then "Preview". The response pane shows text. "What Happened:" and "Fault Determination:" are headings.)
**Christian Ryan:** Uh, we can see here that, um, Claude thinks that this is in relation to skiing accident, that happened on a street called Köpmangatan, it's a very common street in Sweden. Um, and in many ways, you can sort of understand this as innocent mistake, in a sense that in our prompt we actually haven't done anything to set the stage on what is actually taking place here. So, it's sort of first guess is not too bad, but we still notice a lot of intuition that we can bake into Claude.

[03:22] (The screen transitions back to a slide: "Let's take a step back and do some prompt engineering!" with a flowchart diagram: "Engineer preliminary prompt" -> "Test prompt against cases" -> "Refine prompt". Text below reads: "Prompting is an empirical science: always test your prompts & iterate often!").
**Christian Ryan:** So we switch back to slides, you can see here that, um, in many ways prompt engineering is a very iterative empirical science. In this case here, we could almost have a test case where Claude is supposed to make sure it understands it's in a car or vehicular environment, nothing to do with skiing, uh, and in that way, you iteratively build upon your prompt to make sure it's actually tackling the problem you're intending to solve.

[03:47] (The speakers are back on screen.)
**Christian Ryan:** Um, and to do so, we'll go through some best practices of how we we, at Anthropic, break this down, uh, internally, and how we recommend others to do so as well.

[03:57] (The slide changes to "Best practices for developing a great prompt".)
**Hannah Moran:** So we're going to talk about some best practices for developing a great prompt. Uh, first, we want to talk a little bit about what a great prompt structure looks like. So, you might be familiar with kind of interacting with a chatbot, with Claude, going back and forth, having a more kind of conversational style interaction.

[04:25] **Hannah Moran:** When we're working with a task like this, we're probably using the API, and we kind of want to send one single message to Claude and have it nail the task the first time around, without needing to, uh, kind of move back and forth. Uh, so the kind of structure that we recommend is setting the task description up front, so telling Claude, what are you here to do? What's your role? What tasks are you trying to accomplish today? Then we provide content, so in this case, it's the images that Christian was showing, the form and the drawing of the accident and how they occurred. That's our dynamic content. This might also be something you're retrieving from another system, depending on what your use case is. We're going to give some detailed instructions to Claude, so almost like a step-by-step list of how we want Claude to go through the task and how we want it to, um, tackle the reasoning.

[05:05] **Hannah Moran:** We may give some examples to Claude. Here's an example of if some piece of content you might receive, here's how you should respond when given that content. And at the end, we usually recommend repeating anything that's really important for Claude to understand about this task. Kind of, uh, reviewing the information with Claude, emphasizing things that are extra critical, and then telling Claude, okay, go ahead and do your work.

[05:19] (The slide changes to "Prompt structure" with a more detailed 10-point list on the left and a sample user prompt with different colored sections on the right.)
**Hannah Moran:** So here's another view. This has a little bit more detail, a little bit more of a breakdown, and we're going to walk through each of these 10 points individually and show you how we build this up, um, in the console. So the first couple things, um, Christian's going to talk about the task context and the tone context.

[05:37] (The slide changes to "Let's build a great prompt from scratch in the console". "1. Task context" is highlighted.)
**Christian Ryan:** Perfect. So yeah, if we begin with the task context. As you realized when I went through the little demo there, um, we didn't have much elaborating what what's the scenario Claude's actually working within. And because of that, you can also tell that Claude doesn't necessarily needs to guess a lot more on what you actually want from it.

[06:01] (The slide changes. "1. Task context" and "2. Tone context" are highlighted.)
**Christian Ryan:** So, in our case, we really want to break that down, make sure we can give more clear-cut instructions, um, and also make sure we understand what's the task that we're asking Claude to do. Um, secondly as well, we also make sure we add a little bit of tone into it all. Um, key thing here is we want Claude to stay factual and to stay confident. So if, uh, Claude can't understand what it's looking at, we don't want it to guess and just sort of mislead us. We want to make sure that any assessment, and in our case we want to make sure that we can understand who's at fault here. We want to make sure that assessment is as clear and as confident as possible. If not, we're sort of losing track of what we're doing.

[06:30] (The screen transitions to the Anthropic Console. The prompt is "Accident Report Form Analysis - v1". Christian clicks "Accident Report Form Analysis - v1" in the header to open a dropdown list of prompts and selects "Accident Report Form Analysis - v2".)
**Christian Ryan:** So if we transition back to the console, um, we can jump to a V2 that we have here. So I'll just navigate to V2. And you can see here, um, I'll also just illustrate the data because we didn't really do that last time around, just to really highlight what we're looking at. So what we're seeing here, this is the car accident report form, and it's just 17 different checkboxes going through what actually happened.

[07:00] (Christian zooms in on the `form-numbered.jpg` image in the console, showing the Swedish car accident report form. He then closes the image and opens `IMG_8157.jpg` which shows the hand-drawn sketch.)
**Christian Ryan:** You can see there's a vehicle A and vehicle B, both on the left and right-hand side. And the main purpose of this is that we want to make sure that Claude can understand this manually generated data to assess what's actually going on. And that is, uh, corroborated by, if I navigate back here, to the sketch that we can highlight here as well. In this case, the form is just a different, um, data point for the same scenario. Um, and in this case here, we want to bake in more information into our version two. Uh, and by doing so, I'm actually elaborating a lot more on what's going on.

[07:29] (Christian highlights text in the "User" prompt area: "You are an AI assistant helping a human claims adjuster review car accident reporting documentation written in Swedish." and "You should be able to state whether or not you can fully confidently determine that one vehicle was clearly at fault or if the human adjuster needs to follow up for more information.")
**Christian Ryan:** So you can see here I'm specifying that this AI system is supposed to help a human claims adjuster that's reviewing car accident report forms in Swedish as well. Um, you can see here that we'll also elaborating that it's a human-drawn sketch of the incident and that should not, um, make an assessment if it's not actually fully confident. And that's really key because if we run this, you'll see that, and you can see the same settings as well. Claude 4, my new shiny model, zero temperature as well. If we run this, we can see here what actually happens. In this case, um, Claude's able to pick up that, uh, now it's related to car accidents, not skiing accidents, which is great.

[08:08] (Christian scrolls down to the "Fault Determination" section of the response, which now states: "I cannot fully confidently determine clear fault based solely on these documents. Here's why additional investigation is needed: 1. Missing critical details: The exact positioning of vehicles, traffic control devices, right-of-way rules, and the specific circumstances of the parking/stopping maneuver are unclear. 2. Incomplete visuals: While Vehicle B was turning right and Vehicle A was stationary, other contributing factors not captured in the sketch. 3. Insufficient narrative: The form and sketch provide limited descriptive details, making it difficult to establish a clear sequence of events or driver intentions.")
**Christian Ryan:** We can see it's able to pick up that vehicle A was marked on on checkbox one, and then vehicle B was on 12. Um, and if we scroll down though, we can still tell that there's some information missing for Claude to make a fully confident determination of who's at fault here. And this is great. This is pertaining to the task that we're set. Make sure you don't make anything any claims that aren't, um, uh, factual, and make sure you you only sort of set things when you're when you're confident. But there's a lot of information we're still missing here, um, regarding the form, uh, what the form actually entails. And a lot of that information is what we want to want to bake into this LLM application as well. And the best way of doing so is actually adding it to the system prompt, which Hannah will elaborate on.

[08:46] (The slide changes to "Let's build a great prompt from scratch in the console". "1. Task context", "2. Tone context" are visible, and "3. Background data, documents, and Images" is highlighted.)
**Hannah Moran:** Um, so back in the slides, uh, we have the next item we're going to add to the prompt. And this is, um, background detailed, data, documents, and images. And here, as Christian was saying, we actually know a lot about this form. This form is going to be the same every single time. The form will never change. And so this is a really great type of information to provide to Claude, to tell Claude, here's the structure of the form you'll be looking at. We know that will not ever alter between different queries. The way the form is filled out will change, but the form itself is not going to change. And so this is a great type of, um, information to put into the system prompt. Also a great thing to use prompt caching for if you're considering using prompt caching, this will always be the same. And what this will help Claude do is spend less time trying to figure out what the form is the first time it sees the form each time. And it's going to do a better job of reading the form because it already knows, um, what to expect there.

[09:41] (The slide changes to ">> how to organize information in your prompts". On the left are bullet points about using delimiters. On the right, a sample prompt with XML tags is shown.)
**Hannah Moran:** So, another thing I want to touch on here is how we like to organize information in prompts. So Claude really loves structure, loves organization. That's why we recommend following kind of a standard structure in your prompts. And there's a couple other tools you can use to help Claude understand the information better. I also just want to mention, all of this is in our docs with a lot of really great examples. So definitely take pictures, but if you forget to take a picture, don't worry. All of this content is online with lots of examples and definitely encourage you guys to check it out there too.

[10:14] **Hannah Moran:** Um, anyway, the, uh, so some things you can use delimiters, like XML tags. Also markdown is pretty useful to Claude, but XML tags are nice because you can actually specify what's inside those tags. So we can tell Claude, here's, here's user preferences. Now you're going to read some content, and these XML tags are letting you know that everything wrapped in those tags is related to the user's preferences. And it helps Claude refer back to that information maybe at later points in the prompt. Um, so I want to show in the, back in the console, how we actually do this in this case.

[10:46] (The speakers are back on screen. Christian is at the laptop, Hannah is gesturing.)
**Hannah Moran:** And Christian's going to pull up our version three. So we're keeping everything about the other part of the user prompt the same. And we've decided, in this case, to put this information in the system prompt. You could try this different ways. Um, we're doing it in the system prompt here.

[11:00] (The screen transitions to the Anthropic Console. The prompt is "Accident Report Form Analysis - v3". The system prompt now contains detailed information about the form's structure, completion rules, meaning, and interpretation, enclosed in XML tags like `<form_structure>`, `<driving_actions>`, `<form_completion_rules>`, `<form_meaning>`, `<form_interpretation>`.)
**Hannah Moran:** And we're going to tell Claude everything it needs to know about this form. So this is a Swedish car accident form. The form will be in Swedish. It'll have this title. It'll have two columns. The columns represent different vehicles. We'll tell Claude about each of the 17 rows, and what they mean. You might have noticed when we ran it before, Claude was reading individually each of the lines to understand what they are. We can provide all of that information up front. And we're also going to give Claude a little bit of information about how this form should be filled out. This is also really useful for Claude. We can tell it things like, you know, humans are filling this format, basically. So it's not going to be perfect. People might put a circle. They might scribble. They might not put an X in the box.

[11:45] **Hannah Moran:** There could be many types of markings that you need to look for when you're reading this form. Uh, we can also give Claude a little bit of information about how to interpret this or what the purpose or meaning of this form is. And all of this is context that is hopefully really going to help Claude, um, do a better job analyzing the form.

[12:00] (Christian clicks "Run", then "Preview". Claude's response is more structured, detailing "Form Analysis", "Driver A (Left Column)", "Driver B (Right Column)", "Total boxes checked", "Sketch Analysis", and "Fault Determination". The fault determination is "Vehicle B is clearly at fault because: 1. Vehicle A was parked/stopped (stationary) as indicated by the checked box and confirmed by the sketch notation "stillstående". 2. Vehicle B was actively turning right when the collision occurred. 3. A moving vehicle that strikes a stationary vehicle is typically at fault, especially when the stationary vehicle was properly parked/stopped. This is a straightforward case where the moving vehicle (B) failed to avoid the stationary vehicle (A) during a right turn maneuver. No additional follow-up information should be needed to determine liability, as the circumstances clearly indicate Vehicle B's responsibility for the accident.")
**Hannah Moran:** So if we run it, everything else is still the same. So we've kept the same user prompt down here, oh your scroll is backwards from mine, uh, we have the same user prompt here, still asking Claude to do the same task, same context. And we'll see here that it's spending less time, it's kind of narrating to us a little bit less about what the form is because it already knows what that is. And it's not concerned with kind of bringing us that information back. It's going to give us a whole list of what it found to be checked, what the sketch shows, and here Claude is now becoming much more confident. With this additional context that we gave to Claude, Claude now feels it's appropriate to say, vehicle B was at fault in this case based on this drawing and based on this sketch. So already we're seeing some improvement in the way Claude is analyzing these.

[12:48] **Hannah Moran:** I think we could probably all agree if we looked at the drawing and at the list that vehicle B is at fault. Um, so we'd like to see that. Uh, so we're going to go back to the slides and talk about a couple of other items that we're not really using in this prompt, but can be really helpful to building up uh, building up your prompt and making it work better.

[13:08] (The slide changes to "Let's build a great prompt from scratch in the console". "5. Examples" is highlighted.)
**Christian Ryan:** Indeed. Thank you so much. So, as Hannah mentioned, um, we sort of set the stage in this prompt to make sure that Claude's really acting on our behalf in the right manner. Um, and a key step that we also add towards the end of this prompt that I'm going to show you in a second is a simple sort of guidelines or reminder part as well. Just strengthening it, reinforcing exactly what we want to get out of it. And one important piece is actually output formatting. You can imagine if you're a data engineer working on this LLM application, all this sort of fancy preamble is great, but at the end of the day, you want your piece of information to to be stored in, let's say, your SQL database, wherever you want to store that data. And the rest of it that is necessary for Claude to sort of give its verdict isn't really that necessary for your application. You want the Nitty Gritty information for your application.

[14:27] (The slide changes to ">> providing examples" with bullet points on the left and a sample user prompt with an XML structured itinerary and prefilled assistant response on the right.)
**Christian Ryan:** You can see here as well, this is just a little example of how we do this. Again, really emphasizing the sort of XML structure that we we, um, we enjoy. It, it gives a lot of structure to Claude. It's what it's been fine-tuned on as well. Um, and it works perfectly well for this example. And in our case, we're not doing this just because it's a simple demo, but you can realistically imagine if you were building this for an insurance company, you would have tens, maybe even hundreds of examples that are quite difficult, maybe in the gray, that you'd like to make sure that Claude actually has some basis in to make the verdict next time.

[14:59] (The slide changes to "Let's build a great prompt from scratch in the console". "6. Conversation history" is highlighted.)
**Christian Ryan:** Um, another topic we really want to highlight, which we're not doing in this demo, is conversation history. It's in the same vein as examples. Uh, we use this to make sure that the enough context-rich information is at Claude's disposal when it when it when Claude's working on on on your behalf. Um, in our case now, this isn't really a user-facing LLM application. It's more something happening in the background. You could imagine for this insurance company, they have this automated system, some data is generated out of this, and then you might have a human in the loop towards the end. If you were to build something much more user-facing where you'd have a long conversation history, that would be, um, relevant to bring in, this is a perfect place in the system prompt to include because it enriches the context that Claude works within.

[15:44] (The speakers are back on screen.)
**Christian Ryan:** Um, in our case, we haven't done so. But what we do is, in the next step, is try to make sure we give a concrete reminder of the task at hand.

[15:56] (The speakers are back on screen, Christian hands Hannah a small remote control.)
**Hannah Moran:** So now we're going to build out the final part of this prompt for Claude, and that's coming back to the reminder of what the immediate task is, and giving Claude a reminder about any important guidelines that we want it to follow.

[16:08] (The slide changes to ">> preventing hallucinations" with four boxes: "Have Claude say "I don't know" if it doesn't know", "Tell Claude to answer only if it is very confident in its response", "Have Claude think before answering", and "Ask Claude to find relevant quotes from long documents then answer using the quotes".)
**Hannah Moran:** Some reasons that we may do this are, A, preventing hallucinations, um, so we want Claude to, uh, not invent details that it's not finding in this prompt, right? Or not finding in the data. If Claude can't tell which form is checked, we don't want Claude to to take its best guess or invent the idea that a box might be checked when it's not. If the sketch is unintelligible, the person did a really bad job drawing this drawing and even a human would not be able to figure it out, we want Claude to be able to say that. And so these are some of the things we'll include in this final reminder and kind of wrap up step for Claude. Uh, remind it to do things like answer only if it's very confident. We could even ask it to refer back to what it has seen in the form anytime it's making a factual claim. So if it wants to say, vehicle B turned right, it should say, I know this based on the fact that box two is clearly checked or whatever it might be. We can kind of give Claude some guidelines about that.

[17:01] (The speakers are back on screen.)
**Hannah Moran:** So if we go back to the console, we can see the next version of the prompt.
(The screen transitions to the Anthropic Console. The prompt is "Accident Report Form Analysis - v4". The "User" prompt area now includes a detailed list of numbered `<tasks>`.)
**Hannah Moran:** And we're going to keep, uh, we're going to keep everything the same here in the system prompt. So we're not changing any of that background context that we gave to Claude about the form, about how it's going to fill it all out. We're not changing anything else about the context and the role. We're just adding this detailed list of tasks. And this is how we want Claude to go about analyzing this. And a really key thing that we found here, as we were building this demo and when we were working on the customer example is that the order in which Claude analyzes this information is very important.

[17:35] **Hannah Moran:** And this is analogous to way you might think about doing this if you were a human. You would probably not look at the drawing first and try to understand what was going on, right? It's pretty unclear. It's a bunch of boxes and lines. We don't really know what that drawing is supposed to mean without any additional context. But if we have the form, and we can read the form first and understand that we're talking about a car accident, and that we're seeing some checkboxes that indicate what vehicles were doing at certain times, then we know a little bit more about how to understand what might be in the drawing. And so that's the kind of detail that we're going to give Claude here is to say, hey, first, go look at the form. Look at it very carefully. Make sure you can tell what boxes are checked. Make sure you're not missing anything here. Um, make a list for yourself of what you see in that. And then move on to the sketch.

[18:22] (Christian clicks "Run", then "Preview". Claude's response now includes a very detailed, itemized analysis of each box in the form, followed by a sketch analysis and fault determination.)
**Hannah Moran:** So after you've kind of confidently gotten information out of the form and you can say what's factually true, then you can go on and think about what you can gain from that sketch, keeping in mind your understanding of the accident so far. So whatever you've learned from the form, you're trying to match that up with the sketch. And that's how you're going to arrive, um, at your final, uh, at your final assessment of the form. And we'll run it. And here you can see one behavior that this produced for Claude. Because I told it to very carefully examine the form, it's showing me it's work as it does that. So it's telling me, each individual box, is the box checked? Is it not checked? And so this is one thing you'll notice as you do prompt engineering. In our previous prompts, we were kind of letting Claude decide how much it wanted to tell us about what it saw in the form.

[19:15] **Hannah Moran:** Here, because I've told it, carefully examine each and every box, it's very carefully examining each and every box. And that might not be what we want in the end, so that's something we might change. Um, but it's also going to give me these other things that I asked for in XML tags. So a nice analysis of the form, the accident summary so far. It's going to give me a sketch analysis, and it's going to continue to say that vehicle B appears to be clearly at fault. In this in this example, it's a pretty simple example. With more complicated drawings, more, uh, less clarity in the forms, this kind of step-by-step thinking for Claude is really impactful in its ability to make a correct assessment here.

[19:54] (The speakers are back on screen.)
**Hannah Moran:** Um, so I think we'll go back to the slides and Christian's going to talk about a last kind of piece that we might add to this, um, to really make it useful for a real-world task.

[20:04] (The slide changes to "Let's build a great prompt from scratch in the console". "8. Thinking step by step / take a deep breath" is highlighted.)
**Christian Ryan:** Indeed. Thank you so much. So, as Hannah mentioned, um, we sort of set the stage in this prompt to make sure that Claude's really acting on our behalf in the right manner. Um, and a key step that we also add towards the end of this prompt that I'm going to show you in a second is a simple sort of guidelines or reminder part as well. Just strengthening it, reinforcing exactly what we want to get out of it. And one important piece is actually output formatting.

[20:29] (The slide changes. "9. Output formatting" is highlighted.)
**Christian Ryan:** You can imagine if you're a data engineer working on this LLM application, all this sort of fancy preamble is great, but at the end of the day, you want your piece of information to to be stored in, let's say, your SQL database, wherever you want to store that data. And the rest of it that is necessary for Claude to sort of give its verdict isn't really that necessary for your application. You want the Nitty Gritty information for your application.

[20:50] (The screen transitions to the Anthropic Console. The prompt is "Accident Report Form Analysis - v5". The "User" prompt area now includes "Important guidelines" and a final instruction "Wrap your final verdict in <final_verdict> XML tags.".)
**Christian Ryan:** So if we transition back to the console, you'll see here that we've just added a simple importance guidelines part. And again, this is just reinforcing the sort of mechanical behavior that we want out of Claude here. We want to make sure that the summary is clear, concise, and accurate. We want to make sure that nothing is sort of impeding and in in Claude's assessment, uh, apart from the data is analyzing. And then finally, when it comes to output formatting, in my case here, I'm just going to ask Claude to wrap its final verdict. All other stuff I'm actually going to ignore for my application and just look at what it's actually assessing.

[21:28] (Christian clicks "Run". The response shows Claude's detailed thinking and then a final summary within `<final_verdict>` XML tags.)
**Christian Ryan:** And that is I can I can use this, uh, if I want to build some sort of, uh, analytics tool after this as well. Or if I just want a clear cut, um, uh, determination. This is the way I can do so. So if I just run this here, you'll see what it's going through the same sort of process that we've seen before. In this case, it's much more succinct because we've asked it to be to summarize its findings in a in a much more straightforward manner. And then finally, towards the end, you'll see that it'll wrap my output in these final verdict XML tags. So you can see that during this demo, we've gone from a skiing accident to to sort of unconfident, insecure outputs from perhaps a car accident in the second version to now a much more strictly formatted, confident output that we can actually build an LLM application around. And actually help, you know, a real-world, um, car insurance company, for example.

[22:16] (The slide changes back to the 10-point "Let's build a great prompt from scratch in the console" diagram. "10. Prefilled response (if any)" is highlighted.)
**Christian Ryan:** Um, and finally if we transition back to the, um, slides, another key way of shaping Claude's output is actually putting words in Claude's mouth. Or, as we call it, prefilled responses.

[22:29] (The slide changes to "Prefill Claude's response" with bullet points and a console example. The console shows an assistant response starting with `<itinerary>`.)
**Christian Ryan:** You could imagine that parsing XML tags is nice and all, but maybe you want a structured JSON output to make sure that it's JSON serializable and you can use this in a subsequent subsequent call, for example. Um, this is quite simple to do. You could just add that, um, Claude needs to begin its output with a certain format. This could be, for example, a uh, open squarely bracket, for example, or even in this case that we see in front of us, this would be an XML tag for itinerary. In our case, it could also be that final verdict XML tag. Um, and this is just a great way of again, shaping how Claude is supposed to respond, um, without all the preamble if you don't want that, even though that is also key in shaping its output to make sure that Claude is reasoning through the steps that we want it.

[23:15] (The speakers are back on screen.)
**Christian Ryan:** So, in our case here, we would just wrap it in the final verdict and then parse it off to it. But you can use prefill as well. Now, finally, one step that I would like to highlight here as well is that both Claude 2.1 and especially Claude 4, of course, is a has a sort of a hybrid reasoning model, meaning that there's extended thinking at your disposal.

[23:36] (The slide changes to "Extended thinking vs. prompt engineering" with two columns: "When do we want to use extended thinking?" and "What are the cons of extended thinking?". "When" points are: "Great first step to give Claude more time to think", "Follow the traces to understand how Claude thinks -> Use this to steer the system prompt". "Cons" points are: "Can often need to 'reinvent the wheel' in its thinking process, which leads to higher token usage", "As thinking requires temperature = 1, can at times be less reproducible".)
**Christian Ryan:** Um, and this is something we want to highlight because you can use extended thinking as a crutch for your prompt engineering. Basically, you can enable this to make sure that Claude actually has time to think. It adds these thinking tags in the scratch pad. Um, and the beauty of that is that you can actually analyze that transcript to understand how Claude is going about that data. So, as we mentioned, we have these checkboxes where it goes through step-by-step of the scenario that transpired for the accident. And in many ways there, you can actually try to help Claude in building this into the system prompt itself. This is not only more token efficient, but it's a good way of understanding how these intelligent models that don't have our intuition actually go about the data that we provide them. And because of that, it's quite key in actually trying to break down how your system prompt can get a lot better.

[24:19] (The speakers are back on screen, smiling.)
**Christian Ryan:** Um, and with that said, I think, uh, I'd like to thank all of you for coming today. We'll be around as well. So if you have any questions on prompting, please, uh, please go ahead. I know there's a prompting.
**Hannah Moran:** You want to learn more about prompting in an hour, we have prompting for agents. And right now, we have an amazing demo of Claude plays Pokémon. So don't go anywhere for that. And as Christian said, we'll be around all day. So I know we didn't have time for Q&A in this session, but um, please come find us if you want to chat, and thank you guys for coming.
**Christian Ryan:** Thank you so much.

[24:46] (The screen fades to black, then a white loading icon appears, followed by a white logo "c/c" with a smiley face under it and the text "Thank you".)

_This presentation provided an introduction to prompt engineering, highlighting the iterative process of developing effective prompts and specific best practices for structuring prompts and preventing hallucinations, demonstrated through a car accident analysis scenario._

</details>


## Additional Sources Scraped

<details>
<summary>building-effective-ai-agents-anthropic</summary>

The provided markdown content is an external article from Anthropic titled "Building effective agents".
The article guidelines, on the other hand, describe the content and structure of *Lesson 22* of a specific course, which is about "Implementing the foundations of Brown the writing workflow (such as applying the orchestrator-worker pattern to generate media items + context engineering to write high-quality articles that follow a specific pattern)".

The Anthropic article is a general piece on agents and workflows, discussing concepts like prompt chaining, routing, parallelization, orchestrator-workers, and evaluator-optimizer patterns, which are foundational to AI agents. Some of these patterns, such as the orchestrator-worker pattern, are explicitly mentioned in the Lesson 22 outline ("Generating Media Items Using the Orchestrator-Worker Pattern") and syllabus (L05, L22, L23). This suggests the Anthropic article is likely a valuable "Golden Source" or foundational reading, especially for the intended audience of aspiring AI engineers learning about these concepts.

Therefore, the entire Anthropic article is relevant core textual content for the course's broader understanding of agentic systems and patterns, even if it's not the direct text of Lesson 22 itself. The task is to "keep only the core textual content (...) that is pertinent to the article guidelines". Given that the guidelines outline a lesson *within a course on AI agents and LLM workflows*, and the Anthropic article discusses these very concepts, it serves as highly pertinent background or reference material for the student.

The elements to remove are "headers, footers, navigation bars, advertisements, sidebars, self-promotion, call-to-actions, etc."

Looking at the content:
*   `[Engineering at Anthropic](https://www.anthropic.com/engineering)`: A link, acts as a header/branding. Remove.
*   `https://www-cdn.anthropic.com/images/4zrzovbb/website/039b6648c28eb33070a63a58d49013600b229238-2554x2554.svg`: Logo/branding. Remove.
*   `Published Dec 19, 2024`: Publication date, meta-information. Remove.
*   The actual article content (from "We've worked with dozens of teams..." to "…Please provide your email address if you’d like to receive our monthly developer newsletter. You can unsubscribe at any time.") is the core textual content.
*   `https://www.anthropic.com/_next/image?url=...`: These are diagrams that support the article's explanations. The guidelines mention "Provide simple examples or diagrams where useful" and diagrams are part of understanding the concepts. Keep.
*   `### Acknowledgements`: Standard article acknowledgment, but not part of the core *educational* content for the student. It's meta-information about the article's authors. Remove.
*   `## Appendix 1: Agents in practice` and `## Appendix 2: Prompt engineering your tools`: These appendices provide deeper insights into practical applications and tool prompt engineering, which are highly relevant to an "AI agents and LLM workflows" course and the specific lesson (L22) on implementing workflows and using tools. Keep as core content.
*   `## Get the developer newsletter` and "Please provide your email address...": This is a call-to-action for self-promotion/marketing. Remove.

```markdown
# Building effective agents

We've worked with dozens of teams building LLM agents across industries. Consistently, the most successful implementations use simple, composable patterns rather than complex frameworks.

Over the past year, we've worked with dozens of teams building large language model (LLM) agents across industries. Consistently, the most successful implementations weren't using complex frameworks or specialized libraries. Instead, they were building with simple, composable patterns.

In this post, we share what we’ve learned from working with our customers and building agents ourselves, and give practical advice for developers on building effective agents.

## What are agents?

"Agent" can be defined in several ways. Some customers define agents as fully autonomous systems that operate independently over extended periods, using various tools to accomplish complex tasks. Others use the term to describe more prescriptive implementations that follow predefined workflows. At Anthropic, we categorize all these variations as **agentic systems**, but draw an important architectural distinction between **workflows** and **agents**:

- **Workflows** are systems where LLMs and tools are orchestrated through predefined code paths.
- **Agents**, on the other hand, are systems where LLMs dynamically direct their own processes and tool usage, maintaining control over how they accomplish tasks.

Below, we will explore both types of agentic systems in detail. In Appendix 1 (“Agents in Practice”), we describe two domains where customers have found particular value in using these kinds of systems.

<h2>When (and when not) to use agents</h2>

When building applications with LLMs, we recommend finding the simplest solution possible, and only increasing complexity when needed. This might mean not building agentic systems at all. Agentic systems often trade latency and cost for better task performance, and you should consider when this tradeoff makes sense.

When more complexity is warranted, workflows offer predictability and consistency for well-defined tasks, whereas agents are the better option when flexibility and model-driven decision-making are needed at scale. For many applications, however, optimizing single LLM calls with retrieval and in-context examples is usually enough.

<h2>When and how to use frameworks</h2>

There are many frameworks that make agentic systems easier to implement, including:

- The [Claude Agent SDK](https://platform.claude.com/docs/en/agent-sdk/overview);
- Amazon Bedrock's [AI Agent framework](https://aws.amazon.com/bedrock/agents/);
- [Rivet](https://rivet.ironcladapp.com/), a drag and drop GUI LLM workflow builder; and
- [Vellum](https://www.vellum.ai/), another GUI tool for building and testing complex workflows.

These frameworks make it easy to get started by simplifying standard low-level tasks like calling LLMs, defining and parsing tools, and chaining calls together. However, they often create extra layers of abstraction that can obscure the underlying prompts ​​and responses, making them harder to debug. They can also make it tempting to add complexity when a simpler setup would suffice.

We suggest that developers start by using LLM APIs directly: many patterns can be implemented in a few lines of code. If you do use a framework, ensure you understand the underlying code. Incorrect assumptions about what's under the hood are a common source of customer error.

See our [cookbook](https://github.com/anthropics/anthropic-cookbook/tree/main/patterns/agents) for some sample implementations.

## Building blocks, workflows, and agents

In this section, we’ll explore the common patterns for agentic systems we’ve seen in production. We'll start with our foundational building block—the augmented LLM—and progressively increase complexity, from simple compositional workflows to autonomous agents.

### Building block: The augmented LLM

The basic building block of agentic systems is an LLM enhanced with augmentations such as retrieval, tools, and memory. Our current models can actively use these capabilities—generating their own search queries, selecting appropriate tools, and determining what information to retain.

https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fd3083d3f40bb2b6f477901cc9a240738d3dd1371-2401x1000.png&w=3840&q=75
The augmented LLM

We recommend focusing on two key aspects of the implementation: tailoring these capabilities to your specific use case and ensuring they provide an easy, well-documented interface for your LLM. While there are many ways to implement these augmentations, one approach is through our recently released [Model Context Protocol](https://www.anthropic.com/news/model-context-protocol), which allows developers to integrate with a growing ecosystem of third-party tools with a simple [client implementation](https://modelcontextprotocol.io/tutorials/building-a-client#building-mcp-clients).

For the remainder of this post, we'll assume each LLM call has access to these augmented capabilities.

### Workflow: Prompt chaining

Prompt chaining decomposes a task into a sequence of steps, where each LLM call processes the output of the previous one. You can add programmatic checks (see "gate” in the diagram below) on any intermediate steps to ensure that the process is still on track.

https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F7418719e3dab222dccb379b8879e1dc08ad34c78-2401x1000.png&w=3840&q=75
The prompt chaining workflow

**When to use this workflow:** This workflow is ideal for situations where the task can be easily and cleanly decomposed into fixed subtasks. The main goal is to trade off latency for higher accuracy, by making each LLM call an easier task.

**Examples where prompt chaining is useful:**

- Generating Marketing copy, then translating it into a different language.
- Writing an outline of a document, checking that the outline meets certain criteria, then writing the document based on the outline.

<h3>Workflow: Routing</h3>

Routing classifies an input and directs it to a specialized followup task. This workflow allows for separation of concerns, and building more specialized prompts. Without this workflow, optimizing for one kind of input can hurt performance on other inputs.

https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F5c0c0e9fe4def0b584c04d37849941da55e5e71c-2401x1000.png&w=3840&q=75
The routing workflow

**When to use this workflow:** Routing works well for complex tasks where there are distinct categories that are better handled separately, and where classification can be handled accurately, either by an LLM or a more traditional classification model/algorithm.

**Examples where routing is useful:**

- Directing different types of customer service queries (general questions, refund requests, technical support) into different downstream processes, prompts, and tools.
- Routing easy/common questions to smaller, cost-efficient models like Claude Haiku 4.5 and hard/unusual questions to more capable models like Claude Sonnet 4.5 to optimize for best performance.

<h3>Workflow: Parallelization</h3>

LLMs can sometimes work simultaneously on a task and have their outputs aggregated programmatically. This workflow, parallelization, manifests in two key variations:

- **Sectioning**: Breaking a task into independent subtasks run in parallel.
- **Voting:** Running the same task multiple times to get diverse outputs.

https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F406bb032ca007fd1624f261af717d70e6ca86286-2401x1000.png&w=3840&q=75
The parallelization workflow

**When to use this workflow:** Parallelization is effective when the divided subtasks can be parallelized for speed, or when multiple perspectives or attempts are needed for higher confidence results. For complex tasks with multiple considerations, LLMs generally perform better when each consideration is handled by a separate LLM call, allowing focused attention on each specific aspect.

**Examples where parallelization is useful:**

- **Sectioning**:
  - Implementing guardrails where one model instance processes user queries while another screens them for inappropriate content or requests. This tends to perform better than having the same LLM call handle both guardrails and the core response.
  - Automating evals for evaluating LLM performance, where each LLM call evaluates a different aspect of the model’s performance on a given prompt.
- **Voting**:
  - Reviewing a piece of code for vulnerabilities, where several different prompts review and flag the code if they find a problem.
  - Evaluating whether a given piece of content is inappropriate, with multiple prompts evaluating different aspects or requiring different vote thresholds to balance false positives and negatives.

<h3>Workflow: Orchestrator-workers</h3>

In the orchestrator-workers workflow, a central LLM dynamically breaks down tasks, delegates them to worker LLMs, and synthesizes their results.

https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F8985fc683fae4780fb34eab1365ab78c7e51bc8e-2401x1000.png&w=3840&q=75
The orchestrator-workers workflow

**When to use this workflow:** This workflow is well-suited for complex tasks where you can’t predict the subtasks needed (in coding, for example, the number of files that need to be changed and the nature of the change in each file likely depend on the task). Whereas it’s topographically similar, the key difference from parallelization is its flexibility—subtasks aren't pre-defined, but determined by the orchestrator based on the specific input.

**Example where orchestrator-workers is useful:**

- Coding products that make complex changes to multiple files each time.
- Search tasks that involve gathering and analyzing information from multiple sources for possible relevant information.

<h3>Workflow: Evaluator-optimizer</h3>

In the evaluator-optimizer workflow, one LLM call generates a response while another provides evaluation and feedback in a loop.

https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F14f51e6406ccb29e695da48b17017e899a6119c7-2401x1000.png&w=3840&q=75
The evaluator-optimizer workflow

**When to use this workflow:** This workflow is particularly effective when we have clear evaluation criteria, and when iterative refinement provides measurable value. The two signs of good fit are, first, that LLM responses can be demonstrably improved when a human articulates their feedback; and second, that the LLM can provide such feedback. This is analogous to the iterative writing process a human writer might go through when producing a polished document.

**Examples where evaluator-optimizer is useful:**

- Literary translation where there are nuances that the translator LLM might not capture initially, but where an evaluator LLM can provide useful critiques.
- Complex search tasks that require multiple rounds of searching and analysis to gather comprehensive information, where the evaluator decides whether further searches are warranted.

### Agents

Agents are emerging in production as LLMs mature in key capabilities—understanding complex inputs, engaging in reasoning and planning, using tools reliably, and recovering from errors. Agents begin their work with either a command from, or interactive discussion with, the human user. Once the task is clear, agents plan and operate independently, potentially returning to the human for further information or judgement. During execution, it's crucial for the agents to gain “ground truth” from the environment at each step (such as tool call results or code execution) to assess its progress. Agents can then pause for human feedback at checkpoints or when encountering blockers. The task often terminates upon completion, but it’s also common to include stopping conditions (such as a maximum number of iterations) to maintain control.

Agents can handle sophisticated tasks, but their implementation is often straightforward. They are typically just LLMs using tools based on environmental feedback in a loop. It is therefore crucial to design toolsets and their documentation clearly and thoughtfully. We expand on best practices for tool development in Appendix 2 ("Prompt Engineering your Tools").

https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F58d9f10c985c4eb5d5379dea315f7bb5ab6249e-2401x1000.png&w=3840&q=75
Autonomous agent

**When to use agents:** Agents can be used for open-ended problems where it’s difficult or impossible to predict the required number of steps, and where you can’t hardcode a fixed path. The LLM will potentially operate for many turns, and you must have some level of trust in its decision-making. Agents' autonomy makes them ideal for scaling tasks in trusted environments.

The autonomous nature of agents means higher costs, and the potential for compounding errors. We recommend extensive testing in sandboxed environments, along with the appropriate guardrails.

**Examples where agents are useful:**

The following examples are from our own implementations:

- A coding Agent to resolve [SWE-bench tasks](https://www.anthropic.com/research/swe-bench-sonnet), which involve edits to many files based on a task description;
- Our [“computer use” reference implementation](https://github.com/anthropics/anthropic-quickstarts/tree/main/computer-use-demo), where Claude uses a computer to accomplish tasks.

https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F4b9a1f4eb63d5962a6e1746ac26bbc857cf3474f-2400x1666.png&w=3840&q=75
High-level flow of a coding agent

## Combining and customizing these patterns

These building blocks aren't prescriptive. They're common patterns that developers can shape and combine to fit different use cases. The key to success, as with any LLM features, is measuring performance and iterating on implementations. To repeat: you should consider adding complexity _only_ when it demonstrably improves outcomes.

## Summary

Success in the LLM space isn't about building the most sophisticated system. It's about building the _right_ system for your needs. Start with simple prompts, optimize them with comprehensive evaluation, and add multi-step agentic systems only when simpler solutions fall short.

When implementing agents, we try to follow three core principles:

1. Maintain **simplicity** in your agent's design.
2. Prioritize **transparency** by explicitly showing the agent’s planning steps.
3. Carefully craft your agent-computer interface (ACI) through thorough tool **documentation and testing**.

Frameworks can help you get started quickly, but don't hesitate to reduce abstraction layers and build with basic components as you move to production. By following these principles, you can create agents that are not only powerful but also reliable, maintainable, and trusted by their users.

## Appendix 1: Agents in practice

Our work with customers has revealed two particularly promising applications for AI agents that demonstrate the practical value of the patterns discussed above. Both applications illustrate how agents add the most value for tasks that require both conversation and action, have clear success criteria, enable feedback loops, and integrate meaningful human oversight.

### A. Customer support

Customer support combines familiar chatbot interfaces with enhanced capabilities through tool integration. This is a natural fit for more open-ended agents because:

- Support interactions naturally follow a conversation flow while requiring access to external information and actions;
- Tools can be integrated to pull customer data, order history, and knowledge base articles;
- Actions such as issuing refunds or updating tickets can be handled programmatically; and
- Success can be clearly measured through user-defined resolutions.

Several companies have demonstrated the viability of this approach through usage-based pricing models that charge only for successful resolutions, showing confidence in their agents' effectiveness.

### B. Coding agents

The software development space has shown remarkable potential for LLM features, with capabilities evolving from code completion to autonomous problem-solving. Agents are particularly effective because:

- Code solutions are verifiable through automated tests;
- Agents can iterate on solutions using test results as feedback;
- The problem space is well-defined and structured; and
- Output quality can be measured objectively.

In our own implementation, agents can now solve real GitHub issues in the [SWE-bench Verified](https://www.anthropic.com/research/swe-bench-sonnet) benchmark based on the pull request description alone. However, whereas automated testing helps verify functionality, human review remains crucial for ensuring solutions align with broader system requirements.

<h2>Appendix 2: Prompt engineering your tools</h2>

No matter which agentic system you're building, tools will likely be an important part of your agent. [Tools](https://www.anthropic.com/news/tool-use-ga) enable Claude to interact with external services and APIs by specifying their exact structure and definition in our API. When Claude responds, it will include a [tool use block](https://docs.anthropic.com/en/docs/build-with-claude/tool-use#example-api-response-with-a-tool-use-content-block) in the API response if it plans to invoke a tool. Tool definitions and specifications should be given just as much prompt engineering attention as your overall prompts. In this brief appendix, we describe how to prompt engineer your tools.

There are often several ways to specify the same action. For instance, you can specify a file edit by writing a diff, or by rewriting the entire file. For structured output, you can return code inside markdown or inside JSON. In software engineering, differences like these are cosmetic and can be converted losslessly from one to the other. However, some formats are much more difficult for an LLM to write than others. Writing a diff requires knowing how many lines are changing in the chunk header before the new code is written. Writing code inside JSON (compared to markdown) requires extra escaping of newlines and quotes.

Our suggestions for deciding on tool formats are the following:

- Give the model enough tokens to "think" before it writes itself into a corner.
- Keep the format close to what the model has seen naturally occurring in text on the internet.
- Make sure there's no formatting "overhead" such as having to keep an accurate count of thousands of lines of code, or string-escaping any code it writes.

One rule of thumb is to think about how much effort goes into human-computer interfaces (HCI), and plan to invest just as much effort in creating good _agent_-computer interfaces (ACI). Here are some thoughts on how to do so:

- Put yourself in the model's shoes. Is it obvious how to use this tool, based on the description and parameters, or would you need to think carefully about it? If so, then it’s probably also true for the model. A good tool definition often includes example usage, edge cases, input format requirements, and clear boundaries from other tools.
- How can you change parameter names or descriptions to make things more obvious? Think of this as writing a great docstring for a junior developer on your team. This is especially important when using many similar tools.
- Test how the model uses your tools: Run many example inputs in our [workbench](https://console.anthropic.com/workbench) to see what mistakes the model makes, and iterate.
- [Poka-yoke](https://en.wikipedia.org/wiki/Poka-yoke) your tools. Change the arguments so that it is harder to make mistakes.

While building our agent for [SWE-bench](https://www.anthropic.com/research/swe-bench-sonnet), we actually spent more time optimizing our tools than the overall prompt. For example, we found that the model would make mistakes with tools using relative filepaths after the agent had moved out of the root directory. To fix this, we changed the tool to always require absolute filepaths—and we found that the model used this method flawlessly.
```

</details>

<details>
<summary>effective-context-engineering-for-ai-agents-anthropic</summary>

# Effective context engineering for AI agents

Context is a critical but finite resource for AI agents. In this post, we explore strategies for effectively curating and managing the context that powers them.

After a few years of prompt engineering being the focus of attention in applied AI, a new term has come to prominence: **context engineering**. Building with language models is becoming less about finding the right words and phrases for your prompts, and more about answering the broader question of “what configuration of context is most likely to generate our model’s desired behavior?"

**Context** refers to the set of tokens included when sampling from a large-language model (LLM). The **engineering** problem at hand is optimizing the utility of those tokens against the inherent constraints of LLMs in order to consistently achieve a desired outcome. Effectively wrangling LLMs often requires _thinking in context_— in other words: considering the holistic state available to the LLM at any given time and what potential behaviors that state might yield.

In this post, we’ll explore the emerging art of context engineering and offer a refined mental model for building steerable, effective agents.

## Context engineering vs. prompt engineering

At Anthropic, we view context engineering as the natural progression of prompt engineering. Prompt engineering refers to methods for writing and organizing LLM instructions for optimal outcomes (see [our docs](https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/overview) for an overview and useful prompt engineering strategies). **Context engineering** refers to the set of strategies for curating and maintaining the optimal set of tokens (information) during LLM inference, including all the other information that may land there outside of the prompts.

In the early days of engineering with LLMs, prompting was the biggest component of AI engineering work, as the majority of use cases outside of everyday chat interactions required prompts optimized for one-shot classification or text generation tasks. As the term implies, the primary focus of prompt engineering is how to write effective prompts, particularly system prompts. However, as we move towards engineering more capable agents that operate over multiple turns of inference and longer time horizons, we need strategies for managing the entire context state (system instructions, tools, [Model Context Protocol](https://modelcontextprotocol.io/docs/getting-started/intro) (MCP), external data, message history, etc).

An agent running in a loop generates more and more data that _could_ be relevant for the next turn of inference, and this information must be cyclically refined. Context engineering is the [art and science](https://x.com/karpathy/status/1937902205765607626?lang=en) of curating what will go into the limited context window from that constantly evolving universe of possible information.

https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Ffaa261102e46c7f090a2402a49000ffae18c5dd6-2292x1290.png&w=3840&q=75
_In contrast to the discrete task of writing a prompt, context engineering is iterative and the curation phase happens each time we decide what to pass to the model._

## Why context engineering is important to building capable agents

Despite their speed and ability to manage larger and larger volumes of data, we’ve observed that LLMs, like humans, lose focus or experience confusion at a certain point. Studies on needle-in-a-haystackstyle benchmarking have uncovered the concept of [context rot](https://research.trychroma.com/context-rot): as the number of tokens in the context window increases, the model’s ability to accurately recall information from that context decreases.

While some models exhibit more gentle degradation than others, this characteristic emerges across all models. Context, therefore, must be treated as a finite resource with diminishing marginal returns. Like humans, who have [limited working memory capacity](https://journals.sagepub.com/doi/abs/10.1177/0963721409359277), LLMs have an “attention budget” that they draw on when parsing large volumes of context. Every new token introduced depletes this budget by some amount, increasing the need to carefully curate the tokens available to the LLM.

This attention scarcity stems from architectural constraints of LLMs. LLMs are based on the [transformer architecture](https://arxiv.org/abs/1706.03762), which enables every token to [attend to every other token](https://huggingface.co/blog/Esmail-AGumaan/attention-is-all-you-need) across the entire context. This results in n² pairwise relationships for n tokens.

As its context length increases, a model's ability to capture these pairwise relationships gets stretched thin, creating a natural tension between context size and attention focus. Additionally, models develop their attention patterns from training data distributions where shorter sequences are typically more common than longer ones. This means models have less experience with, and fewer specialized parameters for, context-wide dependencies.

Techniques like [position encoding interpolation](https://arxiv.org/pdf/2306.15595) allow models to handle longer sequences by adapting them to the originally trained smaller context, though with some degradation in token position understanding. These factors create a performance gradient rather than a hard cliff: models remain highly capable at longer contexts but may show reduced precision for information retrieval and long-range reasoning compared to their performance on shorter contexts.

These realities mean that thoughtful context engineering is essential for building capable agents.

## The anatomy of effective context

Given that LLMs are constrained by a finite attention budget, _good_ context engineering means finding the _smallest_ _possible_ set of high-signal tokens that maximize the likelihood of some desired outcome. Implementing this practice is much easier said than done, but in the following section, we outline what this guiding principle means in practice across the different components of context.

**System prompts** should be extremely clear and use simple, direct language that presents ideas at the _right altitude_ for the agent. The right altitude is the Goldilocks zone between two common failure modes. At one extreme, we see engineers hardcoding complex, brittle logic in their prompts to elicit exact agentic behavior. This approach creates fragility and increases maintenance complexity over time. At the other extreme, engineers sometimes provide vague, high-level guidance that fails to give the LLM concrete signals for desired outputs or falsely assumes shared context. The optimal altitude strikes a balance: specific enough to guide behavior effectively, yet flexible enough to provide the model with strong heuristics to guide behavior.

https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F0442fe138158e84ffce92bed1624dd09f37ac46f-2292x1288.png&w=3840&q=75
_At one end of the spectrum, we see brittle if-else hardcoded prompts, and at the other end we see prompts that are overly general or falsely assume shared context._

We recommend organizing prompts into distinct sections (like `<background_information>`, `<instructions>`, `## Tool guidance`, `## Output description`, etc) and using techniques like XML tagging or Markdown headers to delineate these sections, although the exact formatting of prompts is likely becoming less important as models become more capable.

Regardless of how you decide to structure your system prompt, you should be striving for the minimal set of information that fully outlines your expected behavior. (Note that minimal does not necessarily mean short; you still need to give the agent sufficient information up front to ensure it adheres to the desired behavior.) It’s best to start by testing a minimal prompt with the best model available to see how it performs on your task, and then add clear instructions and examples to improve performance based on failure modes found during initial testing.

**Tools** allow agents to operate with their environment and pull in new, additional context as they work. Because tools define the contract between agents and their information/action space, it’s extremely important that tools promote efficiency, both by returning information that is token efficient and by encouraging efficient agent behaviors.

In [Writing tools for AI agents – with AI agents](https://www.anthropic.com/engineering/writing-tools-for-agents), we discussed building tools that are well understood by LLMs and have minimal overlap in functionality. Similar to the functions of a well-designed codebase, tools should be self-contained, robust to error, and extremely clear with respect to their intended use. Input parameters should similarly be descriptive, unambiguous, and play to the inherent strengths of the model.

One of the most common failure modes we see is bloated tool sets that cover too much functionality or lead to ambiguous decision points about which tool to use. If a human engineer can’t definitively say which tool should be used in a given situation, an AI agent can’t be expected to do better. As we’ll discuss later, curating a minimal viable set of tools for the agent can also lead to more reliable maintenance and pruning of context over long interactions.

Providing examples, otherwise known as few-shot prompting, is a well known best practice that we continue to strongly advise. However, teams will often stuff a laundry list of edge cases into a prompt in an attempt to articulate every possible rule the LLM should follow for a particular task. We do not recommend this. Instead, we recommend working to curate a set of diverse, canonical examples that effectively portray the expected behavior of the agent. For an LLM, examples are the “pictures” worth a thousand words.

Our overall guidance across the different components of context (system prompts **,** tools **,** examples **,** message history, etc) is to be thoughtful and keep your context informative, yet tight. Now let's dive into dynamically retrieving context at runtime.

## Context retrieval and agentic search

In [Building effective AI agents](https://www.anthropic.com/research/building-effective-agents), we highlighted the differences between LLM-based workflows and agents. Since we wrote that post, we’ve gravitated towards a [simple definition](https://simonwillison.net/2025/Sep/18/agents/) for agents: LLMs autonomously using tools in a loop.

Working alongside our customers, we’ve seen the field converging on this simple paradigm. As the underlying models become more capable, the level of autonomy of agents can scale: smarter models allow agents to independently navigate nuanced problem spaces and recover from errors.

We’re now seeing a shift in how engineers think about designing context for agents. Today, many AI-native applications employ some form of embedding-based pre-inference time retrieval to surface important context for the agent to reason over. As the field transitions to more agentic approaches, we increasingly see teams augmenting these retrieval systems with “just in time” context strategies.

Rather than pre-processing all relevant data up front, agents built with the “just in time” approach maintain lightweight identifiers (file paths, stored queries, web links, etc.) and use these references to dynamically load data into context at runtime using tools. Anthropic’s agentic coding solution [Claude Code](https://www.anthropic.com/claude-code) uses this approach to perform complex data analysis over large databases. The model can write targeted queries, store results, and leverage Bash commands like head and tail to analyze large volumes of data without ever loading the full data objects into context. This approach mirrors human cognition: we generally don’t memorize entire corpuses of information, but rather introduce external organization and indexing systems like file systems, inboxes, and bookmarks to retrieve relevant information on demand.

Beyond storage efficiency, the metadata of these references provides a mechanism to efficiently refine behavior, whether explicitly provided or intuitive. To an agent operating in a file system, the presence of a file named `test_utils.py` in a `tests` folder implies a different purpose than a file with the same name located in `src/core_logic/` Folder hierarchies, naming conventions, and timestamps all provide important signals that help both humans and agents understand how and when to utilize information.

Letting agents navigate and retrieve data autonomously also enables progressive disclosure—in other words, allows agents to incrementally discover relevant context through exploration. Each interaction yields context that informs the next decision: file sizes suggest complexity; naming conventions hint at purpose; timestamps can be a proxy for relevance. Agents can assemble understanding layer by layer, maintaining only what's necessary in working memory and leveraging note-taking strategies for additional persistence. This self-managed context window keeps the agent focused on relevant subsets rather than drowning in exhaustive but potentially irrelevant information.

Of course, there's a trade-off: runtime exploration is slower than retrieving pre-computed data. Not only that, but opinionated and thoughtful engineering is required to ensure that an LLM has the right tools and heuristics for effectively navigating its information landscape. Without proper guidance, an agent can waste context by misusing tools, chasing dead-ends, or failing to identify key information.

In certain settings, the most effective agents might employ a hybrid strategy, retrieving some data up front for speed, and pursuing further autonomous exploration at its discretion. The decision boundary for the ‘right’ level of autonomy depends on the task. Claude Code is an agent that employs this hybrid model: [CLAUDE.md](http://claude.md/) files are naively dropped into context up front, while primitives like glob and grep allow it to navigate its environment and retrieve files just-in-time, effectively bypassing the issues of stale indexing and complex syntax trees.

The hybrid strategy might be better suited for contexts with less dynamic content, such as legal or finance work. As model capabilities improve, agentic design will trend towards letting intelligent models act intelligently, with progressively less human curation. Given the rapid pace of progress in the field, "do the simplest thing that works" will likely remain our best advice for teams building agents on top of Claude.

### Context engineering for long-horizon tasks

Long-horizon tasks require agents to maintain coherence, context, and goal-directed behavior over sequences of actions where the token count exceeds the LLM’s context window. For tasks that span tens of minutes to multiple hours of continuous work, like large codebase migrations or comprehensive research projects, agents require specialized techniques to work around the context window size limitation.

Waiting for larger context windows might seem like an obvious tactic. But it's likely that for the foreseeable future, context windows of all sizes will be subject to context pollution and information relevance concerns—at least for situations where the strongest agent performance is desired. To enable agents to work effectively across extended time horizons, we've developed a few techniques that address these context pollution constraints directly: compaction, structured note-taking, and multi-agent architectures.

**Compaction**

Compaction is the practice of taking a conversation nearing the context window limit, summarizing its contents, and reinitiating a new context window with the summary. Compaction typically serves as the first lever in context engineering to drive better long-term coherence. At its core, compaction distills the contents of a context window in a high-fidelity manner, enabling the agent to continue with minimal performance degradation.

In Claude Code, for example, we implement this by passing the message history to the model to summarize and compress the most critical details. The model preserves architectural decisions, unresolved bugs, and implementation details while discarding redundant tool outputs or messages. The agent can then continue with this compressed context plus the five most recently accessed files. Users get continuity without worrying about context window limitations.

The art of compaction lies in the selection of what to keep versus what to discard, as overly aggressive compaction can result in the loss of subtle but critical context whose importance only becomes apparent later. For engineers implementing compaction systems, we recommend carefully tuning your prompt on complex agent traces. Start by maximizing recall to ensure your compaction prompt captures every relevant piece of information from the trace, then iterate to improve precision by eliminating superfluous content.

An example of low-hanging superfluous content is clearing tool calls and results – once a tool has been called deep in the message history, why would the agent need to see the raw result again? One of the safest lightest touch forms of compaction is tool result clearing, most recently launched as a [feature on the Claude Developer Platform](https://www.anthropic.com/news/context-management).

**Structured note-taking**

Structured note-taking, or agentic memory, is a technique where the agent regularly writes notes persisted to memory outside of the context window. These notes get pulled back into the context window at later times.

This strategy provides persistent memory with minimal overhead. Like Claude Code creating a to-do list, or your custom agent maintaining a NOTES.md file, this simple pattern allows the agent to track progress across complex tasks, maintaining critical context and dependencies that would otherwise be lost across dozens of tool calls.

[Claude playing Pokémon](https://www.twitch.tv/claudeplayspokemon) demonstrates how memory transforms agent capabilities in non-coding domains. The agent maintains precise tallies across thousands of game steps—tracking objectives like "for the last 1,234 steps I've been training my Pokémon in Route 1, Pikachu has gained 8 levels toward the target of 10." Without any prompting about memory structure, it develops maps of explored regions, remembers which key achievements it has unlocked, and maintains strategic notes of combat strategies that help it learn which attacks work best against different opponents.

After context resets, the agent reads its own notes and continues multi-hour training sequences or dungeon explorations. This coherence across summarization steps enables long-horizon strategies that would be impossible when keeping all the information in the LLM’s context window alone.

As part of our [Sonnet 4.5 launch](https://www.anthropic.com/effective-context-engineering-for-ai-agents), we released [a memory tool](http://anthropic.com/news/context-management) in public beta on the Claude Developer Platform that makes it easier to store and consult information outside the context window through a file-based system. This allows agents to build up knowledge bases over time, maintain project state across sessions, and reference previous work without keeping everything in context.

**Sub-agent architectures**

Sub-agent architectures provide another way around context limitations. Rather than one agent attempting to maintain state across an entire project, specialized sub-agents can handle focused tasks with clean context windows. The main agent coordinates with a high-level plan while subagents perform deep technical work or use tools to find relevant information. Each subagent might explore extensively, using tens of thousands of tokens or more, but returns only a condensed, distilled summary of its work (often 1,000-2,000 tokens).

This approach achieves a clear separation of concerns—the detailed search context remains isolated within sub-agents, while the lead agent focuses on synthesizing and analyzing the results. This pattern, discussed in [How we built our multi-agent research system](https://www.anthropic.com/engineering/multi-agent-research-system), showed a substantial improvement over single-agent systems on complex research tasks.

The choice between these approaches depends on task characteristics. For example:

-   Compaction maintains conversational flow for tasks requiring extensive back-and-forth;
-   Note-taking excels for iterative development with clear milestones;
-   Multi-agent architectures handle complex research and analysis where parallel exploration pays dividends.

Even as models continue to improve, the challenge of maintaining coherence across extended interactions will remain central to building more effective agents.

## Conclusion

Context engineering represents a fundamental shift in how we build with LLMs. As models become more capable, the challenge isn't just crafting the perfect prompt—it's thoughtfully curating what information enters the model's limited attention budget at each step. Whether you're implementing compaction for long-horizon tasks, designing token-efficient tools, or enabling agents to explore their environment just-in-time, the guiding principle remains the same: find the smallest set of high-signal tokens that maximize the likelihood of your desired outcome.

The techniques we've outlined will continue evolving as models improve. We're already seeing that smarter models require less prescriptive engineering, allowing agents to operate with more autonomy. But even as capabilities scale, treating context as a precious, finite resource will remain central to building reliable, effective agents.

</details>

<details>
<summary>how-we-built-our-multi-agent-research-system-anthropic</summary>

The provided markdown content is an article from Anthropic's engineering blog describing how they built their multi-agent research system.
The article guidelines, on the other hand, detail the structure and content for a specific lesson (L22) on *implementing* the foundations of a *writing workflow*, applying the orchestrator-worker pattern, and context engineering.

While the scraped article discusses relevant concepts like multi-agent systems and the orchestrator-worker pattern (which are mentioned in the lesson guidelines' syllabus and section outlines), it is an external reference/golden source, not the core content for the lesson itself. The guidelines specify what the *lesson* should contain, which is practical implementation details for a *writing workflow*, not a report on Anthropic's internal systems.

Therefore, the entire provided markdown content, while high-quality, is not the "core textual content that is pertinent to the article guidelines" for *this specific lesson*. It would serve as excellent *reference material* (a "golden source" as per the guidelines), but not the lesson's instructional content itself.

The task is to keep *only* the core textual content pertinent to the article guidelines. Since the entire content is an external article that isn't directly the lesson's body, the most accurate action based on the strict interpretation of "keeping only the core textual content... pertinent to the article guidelines" is to remove it all.

However, if the intent was that *any* content discussing relevant concepts should be retained, even if it's not the *actual lesson text*, then the prompt's instructions clash. The prompt says "Focus on keeping only the core textual content... that is pertinent to the article guidelines provided." The guidelines are for a *lesson*, with a specific outline and narrative flow. The scraped content is an *external article* that could *inform* the lesson, but is not *the lesson itself*.

Given the strict instruction: "Focus on keeping only the core textual content (and code content if there are code sections) that is pertinent to the article guidelines provided. Return *only* the cleaned markdown. Do not summarize or rewrite the original content. This task is only about *removing* irrelevant content. Good content should be kept as is, do not touch it."

The scraped content is an engineering blog post about a *research system*, not the "Brown writing workflow". It *explains* principles and architecture that are *relevant* to the lesson's topics (e.g., orchestrator-worker pattern), but it is not the *lesson itself*. The guidelines are for the *lesson*, not for *scraping and cleaning a relevant external article*.

Therefore, none of the content from the Anthropic article is "core textual content" for *this specific lesson*. It's a related article, likely a "Golden Source" as mentioned in the guidelines.

Thus, the cleaned markdown should be empty.

</details>

<details>
<summary>models-langchain-reference</summary>

```markdown
# Chat models

Reference docs

This page contains **reference documentation** for chat models. See [the docs](https://docs.langchain.com/oss/python/langchain/models) for conceptual guides, tutorials, and examples on using chat models.

## ``chat\_models

Entrypoint to using [chat models](https://docs.langchain.com/oss/python/langchain/models) in LangChain.

### ``init\_chat\_model

```
init_chat_model(
    model: str | None = None,
    *,
    model_provider: str | None = None,
    configurable_fields: Literal["any"] | list[str] | tuple[str, ...] | None = None,
    config_prefix: str | None = None,
    **kwargs: Any,
) -> BaseChatModel | _ConfigurableModel
```

Initialize a chat model from any supported provider using a unified interface.

**Two main use cases:**

1. **Fixed model** – specify the model upfront and get a ready-to-use chat model.
2. **Configurable model** – choose to specify parameters (including model name) at
    runtime via `config`. Makes it easy to switch between models/providers without
    changing your code

Note

Requires the integration package for the chosen model provider to be installed.

See the `model_provider` parameter below for specific package names
(e.g., `pip install langchain-openai`).

Refer to the [provider integration's API reference](https://docs.langchain.com/oss/python/integrations/providers)
for supported model parameters to use as `**kwargs`.

| PARAMETER | DESCRIPTION |
| --- | --- |
| #### `model` | The name or ID of the model, e.g. `'o3-mini'`, `'claude-sonnet-4-5-20250929'`.<br>You can also specify model and model provider in a single argument using<br>`'{model_provider}:{model}'` format, e.g. `'openai:o1'`.<br>Will attempt to infer `model_provider` from model if not specified.<br>The following providers will be inferred based on these model prefixes:<br>- `gpt-...` \| `o1...` \| `o3...` -\> `openai`<br>- `claude...` -\> `anthropic`<br>- `amazon...` -\> `bedrock`<br>- `gemini...` -\> `google_vertexai`<br>- `command...` -\> `cohere`<br>- `accounts/fireworks...` -\> `fireworks`<br>- `mistral...` -\> `mistralai`<br>- `deepseek...` -\> `deepseek`<br>- `grok...` -\> `xai`<br>- `sonar...` -\> `perplexity`<br>**TYPE:**`str | None`**DEFAULT:**`None` |
| #### `model_provider` | The model provider if not specified as part of the model arg<br>(see above).<br>Supported `model_provider` values and the corresponding integration package<br>are:<br>- `openai` -\> [`langchain-openai`](https://docs.langchain.com/oss/python/integrations/providers/openai)<br>- `anthropic` -\> [`langchain-anthropic`](https://docs.langchain.com/oss/python/integrations/providers/anthropic)<br>- `azure_openai` -\> [`langchain-openai`](https://docs.langchain.com/oss/python/integrations/providers/openai)<br>- `azure_ai` -\> [`langchain-azure-ai`](https://docs.langchain.com/oss/python/integrations/providers/microsoft)<br>- `google_vertexai` -\> [`langchain-google-vertexai`](https://docs.langchain.com/oss/python/integrations/providers/google)<br>- `google_genai` -\> [`langchain-google-genai`](https://docs.langchain.com/oss/python/integrations/providers/google)<br>- `bedrock` -\> [`langchain-aws`](https://docs.langchain.com/oss/python/integrations/providers/aws)<br>- `bedrock_converse` -\> [`langchain-aws`](https://docs.langchain.com/oss/python/integrations/providers/aws)<br>- `cohere` -\> [`langchain-cohere`](https://docs.langchain.com/oss/python/integrations/providers/cohere)<br>- `fireworks` -\> [`langchain-fireworks`](https://docs.langchain.com/oss/python/integrations/providers/fireworks)<br>- `together` -\> [`langchain-together`](https://docs.langchain.com/oss/python/integrations/providers/together)<br>- `mistralai` -\> [`langchain-mistralai`](https://docs.langchain.com/oss/python/integrations/providers/mistralai)<br>- `huggingface` -\> [`langchain-huggingface`](https://docs.langchain.com/oss/python/integrations/providers/huggingface)<br>- `groq` -\> [`langchain-groq`](https://docs.langchain.com/oss/python/integrations/providers/groq)<br>- `ollama` -\> [`langchain-ollama`](https://docs.langchain.com/oss/python/integrations/providers/ollama)<br>- `google_anthropic_vertex` -\> [`langchain-google-vertexai`](https://docs.langchain.com/oss/python/integrations/providers/google)<br>- `deepseek` -\> [`langchain-deepseek`](https://docs.langchain.com/oss/python/integrations/providers/deepseek)<br>- `ibm` -\> [`langchain-ibm`](https://docs.langchain.com/oss/python/integrations/providers/ibm)<br>- `nvidia` -\> [`langchain-nvidia-ai-endpoints`](https://docs.langchain.com/oss/python/integrations/providers/nvidia)<br>- `xai` -\> [`langchain-xai`](https://docs.langchain.com/oss/python/integrations/providers/xai)<br>- `perplexity` -\> [`langchain-perplexity`](https://docs.langchain.com/oss/python/integrations/providers/perplexity)<br>**TYPE:**`str | None`**DEFAULT:**`None` |
| #### `configurable_fields` | Which model parameters are configurable at runtime:<br>- `None`: No configurable fields (i.e., a fixed model).<br>- `'any'`: All fields are configurable. **See security note below.**<br>- `list[str] | Tuple[str, ...]`: Specified fields are configurable.<br>Fields are assumed to have `config_prefix` stripped if a `config_prefix` is<br>specified.<br>If `model` is specified, then defaults to `None`.<br>If `model` is not specified, then defaults to `("model", "model_provider")`.<br>Security note<br>Setting `configurable_fields="any"` means fields like `api_key`,<br>`base_url`, etc., can be altered at runtime, potentially redirecting<br>model requests to a different service/user.<br>Make sure that if you're accepting untrusted configurations that you<br>enumerate the `configurable_fields=(...)` explicitly.<br>**TYPE:**`Literal['any'] | list[str] | tuple[str, ...] | None`**DEFAULT:**`None` |
| #### `config_prefix` | Optional prefix for configuration keys.<br>Useful when you have multiple configurable models in the same application.<br>If `'config_prefix'` is a non-empty string then `model` will be configurable<br>at runtime via the `config["configurable"]["{config_prefix}_{param}"]` keys.<br>See examples below.<br>If `'config_prefix'` is an empty string then model will be configurable via<br>`config["configurable"]["{param}"]`.<br>**TYPE:**`str | None`**DEFAULT:**`None` |
| #### `**kwargs` | Additional model-specific keyword args to pass to the underlying<br>chat model's `__init__` method. Common parameters include:<br>- `temperature`: Model temperature for controlling randomness.<br>- `max_tokens`: Maximum number of output tokens.<br>- `timeout`: Maximum time (in seconds) to wait for a response.<br>- `max_retries`: Maximum number of retry attempts for failed requests.<br>- `base_url`: Custom API endpoint URL.<br>- `rate_limiter`: A<br>[`BaseRateLimiter`](https://reference.langchain.com/python/langchain_core/rate_limiters/#langchain_core.rate_limiters.BaseRateLimiter "<code class=\"doc-symbol doc-symbol-heading doc-symbol-class\"></code>            <span class=\"doc doc-object-name doc-class-name\">BaseRateLimiter</span>")<br>instance to control request rate.<br>Refer to the specific model provider's<br>[integration reference](https://reference.langchain.com/python/integrations/)<br>for all available parameters.<br>**TYPE:**`Any`**DEFAULT:**`{}` |

| RETURNS | DESCRIPTION |
| --- | --- |
| `BaseChatModel | _ConfigurableModel` | A `BaseChatModel` corresponding to the `model_name` and `model_provider`<br>specified if configurability is inferred to be `False`. If configurable, a<br>chat model emulator that initializes the underlying model at runtime once a<br>config is passed in. |

| RAISES | DESCRIPTION |
| --- | --- |
| `ValueError` | If `model_provider` cannot be inferred or isn't supported. |
| `ImportError` | If the model provider integration package is not installed. |

Initialize a non-configurable model

```
# pip install langchain langchain-openai langchain-anthropic langchain-google-vertexai

from langchain.chat_models import init_chat_model

o3_mini = init_chat_model("openai:o3-mini", temperature=0)
claude_sonnet = init_chat_model("anthropic:claude-sonnet-4-5-20250929", temperature=0)
gemini_2-5_flash = init_chat_model("google_vertexai:gemini-2.5-flash", temperature=0)

o3_mini.invoke("what's your name")
claude_sonnet.invoke("what's your name")
gemini_2-5_flash.invoke("what's your name")
```

Partially configurable model with no default

```
# pip install langchain langchain-openai langchain-anthropic

from langchain.chat_models import init_chat_model

# (We don't need to specify configurable=True if a model isn't specified.)
configurable_model = init_chat_model(temperature=0)

configurable_model.invoke("what's your name", config={"configurable": {"model": "gpt-4o"}})
# Use GPT-4o to generate the response

configurable_model.invoke(
    "what's your name",
    config={"configurable": {"model": "claude-sonnet-4-5-20250929"}},
)
```

Fully configurable model with a default

```
# pip install langchain langchain-openai langchain-anthropic

from langchain.chat_models import init_chat_model

configurable_model_with_default = init_chat_model(
    "openai:gpt-4o",
    configurable_fields="any",  # This allows us to configure other params like temperature, max_tokens, etc at runtime.
    config_prefix="foo",
    temperature=0,
)

configurable_model_with_default.invoke("what's your name")
# GPT-4o response with temperature 0 (as set in default)

configurable_model_with_default.invoke(
    "what's your name",
    config={
        "configurable": {
            "foo_model": "anthropic:claude-sonnet-4-5-20250929",
            "foo_temperature": 0.6,
        }
    },
)
# Override default to use Sonnet 4.5 with temperature 0.6 to generate response
```

Bind tools to a configurable model

You can call any chat model declarative methods on a configurable model in the
same way that you would with a normal model:

```
# pip install langchain langchain-openai langchain-anthropic

from langchain.chat_models import init_chat_model
from pydantic import BaseModel, Field

class GetWeather(BaseModel):
    '''Get the current weather in a given location'''

    location: str = Field(..., description="The city and state, e.g. San Francisco, CA")

class GetPopulation(BaseModel):
    '''Get the current population in a given location'''

    location: str = Field(..., description="The city and state, e.g. San Francisco, CA")

configurable_model = init_chat_model(
    "gpt-4o", configurable_fields=("model", "model_provider"), temperature=0
)

configurable_model_with_tools = configurable_model.bind_tools(
    [\
        GetWeather,\
        GetPopulation,\
    ]
)
configurable_model_with_tools.invoke(
    "Which city is hotter today and which is bigger: LA or NY?"
)
# Use GPT-4o

configurable_model_with_tools.invoke(
    "Which city is hotter today and which is bigger: LA or NY?",
    config={"configurable": {"model": "claude-sonnet-4-5-20250929"}},
)
# Use Sonnet 4.5
```
```
</details>

## Brown Writing Profiles

<details>
<summary>Brown Paul Iusztin Character Profile From `inputs/profiles/character_profiles/paul_iusztin.md`</summary>

## About Paul Iusztin

I’m **Paul Iusztin**. A 29 years old senior AI Engineer and content creator.

I help engineers ship AI products.

I’m the author of the bestseller LLM Engineer’s Handbook, lead instructor of the Agentic AI Engineering course, founding AI Engineer of a San Francisco start-up, and obsessed with making knowledge accessible through AI.

With over 10 years of experience and 20 apps shipped, I teach AI Engineering as I wanted to at the beginning of my career. End-to-end. From idea to production. From data collection to deploying, monitoring and evaluation. With a focus on AI principles, software patterns and infrastructure systems that will thrive in a future dominated by AI coding tools.

My ultimate goal is to help other engineers escape the PoC purgatory and x10 their AI Engineering skills.

I'm also the founder of the **Decoding AI Magazine**, a place for real-world guides taking you from the PoC purgatory to shipping AI products that work.

I founded this magazine to solve the problem I faced for the first five years of my career: escaping the “PoC purgatory.” I realized that finding a team that knows how to ship AI software is rare. Too many AI projects get stuck at Jupyter Notebooks or fancy demos that never see a real user.

Decoding AI is the solution. It’s your weekly hub for learning how to design, build, and ship production-grade AI systems. End-to-end. From idea to production. From data collection to deploying, monitoring and evaluation. With a focus on AI principles, software patterns and infrastructure systems that will thrive in a future dominated by AI coding tools.

Stop building prototypes. Start shipping AI that works.

## Niche

**Broad:** AI Engineering, AI Systems, Software Engineering, Ops
**Specialized**: RAG, LLMs, AI Agents, AI Workflows, LLMOps, AI Evals, AI Monitoring, AI Infrastructure, Serving AI Applications, Memory
**Vibe:** premium, minimalistic, high-quality content, straight-to-the-point content (the opposite of FOMO), zero hype, artsy, builder mindset, deep and real

## Similar Personas

- Andrew Ng
- Chip Huyen
- Sebastian Raschka
- Louis-François Bouchard
- Maxime Labonne
- Jason Liu
- Lex Fridman
- Aleksa Gordic

## My Core Content Pillars I Always Talk About

1. Shipping Production-Ready AI
2. Practical Learning for AI Engineers
3. Behind-the-Scenes of Building in Public
4. AI for Founders & Technical Teams 

## Style

- **Real:** Showing the engineering, social, and economic truth behind AI and content creation
- **Trust:** No FOMO, just real engineering focusing on what matters
- **Minimalist:** Straight to the point engineering. Use only what is required to get shit done.
- **Simple:** “Keep it simple” should be the motto of your persona. Focus on what matters while avoiding FOMO and fluff just to get attention
- **Controversy:**
    - Talk about what everybody thinks, but they're too afraid to say out loud.
    - Going against the flow, Gary Marcus style: "LLMs are stupid," "Agents are stupid,” "LangChain is stupid".

</details>

<details>
<summary>Brown Article Profile From `inputs/profiles/article_profile.md`</summary>

## Tonality

You should write in a humanized way as writing a blog article or book.

Write the description of ideas as fluid as possible. Remember that you are writing a book or blog article. Thus, everything should flow naturally, without too many bullet points or subheaders. Use them only when it really makes sense. Otherwise, stick to normal paragraphs.

## General Article Structure

The article is a collection of blocks that flow naturally one after the other. It starts with one introduction, continues with multiple sections in between and wrap-ups with a conclusion. The information flows naturally from the introduction, to the sections, to the conclusion.

## Introduction Guidelines

The introduction is a short summary of the article, quickly presenting the `why` (problem), `what` (solution), and captivating the reader to continue reading.

- Make it short and concise, it should be a summary of the article transformed into a light, personal and engaging story that makes the reader wanna dig into the whole article
- Make it engaging and interesting, it should be a hook to raise the curiosity of the reader
- Induce curiosity, urgency, or emotional response
- Bring out emotions (Surprise, Shock, Intrigue, Skepticism, fear of missing out, curiosity) (in the hooks mainly)
- Make it memorable and catchy
- The introduction is super highlevel, present a summary of the article in a way that is engaging and increases the 
curiosity of the reader. It should respond to question such as "Why" and "What", not "How".
- Wrap-up the introduction with a clear highlight of what will be covered within the article with a concise, itemized overview.

## Section Guidelines

The sections present the `how`, digging into the solution, where the story is built gradually, from more generic information, to the concrete solution.

- The sections follow a narrative flow, from more general, high-level to more specific, low-level.
- The transition from one section to the next should be smooth and natural, where the introduction sets the stage, 
the sections slowly build up the article, and the conclusion wraps up the article.
- Each section contains a single idea, not a list of ideas.

## Conclusion Guidelines

The conclusion acts as a very short wrap-up, quickly reminding the reader what he learnt.

- Make it short and concise
- Make it memorable and catchy
- If the <article_guideline> doesn't state otherwise, a good conclusion consists of 2-3 sentences paragraph on what the reader learnt during the article and another 2-3 sentences paragraph on connecting our article to the bigger picture (e.g., next steps, applicability, real-world stories on how we applied the learning)
- The conclusion is a short summary of the article, repeating the core ideas of the article as concise as possible. Also, it anchors the reader in the bigger picture and next steps.

## Article Guideline

The article guideline is provided directly by the user, containing exactly what the user wants to be generated. Thus, the article guideline ALWAYS has priority over everything else you've been intruscted. 

Here are some details about the article guideline:
- It contains an outline of the expected article, describing all the components: introduction, sections, and conclusion. You will include in the article only the components suggested in the guideline.
- The article guideline components are already in the right order. Thus, you will respect it.
- Each component from the guideline will contain a description of what it contains. You will follow the order of ideas suggested in it. If the description from from the user guideline is already complete, you will just adapt it to fit within the overall article. Otherwise, if it only has fragments of information, you will fill in the gap based on the provided research and intructions. For example, if a description of a section has detailed instructions on how to write the transitions between sections, what topics to write about, and so on, you will follow them. But if the section description doesn't explictly state a transition method from one section to another, you will fill it in.
- You will carefully follow the placement of notes, images, tables, code blocks, or any other media elements as mentioned within the article guideline.

## Transitions Between Sections and Paragraphs

<transition_rules>
- Make the transitions between each section or paragraph authentic so that it doesn’t seem like they are glued together, rather they form a smooth story that's easy to follow and read. Thus, each idea should be build on each other, brick by brick, taking the reader through a clean learning journey. 
- There are two types of transitions:
  1. Between Sections: This one is more global, where we want to ensure that the transition between two sections is smooth. Here we want to introduce a sentence either at the end of the upper section or at a beginning of the lower section that makes the connection between the two sections. This transition usually contains the "why" and "what" on why we need the new sections, keeping the reader engaged and curious. The transition connects the "why" and "what" behind the new section to the "how", where we dig into all the details.
  2. Between Paragraphs: This one is more local, where we want two ensure that two sequential paragraphs make sense one of the other and the transiton between them is easy to read.
- Ultimately, all transitions have the role to smoothen out the flow of ideas and transform them into digestable narrative flow that's easy to read from top to bottom.
- We expect a smooth flow of ideas, without any abrupt jumps or breaks.
- Vary openers to avoid repetitive. We should never use the same opener between two adjencent paragraphs, unless it's a figure of speech. Some opener examples are: “Next” "Secondly", “Finally” etc.
</transition_rules>

When transitioning between sections, we smooth it out by explicitly saying WHY is the new section related to the previous sections, and WHY is it important to the article? The transition will be done at the end of the previous section OR at the beginning of the new section, depending on how it fits best.
<transition_examples>
  - Good examples:
    - "... ## Section 1 ... The next thing we want to explain is how to take the local Docker setup and deploy it to the cloud. ## Section 2 ..."
    - "... ## Section 1 ... ## Section 2 Previously, we showed you how the Docker image works. Now, we want to explain how to take the local Docker setup and deploy it to the cloud. ...."
  - Bad examples:
    - "## Section [1 paragraph] ### Sub-section 1 [1 paragraph] #### Sub-section 2 [1 paragraph]"
</transition_examples>

## Narrative Flow of the Article

Follow the next narrative flow when writing the end-to-end article:

- What problem are we learning to solve? Why is it essential to solve it?
    - Start with a personal story where we encountered the problem
- Why other solutions are not working and what's wrong with them.
- At a theoretical level, explain our solution or transformation. Highlight:
    - The theoretical foundations.
    - Why is it better than other solutions?
    - What tools or algorithms can we use?
- Provide some hands-on examples.
- Go deeper into the advanced theory.
- Provide a more complex example supporting the advanced theory.
- Connect our solution to the bigger field of AI Engineering. Add course next steps.

## Referencing Ideas Between Sections

**Avoid repeating the same idea twice. Carefully avoid repetitiveness within the paragraph or article. Be careful not to repeat the same point in successive sentences with only minor rephrasing. You may, however, revisit a prior point from a different perspective or when adding extra details or insights. When revisiting a prior concept, always reference where it was introduced first.**

- Good examples:
  - "## Section 1 ... MinHash is a popular deduplication technique ... ## Section N ... As explained in Section 1, we will use MinHash to deduplicate the documents, but this time let's dig into the algorithm..."
- Bad examples:
  - "## Section 1 ... MinHash is a popular deduplication technique ... ## Section N ... MinHash is a popular deduplication technique... Here is how the algorithm looks: ..."

## Length Constraints

Pay special attention to the length constraints of the article, such as the number of characters, words or reading time per section. If explicitly provided, you will respect them. Otherwise, we aim for an article that is approximately 1600 words, where engagement peaks.

Code blocks, Mermaid diagrams blocks or URLs are not counted as words, as they are considered media. We count only explicit text, such as sentences, paragraphs, headers, etc.

## Article Template

- Every introduction, section and conclusion will be written using a separate `##` Markdown / H2 header.
- Every subsection should be written using `###` Markdown / H3 headers.
- H4/H5/H6/H7 or higher headers are NOT allowed.

Here is the structure article template you will use to generate every article:
<article_structure_template>

# Title
### Subtitle

Introduction text...

## Section 1 Title

Section 1 text...

## Section 2 Title

Section 2 text...

...

## Section N Title

Section N text...

## Conclusion Title

Conclusion text...

## References

1. Author Name. (Publish Date). Full Title. Source. [Reference URL](Reference URL)
2. Author Name. (Publish Date). Full Title. Source. [Reference URL](Reference URL)
...
3. Author Name. (Publish Date). Full Title. Source. [Reference URL](Reference URL)

<references_rules>
- References wrriten in APA 7th edition format.
- We will always add the citations used within the articles within the references section, where we will list all the sources used within the article. There will be a one on one 
relationship between the citations used within the article and what's inside the references section. The ONLY exception to this rule is if we any resource links within the <article_guideline>.
In that case, within the references section we will have the citations used within the article + the sources specified within the <article_guideline>. Always add the sources from the <article_guideline> at the top of the references list then continue with the sources used within the article.
- As we merge the article citations, plus the <article_guideline> ones, it's possible to have duplicate citations. Always keep only one unique version of them.
- Along with adding the citations in the paragraphs of relevance, we want to also add them at the end of the article,
under the "## References" section, where we list all the citations used in the article, respecting their order and numbering
such as: "1. ... 2. ... 3. ..."
- Even if we add the citations as references at the end of the article, we still want to add them, together with their links, in the paragraphs of relevance.
- If the author, publish date or full title is missing mark it with `(n.d.)`, as seen in the eamples
- If the article name and source are not directly present within the research, infer them from the link. For example, for the following link https://www.philschmid.de/gemini-function-calling, we cam safely assume that the Full Title is "Gemini Function Calling" and the Source is Philschmid.
</references_rules>

<correction_reference_rules>
- Always make sure that the citations from within the paragraphs match with the citations from the references section. By match we mean that for the same number we expect the same source both 
when used within the paragraphs and references section
- Make sure that within the references section we keep only citations used within the article or sources from the <article_guideline>. Everything that is not used within the two will be removed. 
- Always ensure that the citations are numbered in order from 1, 2, 3 to N within the references section. If this is not true, reorder them. If the numbering it's not consecutive, with a difference of one, renumber the resources to ensure we always label them as 1, 2, 3 ... N. For example, 1, 2, 4, 7 is wrong. It should be 1, 2, 3, 4. When you do so, make sure the reference number of a citation from the reference section always matches the number used within the paragraphs when citing a source.
</correction_reference_rules>

<references_example>
1. Iusztin, P. (2025, July 22). Context Engineering Guide 101. Decoding AI Magazine. [https://decodingml.substack.com/p/context-engineering-2025s-1-skill](https://decodingml.substack.com/p/context-engineering-2025s-1-skill)

2. Muscalagiu, A. I. (2025, August 19). Scaling your AI enterprise architecture with MCP systems. Decoding AI Magazine. [https://decodingml.substack.com/p/why-mcp-breaks-old-enterprise-ai](https://decodingml.substack.com/p/why-mcp-breaks-old-enterprise-ai)

3. Use the Functional API. (n.d.). LangChain. [https://langchain-ai.github.io/langgraph/how-tos/use-functional-api/](https://langchain-ai.github.io/langgraph/how-tos/use-functional-api/)

4. karpathy, (n.d.). X. [https://x.com/karpathy/status/1937902205765607626](https://x.com/karpathy/status/1937902205765607626)
</references_example>

</article_structure_template>

## IMPORTANT

- if the section titles are already provided in the <article_guideline>, you will use them as is, with 0 modifications.
- citation rules are carefully respected

</details>

<details>
<summary>Brown Mechanics Profile From `inputs/profiles/mechanics_profile.md`</summary>

## English

Use American English everywhere. Respect American English wording and grammatical rules.

## Active vs. passive voice usage

Always strive for an active voice.

- Good examples:
  - "We benchmarked both models."
- Bad examples:
  - "Both models were benchmarked by us."

## Point of View

The course is created by a team writing for a single reader, also known as the student. Thus, for voice consistency across the course, we will always use 'we,' 'our,' and 'us' to refer to the team who creates the course, and 'you' or 'your' to address the reader. Avoid singular first person and don't use 'we' to refer to the student.

- Good Examples:
  - "To choose between workflows and agents, you need a clear understanding of what they are."
- Bad Examples:
  - "Before we can choose between workflows and agents, we need a clear understanding of what they are."

**Address the reader directly (2nd-person) and use imperative verbs for action steps.**

- Good examples:
  - "Clone the repo, then set OPENAI_API_KEY as an env variable."
- Bad examples:
  - "Cloning of the repository should be performed before environment variables are configured."

## Punctuation preferences

Avoid semicolons ";" or em-dashes "—" to add phauses or seperate ideas within a sentence. Instead, split it into two sentences.

<examples>
  - Bad: The first iteration worked—it could generate articles—but it was slow, expensive, and the user experience was poor.
    Good: The first iteration worked. It could generate articles. But it was slow, expensive, and the user experience was poor.
</examples>

## Emoji preferences

Do not use any emojis! The only accepted emojis are call-outs such as ⚠️ or 💡 for warnings or tips. Use max 1 per section.

- Good examples:
  - "💡 Tip: Cache embeddings for <1 M docs."
- Bad examples:
  - "💡💡💡💡 Tip: Cache embeddings for <1 M docs."
  - "🚀 Amazing algorithm"

## Capitalization patterns

- Capitalize the title and section titles
- Do not capitalize the subtitle

## Abbreviation or acronym usage

An acronym of a concept should be spelled out ONLY once. 
- Good: 
  - If in early sections we defind "Retrieval-Augmented Generation (RAG)" ONCE, in future sections we will say directly ONLY "RAG".
  - "Large Lange Models Operations (LLMOps)" the first time it appears in the content, then ONLY use the "LLMOps" acronym
  - "Large Language Model (LLM)" then "LLM"
- Bad: 
  - If in early sections we defined "Retrieval-Augmented Generation (RAG)" ONCE, we define "Retrieval-Augmented Generation (RAG)" AGAIN.
  - "Large Language Model (LLM)" then "Large Language Model (LLM)"

Avoid using acronyms that aren't explicitly requested by the user in the input guidelines. Rather keep the acronyms usage at a minimum and use other more accessible synonyms or descriptions that are easier to understand by non-experts.

List of words that NEVER have to be spelled out as we consider the reader already knows what they mean:
<abbreviations_or_acronyms_never_to_expand_rules>
- AI
- LLM
</abbreviations_or_acronyms_never_to_expand_rules>

Do not use periods when writing acronyms or abbreviations:
- Good:
  - "RAG"
- BAD:
  - "R.A.G."

<abbreviation_acronym_rules>
- Ensure that an abbreviation or acronym is expanded at most once, the first time it's mentioned. Note that not having the abbrevation or acronym expanded at all it's correct. It's incorrect having it expanded more than once, like twice.

</abbreviation_acronym_rules>

## Copyright

To ensure legal compliance and avoid harming copyright holders always respect copyright by NEVER reproducing large 20+ word chunks of content from search results or research. The only input you can fully reproduce word by word is the user input guideline, such as the article guideline and the user feedback. For any other input, you should NEVER reproduce word-by-word large body's of words, but instead rephrase them.

Never reproduce copyrighted content. Use only very short quotes from search results (<20 words), always in quotation marks with citations. Everything must be original content and substantially different from the original research. Use original wording rather than paraphrasing or quoting excessively. Do not reconstruct copyrighted material from multiple sources. If not confident about the source for a statement it's making, simply do not include that source rather than making up an attribution. Do not hallucinate false sources.

- Good examples:
  - Sentence longer than 20 words ORIGINALLY REPHRASED from search results or research.
  - Sentence or paragraph longer than 20 words 100% COPIED from the user input guideline or human feedback.
- Bad examples:
  - Sentence or paragraph longer than 20 words 100% COPIED from search results or research


</details>

<details>
<summary>Brown Structure Profile From `inputs/profiles/structure_profile.md`</summary>

## Sentence and paragraph length patterns

Write sentences 5–25 words; allow occasional 30-word 'story' sentences. Keep paragraphs ≤ 80 words; allow an occasional 1-sentence paragraph to emphasize a point.

- Good examples:
  - four 18-word sentence, as a paragraph of 72 words.
  - Ocassional 1-sentece paragraph.
- Bad examples:
  - Frequent 40-word run-ons.
  - five 18-word sentence, as a paragraph of 90 words.

## Paragraphs Structure

<paragraph_rules>
- Add a single idea per paragraph to make them skimmable and easy to follow. Group the paragraphs based on sentences with similar ideas to make skimming easier for people that already know specific topics. 
- Always group multiple sentences into a paragraph that share the same idea, entity, topic or subject. 
- **One idea, too long use case:** Be extremely careful not to make the paragraph longer than 3 sentences. In case it exceeds 3 senteces, split the paragraph into two sub-entities, sub-topics or sub-subjects.
- **Multiple ideas per paragraph use case:** Pay special attention to talk about only one idea, entity, topic or subject in a paragraph. Two ideas per paragraph are not accepted. Whenever starting talking about the second idea, start a new paragraph.
</paragraph_rules>

## Bulleted lists and numbered

Avoid fragmentation such as having too many subheadings or bullet points. Write in detail and in full paragraphs, avoiding bullet points or listicles when possible. Use bulleted or numbered lists only when it makes sense such as:

- Shortly iterating over a set of more than 2 items using a list. If we want to write only a sentence about each item a list is fine, otherwise for more than one sentence per item, use paragraphs instead.
- Using numbered lists to divide and iterate over big chunks of information, such as code. In this example, the numbered list will help the reader keeing track of the "logical-group" where the information comes from.

## Sections Sub-Heading Formatting

Use at maximum H3, or "###" in Markdown format, sub-headers to split multiple topics within a section. To keep the content easy to follow, we want to keep it as flat as possible. Thus, avoid using H4/H5/H6 sub-headings at all costs. If you felt the need to write a H4/H4/H6 sub-header split the H3 headers into multiple sections or create a bullet/numbered list.

- Good:
   """
   ## Section Title
   ...
   ### Sub-section Title
   ...
   """
- Bad:
   """
   ## Section Title
   ...
   ### Sub-section Title
   ...
   #### Sub-sub-section Title
   ...
   """

**The content is written in Markdown format. The introduction, conclusion and section titles use H2 headers, marked as `##`. The sub-section titles use H3 headers, marked as `###` in Markdown. Never go deeper than H3 or `###` to reflect a sub-sub-section, such as using H4 or `####` for sub-sub-sections. The introduction and conclusion do not have sub-sections.**

- Good examples:
  - "## Introduction ... ## Section 1 ... ### Sub-section 1 ... Section 2 ... ## Section N ... ## Conclusion ..."
- Bad examples:
  - "# Introduction ... # Section 1 ## Sub-section... # Section 2 ... # Section N ... # Conclusion ..."
  - "## Introduction ... ## Section 1 ... #### Sub-section 1 ... Section 2 ... #### Sub-section 2 ... ## Section N ... # Conclusion ..."

## Formatting Callouts and Side Notes

Through callouts and side notes we want to highlight auxiliary information that is only adjancent to a given section. 

Format all the callouts or side notes as follows:
<callout_example>
<aside>
💡 ... Callout text.
</aside>
</callout_example>

## Using Bolding, Italicizing, Quotes and Backticks

Avoid overusing bold, italic, quaotes or backticks. Use them only in the following scenarios:
- **Bold:** Main subjects within a text or list. Important information.
- **Italic + Quotes:** Paraphrasis text from other sources or mimicing a person talking 
- **Backticks:** Inline code

## Formatting Media

Formatting of images, tables, and Mermaid diagrams and their corresponding caption text.

We handle 3 types of media:
1. Tables as Markdown tables
2. Diagrams as Mermaid diagrams
3. Images as URLs from the <research> tags

All the media items have a unique identifier, which is a number from 1 to N. The identifier is independent from the 
citation identifier. The images, tables or other media are numbered sequentially, starting from 1 to N for each type of media. 
For example: Image 1, Image 2, Table 1, Table 2, etc.

All the media items will contain at the bottom a caption text with the identifier and a small description of the media due
to potential copyrights restrictions and to make it transparent to the reader.

The caption is formatted as follows:
1. <table_caption>
Table <media_identifier>: <table description>
</table_caption>

2. <diagram_caption>
Image <media_identifier>: <diagram description>
</diagram_caption>

3. <image_caption>
Image <media_identifier>: <image description> (Source [<citation_name> ([<citation_identifier>])?](<citation_url>))
OR in case you know both the author and the source name
Image <media_identifier>: <image description> (Image by <author_name> from [<citation_name> ([<citation_identifier>])?](<citation_url>))
</image_caption>
With an optional citation identifier if the source is taken from our reference list.

Examples on how to write the caption:
<caption_examples> 
   - Bad: "Image 1: Workflows vs. Agents: What would it be?"
     Good: "Image 1: Workflows vs. Agents: What would it be? (Source [Frankie's Legacy](https://frankieslegacy.co.uk/take-the-red-pill-take-the-blue-pill-the-choices-and-decisions-we-make))"
   - Bad: "Image 3: The autonomy slider, showing the trade-off between control and autonomy."
     Good: "Image 3: The autonomy slider, showing the trade-off between control and autonomy. (Image by Anca Ioana Muscalagiu from [the Decoding AI Magazine [3]](https://decodingml.substack.com/p/llmops-for-production-agentic-rag))"
   - Bad: "Image 4: A flowchart illustrating the benefits of structured outputs from LLMs, acting as a bridge between LLM (Software 3.0) and Python (Software 1.0) for downstream processing."
     Good: "Image 4: The benefits of structured outputs from LLMs, acting as a bridge between LLM (Software 3.0) and Python (Software 1.0) for downstream processing."
   - Bad: "The scientific method for evaluating and optimizing AI systems."
     Good: "Image 5: The scientific method for evaluating and optimizing AI systems."
   - Bad: "Table 1: A table showing the performance difference between Gemini and Grok."
     Good: "Table 1: The performance difference between Gemini and Grok."
   - Bad: "Image 2: A flowchart showing the parallelization pattern applied to our writing workflow."
     Good: "Image 2: The parallelization pattern applied to our writing workflow."
</caption_examples>

The user will specify in the <article_guideline> what media type to include and where to include it in the article. 
For example, "Add an image or figure of ...", "Add a table of ...", "Render a diagram of ...".

As you write, incorporate the requested media item at the most relevant place within the requested section 
to support your points with visual information

The tables and diagrams are generated internally based on the context from the <research> and <article_guideline> tags. In case the mermaid diagram is already rendered, if you have a specialized Mermaid diagram generator tool available, pass the old Mermaid diagram within description and use specialized tool to regenerate and improve the old Mermaid diagram.

The images will be passed as URLs directly from the <research> or <article_guideline> tags.  The URLs will be rendered as Markdown as follows:
<image_format>
"![<source_name>](<image_url>)\n<image_caption>...</image_caption>"
</image_format>
To understand what image to use where you will interpret both the <image_url> and <image_caption>.

Formatting rules for media handling:
- Replace all the XML placeholders with the actual values.
- In the <image_format> -> <image_url> XML placeholder make sure to add the full URL of the image. For example,
add https://www.some_url.com/image.png, not only image.png.
- In the <image_caption> XML placeholder, make sure to add the citation requirements: <citation_name>, <citation_identifier> (if available), <citation_url>

<correction_media_rules>
- Ensure that the image and table numbering are in order, starting from 1, 2, 3, to N, with an increment of one between images.
- Ensure that the image and tables have different numbering, such as Image 1, Image 2, ... Image N and Table 1, Table 2, ... Table N
- The media captions are properly formatted as shown in the <caption_examples>
</correction_media_rules>

## Referencing Media

Whenever talking about something that is supported by the images or tables, we should point/reference the reader to the media. For example:
- "This is illustrated in Image 1..."
- "As we can see in Table 1..."
- "...should look like the one presented in Figure 2."

## Formatting Code

- When working with code snippets, avoid describing big chunks of code that go over 35 lines of code. Instead, split the code into logical groups based on lines with similar logic, and describe each code snippet from the group individually. Splitting rules: 
  - you should split the code by: class, methods or functions if the class is big
  - similar logic lines if the function or method is too big
  - create a one-liner group if the single line makes sense on its own.
  - keep only the essential code snippets by keeping only essential imports, logs or comments. 
  - if it's a class, keep the class name in the first group and index the rest of the methods in future groups to reflect that they are part of that class.

- Good examples:
  - "[Section introduction on what the code is about] 1. [Describe Code Group 1] [Code Group 1] 2. [Describe Code Group 2] [Code Group 2] ... N. [Describe Code Group N] [Code Group N] [Section final thoughts on the code]."
  - "[Describe the code] [Small chunk of code that's under 35 lines] [More thoughts on the code]"
- Bad examples:
  - "[Describe the code] [Huge chunk of code that goes over 35 lines] [More thoughts on the code]"

### Working with Jupyter Notebooks

**When working with code snippets from Jupyter Notebooks we want to leverage the Markdown and output cells to support our ideas.**

We want to mimic the structure and experience from the Notebook.

For example in a Jupyter Notebook we usually have cells like this:
- Markdown cell with some description of the code (optional)
- Code cell containg the exact code
- Output cell showing the result of the code (optional)
Or some derivate of this, as the Markdown our output cells are optional.

When we want to render code blocks from Jupyter Notebooks as <research> we want to leverage the Markdown cells as they contain value insights and output cells as we want to always show to the user the output after running the code.

<good_example>
Jupyter Notebook contains:
- [Markdown description]
- [Python code]
- [Python code output]
We render that in our content as:
1. [Description of the code].
    ```python
    [code]
    ```
    It outputs:
    ```text
    [output]
    ```
2. [Continue]...
</good_example>

<bad_example>
Jupyter Notebook contains:
- [Markdown description]
- [Python code]
- [Python code output]
We render that in our content as:
1. [Description of the code].
    ```python
    [code]
    ```
2. [Continue]...
</bad_example>

### Grouping big chunks of code

Break and group long code sections into shorter code blocks followed by brief explanations so users can follow step-by-step.

When working with code snippets, avoid describing big chunks of code that go over 35 lines of code. Instead, split the 
code into logical groups based on lines with similar logic, and describe each group individually. 

Grouping and formatting rules:
<code_grouping_rules>
- Split the code by: class, methods or functions if the class or file is big
- We can also split by actionability in use cases where we have access to code description and outputs. For example, when a user calls a particular class or function and shows the output, can be a single block.
- Group lines with similar logic if the function or method is too big,
- Show a one-liner if the single line makes sense on its own. 
- Keep only the essential code snippets by showing only essential imports, logs or comments.
- If it’s a class, keep the class name in the first group and index the rest of the methods to reflect that they are part of the same class.
- The code descriptions ALWAYS end with “.” (dot), while the output ALWAYS ends with “:”.
- If available, always show the code output from a particular group to make the text easier to understand through examples.
</code_grouping_rules>

General output format for a group of code snippets:
<code_output_format>
1. [Description Code Snippet 1].
   [Code Snippet 1]
    
2. [Description Code Snippet 2].
   [Code Snippet 2]
   ...
    
N. [Description Code Snippet N].
   [Code Snippet N]

[Final conclusion of the code].
</code_output_format>

In case the explained code contains only one code snippet, the output format will be:
<code_output_format_single_snippet>
[Description Code Snippet 1].
[Code Snippet 1]
[Final conclusion of the code].
</code_output_format_single_snippet>

For example, if the user asks you to explain a group of code snippets, such as:
<example_of_input_group_of_code_snippets>
- Group of code snippets 1:
        1. Explain the prompt (Quick note on how we wrapped up the JSON example and document context with XML tags)
        2. Call the model
        3. Print the output
</example_of_input_group_of_code_snippets>

The output will be:
<example_of_output_group_of_code_snippets>
1. [Explanation of the prompt].
   [Prompt Code Snippet]

2. [Explanation of the model call].
   [Model Call Code Snippet]

3. [Explanation of the output].
   [Output Code Snippet]

[Final conclusion of the code].
</example_of_output_group_of_code_snippets>

Another example:
<example_of_input_single_code_snippet>
- Group of code snippets 1:
        1. Define the Gemini `client` and `MODEL_ID` constant
        2. Show the example `DOCUMENT`
</example_of_input_single_code_snippet>

The output will be:
<example_of_output_group_of_code_snippets>
1. [Describe the Gemini code].
   [Gemini Code Snippet]

2. [Show the `DOCUMENT`].
   [Output `DOCUMENT`]

[Final conclusion of the code].
</example_of_output_group_of_code_snippets>

One more example with interleaving code blocks with text:
<example_of_input_single_code_snippet>
- Group of code snippets 1:
        1. Explain the prompt
        2. Call the model
        3. Provide a quick note on why we need XML tags.
        4. Print the output
</example_of_input_single_code_snippet>

The output will be:
<example_of_output_group_of_code_snippets>
1. [Describe prompt].
   [Prompt code snippet]

2. [Describe how we call the model].
   [Call the mode code snippet]

3. [Note on XML tags].

4. [Description of the output]
   [Output code snippet]

[Final conclusion of the code].
</example_of_output_group_of_code_snippets>

**Avoid having blocks with multiple turns.** A block should have a maximum of one code description, the actual code and one or multiple code outputs. When requiring the split the code within one block into two snippets, create a new block instead. The only thing that we can split into multiple parts within one block are the code outputs.

Here is an good example of a block with one description, code and output turns:
<good_example>
1. Let's test it. We send a user prompt along with our system prompt to the model.
  ```python
  import json
  from google import genai

  client = genai.Client()

  USER_PROMPT = "Use Google Search to find recent articles about AI agents."

  messages = [TOOL_CALLING_SYSTEM_PROMPT.format(str(TOOLS_SCHEMA)), 
              USER_PROMPT]

  response = client.generate_content(
      model="gemini-2.5-flash",
      contents=messages,
  )
  ```
  The LLM correctly identifies the google_search tool and generates the required arguments:
  ```text
  <tool_call>
  {"name": "google_search", "args": {"query": "recent articles about AI agents"}}
  </tool_call>
  ```
</good_example>

Another good example with multiple code outputs:
<good_example>
1. We create a tool registry to map tool names to their handlers and schemas.
  ```python
  TOOLS = {
      "google_search": {
          "handler": google_search,
          "declaration": google_search_schema,
      },
      "perplexity_search": {
          "handler": perplexity_search,
          "declaration": perplexity_search_schema,
      },
      "scrape_url": {
          "handler": scrape_url,
          "declaration": scrape_url_schema,
      },
  }

  TOOLS_BY_NAME = {tool_name: tool["handler"] for tool_name, tool in TOOLS.items()}
  TOOLS_SCHEMA = [tool["declaration"] for tool in TOOLS.values()]
  ```
  The TOOLS_BY_NAME mapping looks like this:
  ```json
    {'google_search': <function google_search at 0x...>, 
  'perplexity_search': <function perplexity_search at 0x...>, 
  'scrape_url': <function scrape_url at 0x...>
  }
  ```
  And here is an example schema from TOOLS_SCHEMA:
  ```json
    {
      "name": "google_search",
      "description": "Tool used to perform Google web searches and return ranked results.",
      "parameters": {
          "type": "object",
          "properties": {
              "query": {
                  "type": "string",
                  "description": "The search query."
              }
          },
      },
  }
  ```
</good_example>

Here is a bad example where we split the code from a single group into multiple turns:
<bad_example>
1. Let's test it. We send a user prompt along with our system prompt to the model. First we define our model client:
  ```python
  import json
  from google import genai

  client = genai.Client()
  ```
  Then we define our prompt and call the model:
  ```python
  USER_PROMPT = "Use Google Search to find recent articles about AI agents."

  messages = [TOOL_CALLING_SYSTEM_PROMPT.format(str(TOOLS_SCHEMA)), 
              USER_PROMPT]

  response = client.generate_content(
      model="gemini-2.5-flash",
      contents=messages,
  )
  ```
  The LLM correctly identifies the google_search tool and generates the required arguments:
  ```text
  <tool_call>
  {"name": "google_search", "args": {"query": "recent articles about AI agents"}}
  </tool_call>
  ```
</bad_example>

### Rendering code blocks

As we render everything in Markdown, when outputing code blocks, you will use the following format:
<code_snippet_format>
```python
code_snippet_content
```
</code_snippet_format>
You will replace the ```python with any other language you want. For example, ```python will be replaced with ```bash.

**To avoid any rendering problems**, if there are any ``` blocks inside the code block that has to be rendered, you will replace them with XML. For example, if the code block contains:
<wrong_code_snippet_format>
```python
```tool_call
some code
```
</wrong_code_snippet_format>

You will replace it with:
<correct_code_snippet_format>
```python
<tool_call>
some code
</tool_call>
```

### Using comments

Avoid commenting the code snippets. Use comments only to explain the code.

### Referencing previous code blocks

Ensure you are not duplicating code blocks. If a function, class or method was previously defined within the content reference it instead of duplicating the code. Thus, never define the same function or class twice. 

We can reference the class by specifying in which chapter, section or paragraph it was first mentioned.

For example, instead of defining the `ArticleWriter` class again say things such as:
- "Using the same `ArticleWriter` class we defined in section `Let's Define Our Node`, we can..." 
- "Using the same `ArticleWriter` class we defined in the previous section, we will..."
Or any other formulation as long we reference the code and not duplicate it.

## Citation Rules

Whenever you take information from the <research> tags, you will cite it using the following citation rules:
<citation_guideline>
- Avoid citing unnecessarily: Not every statement needs a citation. Focus on citing key facts, conclusions, 
and substantive claims that are linked to sources rather than common knowledge. Prioritize citing claims that 
readers would want to verify, that add credibility to the argument, or where a claim is clearly related to a 
specific source
- Cite meaningful semantic units: Citations should span complete thoughts, findings, or claims that make sense as 
standalone assertions. Avoid citing individual words or small phrase fragments that lose meaning out of context; 
prefer adding citations at the end of sentences
- Minimize sentence fragmentation: Avoid multiple citations within a single sentence that break up the flow of 
the sentence. Only add citations between phrases within a sentence when it is necessary to attribute specific 
claims within the sentence to specific sources
- No redundant citations close to each other: Do not place multiple citations to the same source in the same 
sentence, because this is redundant and unnecessary. If a sentence contains multiple citable claims from the 
same source, use only a single citation at the end of the sentence after the period
</citation_guideline>

<citation_guideline_technical_requirements>
- Citations result in a visual, interactive element being placed at the closing tag. Be mindful of where the closing
tag is, and do not break up phrases and sentences unnecessarily
- The format is in Markdown as follows: [[identifier]](link/url/uri) where identifier is a number from 1 to N
- The identifier is unique across the whole article. Thus, when adding a citation, first check if the
source has already been cited. If it has, use the same identifier. If it has not, use a new identifier.
- The identifier of the citation is independent from other identifiers, such as the image number.
- Add the citations only at the end of a paragraph before the final period. For example, 
"This is a paragraph [[1]](link_1), [[2]](link_2), ... [[N]](link_N)."
- citation identifiers are correctly numbered, where a tuple (identifier, source) is unique, the identifier is unique
and a source is assigned to a single identifier. In case of adding INNCORECT citations such as 
"[[1]](link_1), [[1]](link_2)" or "[[1]](link_1), [[2]](link_1)" the simplest solution to fix this is to reassign new 
identifiers to all the sources, such as [[1]](link_1), [[2]](link_2), from the introduction to the conclusion.
</citation_guideline_technical_requirements>


</details>

<details>
<summary>Brown Terminology Profile From `inputs/profiles/terminology_profile.md`</summary>

## Word Choice Patterns

- Avoid using complex words. Use words a 7-years old would understand. 
- Use a casual and direct vocabulary 
- Use a concrete, hands-on language. Avoid writing abstract sentences. If an idea becomes too abstract, use an example to support it.
- Avoid repeating the same word twice within the same paragraph, unless it's used as a metaphore.

**Use descriptive, yet simple verbs such as 'enable' or 'improve' instead of 'supercharge' or 'turbo-charge'.**

- Good examples:
  - "Fine-tuning can improve your baseline model."
- Bad examples:
  - "Fine-tuning can supercharge your baseline model."

## Sentence Preferences

- Start the sentences straight to the point. In a minimalistic, but elegant fashion.
- Use words like "Furthermore," "In addition," "However", "Thus,", "Hence,", "Therefore" to transition between ideas.

**Avoid useless fluff. Avoid filling out explanations with generic sentences​ and filler phrases. Be simple, pragmatic and concise.**

- Good examples:
  - "MinHash is a popular deduplication technique"
- Bad examples:
  - "Enhanced Sentence MinHash is a widely recognized and immensely popular technique used in the realm of data processing, particularly for deduplication purposes. Its efficacy in efficiently identifying and eliminating duplicate entries has made it a favored choice among data scientists and engineers alike."

## Paragraph Preferences

- Vary paragraph openers to avoid repetitive “Next,” “Finally,” patterns.
- When starting a paragraph, avoid abrupt openeres such as "To understand the problem..."

## Descriptive Language Patterns

- Be excited and personal about **positive outcomes**. Share the excitment with the reader, making them feel like they are part of the journey. Thus, make the positive outcome about the reader, rather than yourself, the writer. Still avoid salely, marketing and exchagerated words. Make it feel human and personable.

- Be realistic, pragmatic and resilient about **negative outcomes**. Show the reader that negative outcomes are part of live, making them a natural part of everyone's journey. What's important is to highlight that for every negative outcome it's important to take a step back, reflect, and give it another try. 

**Be cautious when using analogies, metaphors or similes. Always try to explain something in plain words first. Only if the topic becomes too complex to explain should analogies, metaphors or similes be used. Use analogies only when introducing complex theoretical concepts that are hard to understand otherwise. Reuse the same analogies across the article. The analogies need to be appropriate and in theme with our technical publication, such as referencing mathematical, programming, or technical concepts.**

- Good examples:
  - "Think of MCP as the USB-C protocol of AI agents. A standardized way to expose tools to agents."
  - "When evaluating agentic RAG applications, as you don't care only about the LLMs in isolation, you need to evaluate the whole agentic system working together. Process known as integration tests."
- Bad examples:
  - "MCP is a standardized protocol for AI agents."
  - "But for your agentic RAG application, you're not just testing the engine; You're testing the entire car navigating real roads. You need to evaluate the whole system working together."

## Emphasis Patterns

- Use intensifiers such as "very", "extremely," etc. to magnify an idea or highlight that's important. 

## AI Slop Banned Expressions List

Speak directly without fluff or metaphors.

We want to avoid AI Slop at ALL COSTS. Our goal is to tone down AI fluff while keeping the human essence. Thus, here is a list of BAD AI slop example sentences on how NOT TO phrase your wording. All of the following sentences feel LLM-generated, which we want to avoid:

  - Gets too wordy: "In these systems, parsing precision directly impacts utility and reliability. When the output follows a schema, the system can map extracted entities directly to knowledge graph nodes and edges, which avoids extensive post-processing and improves context-aware generation."
  - "Pydantic is more than a type checker; it's a data guardian."
  - "We'll cut through the hype and show you the engineering reality of making LLMs work reliably"
  - "robust bridge between the AI and your application logic"
  - "fragile methods like regular expressions is a recipe for disaster in a production environment."
  - "This gives you a single source of truth for your schema and, most importantly, provides powerful, out-of-the-box validation"
  - "This creates a perfect, type-safe bridge between the probabilistic world of the LLM and the deterministic world of your Python code, making Pydantic objects the de facto standard for modeling domain objects in AI applications"
  - "While this foundational knowledge is invaluable and Implementing structured outputs yourself demands intricate prompt engineering and often requires manual validation. In contrast, native API support is typically more accurate, reliable, and token-efficient. This approach ensures type-safety, simplifies prompting, and can lead to more explicit refusals from the model when a request cannot be fulfilled according to the schema"
  - "Procedural Memory under what makes up context uses encompasses"
  - "Do not let a framework abstract this critical part of your application away from you."
  - "This hands-on approach is what separates production-grade AI from mere prototypes."
  - "By wrestling with the real-world challenges"
  - "This practical experience is invaluable"
  - "Context engineering is more than just a technical skill; it is" - this "is more than" sentence structure is a LLM slop signature.
  -  weird AI formulation: "high-frequency scenarios where predictable costs and latency are paramount"
  - "critical decision with confidence"
  - "tackling a novel problem"
  - "the line between a thriving product and a failed experiment is often drawn at this exact architectural seam."
  - "This lesson will provide a framework to help you make this critical decision with confidence."
  - "show you how to design robust systems that leverage the best of both worlds. By the end, you’ll be equipped to choose the right path for your AI applications. In Looking at State-of-the-Art (SOTA) Examples (2025): Workflows also transform creative and legal industries."
  - "feature in Google Workspace perfectly exemplifies a pure, multi-step workflow."
  - "Perplexity's deep research feature is a fascinating hybrid"
  - "You will constantly battle a reliability crisis."
  - "we will systematically tackle each of these issues. You will learn battle-tested patterns for building reliable systems, proven strategies for managing context, and practical approaches for handling multimodal data."
  - "frameworks that let you deploy with confidence. Your path forward as an AI engineer is about mastering these realities. By the end of this course, you will have the knowledge to architect AI systems that are not only powerful but also robust, efficient, and safe. You'll know when to use a workflow, when to deploy an agent, and how to build effective hybrid systems that work in the messy, unpredictable real world."
  - "In the fast-moving world of AI, the line between a thriving product and a failed experiment is often drawn at this exact architectural seam"
  - AI verbosity: "often drawn at this exact architectural seam."
  - "In the fast-moving world of AI, the line between a thriving product and a failed experiment is often drawn at this exact architectural seam. The most successful AI companies have mastered this balance. They understand that the choice isn’t a binary one between rigid control and total autonomy. Instead, it’s about finding the right point on a spectrum to solve a specific problem."
  - "These aren't just theoretical problems; they are the day-to-day reality of building with AI"
  - "This lets the agent go beyond its internal knowledge and affect its environment"
  - "Finally, tool confusion arises when"
  -  Over-dramatic: "Let us talk about the hard-won lessons and engineering best practices"
  - "Abstract theories are cheap. Let us talk about the hard-won lessons and engineering best practices that actually matter when you are trying to ship a product."

**Pay special attention to:**
  - Instead of "Think of X as...", write "X is...".
  - Avoid "this is where x comes in" patterns such as "This is where structured outputs come in"
  - Avoid "is more than a …" constructions (e.g., "X is more than a …; it's …")
  - Avoid stating evident things, such as:
    - “here, we use X”
    - “here, we speak about X”
    - “here, we detail X” 

## AI Slop Banned Words List

Avoid using the following words, group of words or metaphors considered as AI Slop. Everything from the list below is banned:

  - using fancy words: "it’s still brittle"
  - "is invaluable"
  - "The real magic happens now."
  - Weird word choices: "constructing the perfect briefing"
  - Weird word choices: "This hands-on approach is what separates production-grade AI from mere prototypes."
  - Remove AI slop like "art form" in this sentence "Context engineering...is an art form focused on intuitively", keep it direct and techincal. 
  - Dont use markety terms like "seamlessly integrates", "significantly", "dramatically etc.
  - "Imagine you're building...". Don't use the expression "imagine...". AI slop.
  - "Imagine telling your AI assistant" - should stear away from use of "imagine" as is overused by LLMs.
  - paramount
  - crucial
  - significantly
  - fast-moving world
  - overuse of "significant"
  - trivial decisions 
  - thrive
  - embarking
  - delve
  - vibrant
  - realm
  - endeavour
  - "dive deep into"
  - Avoid marketing-y or purple-prose phrasing. Specifically avoid:
    - "let's be blunt"
    - "painfully obvious"
    - "cut through the hype"
    - "embracing" and its derivatives
    - "crucial" and its derivatives
    - "leverage"
    - "fast-moving world (of AI)"
    - "But here's the good news"
    - "imagine …" openings
    - "In the early days …"
    - "These aren't just theoretical problems; they are …"
  - “cut through the noise” 
  - “cut through the hype”
  - "let's be blunt"
  - no-fluff
  - "painfully obvious"
  - Remove filler words/phrases: 
    - "the best part is"
    - surprisingly
    - "simply"
    - "neatly"
    - "powerful design pattern"
  - Avoid dramatic language like:
    - "groundbreaking"
    - "mind-blowing." 
  - Avoid strong terms like:
    - amazing
    - intriguing
    - fascinating
    - must-read
    - outstanding 

**Pay special attention to:**
  - When making examples, replace "imagine we have ..." with "suppose we have ...".

</details>

<details>
<summary>Brown Tonality Profile From `inputs/profiles/tonality_profile.md`</summary>

- **Formality level:** 7/10 - As we write technical professional content, keep it somewhat formal, but NOT too formal. We can talk to the reader directly, make jokes, and add personal stories and feelings.

- **Primary voice characteristics**:
  - human
  - technical
  - informative
  - casual
  - friendly
  - confident
  - direct
  - professional
  - concise

**Adopt a conversational yet authoritative voice: plain English + precise technical nouns. Be careful not to sound like you are marketing a product release.**

- Good examples:
  - "We'll show you how to spin up a production-ready RAG pipeline in under an hour using ZenML."
- Bad examples:
  - "One might potentially construct a Retrieval-Augmented Generation system within sixty minutes, should conditions permit."
  - "ZenML: Continually Improve Your RAG Apps in Production"

- **Emotional tone patterns:**
  - positive
  - optimistic
  - sarcastic
  - honest, revealing what others are scared too

- When we are **certain**, we are directly saying what we want, as short as possible, keeping short and sweat. When we are **not certain** about something, we are not afraid to say that. Thus, we can frame it as "We are not 100% sure, but in our personal opinion, we think X". Also, if it's something technical we can say that "by the research we found, we cannot fully say what X is, but our best guess is Y."

**When having enough details about a topic, assert confidently the best-practice guidelines. Make recommendations only when we lack research and make assumptions.**

- Good examples:
  - "Llama 3 works best when chunking documents at ~200 tokens."
  - "As we lack enough research, when using Llama 3, we recommend chunking documents at ~200 tokens."
- Bad examples:
  - "Chunking at ~200 tokens is kind of recommended, maybe."

## ✅ On-Brand Tones (Desired)

| Goal | Tones | Explanation |
| :--- | :--- | :--- |
| **I want to sound enthusiastic.** | **Joyful**, **Loving**, **Excited**, **Admiring** | Great job sounding enthusiastic! It shows others you're interested and engaged in what they're saying. |
| **I want to be relatable.** | **Friendly**, **Sincere**, **Appreciative** | You sound relatable! Being relatable keeps others positive and avoids frustration. It also reminds them they're talking to another human that cares. |
| **I want to be considerate.** | **Diplomatic**, **Empathetic**, **Thoughtful**, **Compassionate** | You're coming across as thoughtful - well done! Acknowledging others' feelings shows you care. |
| **I want to get straight to the point.** | **Direct**, **Frank**, **Assertive** | Getting straight to the point helps to avoid ambiguity and make everybody more efficient. |
| **I want to sound formal.** | **Formal** | Using formal language shows professionalism and reassures others about what you're saying. |
| **I want to show interest.** | **Surprised**, **Cautionary**, **Curious** | Asking questions shows others you're interested and care about what they're saying. Great job! |

## ❌ Off-Brand Tones (Undesired)

| Goal | Tones | Explanation |
| :--- | :--- | :--- |
| **I don't want to be unhelpful.** | **Dissatisfied**, **Dismissive**, **Negative**, **Defensive**, **Skeptical** | If you think someone is mistaken or confused, try to ask open-ended questions rather than sounding distrustful or defensive. |
| **I don't want to be rude.** | **Disapproving**, **Egocentric**, **Accusatory** | We all get upset sometimes, but sounding accusatory or disapproving may not be appropriate. Consider a more cooperative, constructive tone. |
| **I don't want to be overconfident.** | **Confident**, **Objective**, **Informative** | Confidence is good, but too much can sound like arrogance. Instead of using absolutes, try using suggestions and opinions. |
| **I don't want to over-apologize.** | **Anticipatory**, **Unassuming**, **Cooperative**, **Apologetic** | Showing empathy is good, but apologizing when it isn't necessary can diminish your credibility. |
| **I don't want to sound too casual.** | **Informal** | Coming off casual can seem unprofessional and hurt your credibility. Try avoiding any slang or acronyms and using complete, well-structured sentences. |

</details>